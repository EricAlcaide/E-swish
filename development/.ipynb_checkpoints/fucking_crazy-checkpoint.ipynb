{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Repeating MNIST first experiment of swish paper. \"\"\"\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# For adding new activation function\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train original shape (60000, 28, 28)\n",
      "y_train original shape (60000,)\n"
     ]
    }
   ],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "print(\"X_train original shape\", X_train.shape)\n",
    "print(\"y_train original shape\", Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalization\n",
    "nb_classes = 10\n",
    "# Normalize the data\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\n",
    "Y_train = np_utils.to_categorical(Y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(Y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "random_seed = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the train and the validation set for the fitting\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000, 784) (6000, 784) (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1)\n",
    "X_train = X_train.reshape(-1,784)\n",
    "X_val = X_val.reshape(-1,784)\n",
    "X_test = X_test.reshape(-1,784)\n",
    "# test = test.values.reshape(-1,28,28,1)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def swish(x):\n",
    "    return x*K.sigmoid(x)\n",
    "\n",
    "def e_swish_2(x):\n",
    "    sigmoid = K.sigmoid(x)\n",
    "    return K.maximum(x*sigmoid, x*(2-sigmoid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the CNN model \n",
    "# my CNN architechture is In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Out\n",
    "def create(act, n):\n",
    "    model = Sequential()\n",
    "    # First conv block\n",
    "    model.add(Dense(512, input_shape=(784,)))\n",
    "    model.add(Activation(act))\n",
    "    for i in range(n-1):\n",
    "        model.add(Dense(512))\n",
    "        model.add(Activation(act))\n",
    "        model.add(Dropout(0.3))\n",
    "        \n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "[10, [0.13656429858431221, 0.96499999999999997]]\n",
      "[15, [1.1838343352317811, 0.43409999999999999]]\n",
      "[20, [2.544732809829712, 0.12379999999999999]]\n"
     ]
    }
   ],
   "source": [
    "record = []\n",
    "for n in [10, 15, 20]:\n",
    "    opt = SGD()\n",
    "    # Set a learning rate annealer\n",
    "#         learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=2, verbose=1, factor=0.7, min_lr=0.00001) \n",
    "    # Common params \n",
    "    epochs = 10\n",
    "    batch_size = 100\n",
    "    # Create and compile the model\n",
    "    model = model = create(\"relu\", n)\n",
    "#     model.summary()\n",
    "    # Compile the model\n",
    "    model.compile(optimizer = opt , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    # Train the model\n",
    "    history = model.fit(X_train,Y_train, epochs = epochs, validation_data = (X_val,Y_val),\n",
    "                        verbose = 1)# , callbacks=[learning_rate_reduction])\n",
    "\n",
    "    record.append([n, model.evaluate(X_test, Y_test)])\n",
    "    print(n, \":\", model.evaluate(X_test, Y_test))\n",
    "    K.get_session().close()\n",
    "    K.set_session(K.tf.Session())\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "for r in record:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "54000/54000 [==============================] - 32s 600us/step - loss: 1.2627 - acc: 0.5371 - val_loss: 0.3269 - val_acc: 0.9152\n",
      "Epoch 2/10\n",
      "54000/54000 [==============================] - 31s 570us/step - loss: 0.3505 - acc: 0.8965 - val_loss: 0.1973 - val_acc: 0.9457\n",
      "Epoch 3/10\n",
      "54000/54000 [==============================] - 31s 571us/step - loss: 0.2379 - acc: 0.9323 - val_loss: 0.1671 - val_acc: 0.9557\n",
      "Epoch 4/10\n",
      "54000/54000 [==============================] - 31s 571us/step - loss: 0.1883 - acc: 0.9463 - val_loss: 0.1348 - val_acc: 0.9633\n",
      "Epoch 5/10\n",
      "54000/54000 [==============================] - 31s 574us/step - loss: 0.1596 - acc: 0.9549 - val_loss: 0.1179 - val_acc: 0.9672\n",
      "Epoch 6/10\n",
      "54000/54000 [==============================] - 31s 579us/step - loss: 0.1392 - acc: 0.9615 - val_loss: 0.1103 - val_acc: 0.9707\n",
      "Epoch 7/10\n",
      "54000/54000 [==============================] - 31s 573us/step - loss: 0.1217 - acc: 0.9657 - val_loss: 0.1062 - val_acc: 0.9713\n",
      "Epoch 8/10\n",
      "54000/54000 [==============================] - 31s 571us/step - loss: 0.1094 - acc: 0.9693 - val_loss: 0.1027 - val_acc: 0.9718\n",
      "Epoch 9/10\n",
      "54000/54000 [==============================] - 31s 571us/step - loss: 0.1002 - acc: 0.9715 - val_loss: 0.1485 - val_acc: 0.9603\n",
      "Epoch 10/10\n",
      "54000/54000 [==============================] - 31s 571us/step - loss: 0.0886 - acc: 0.9746 - val_loss: 0.1033 - val_acc: 0.9718\n",
      "10000/10000 [==============================] - 1s 124us/step\n",
      "10000/10000 [==============================] - 1s 126us/step\n",
      "10 : [0.088420898429770023, 0.97629999999999995]\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "54000/54000 [==============================] - 48s 896us/step - loss: 2.3081 - acc: 0.1017 - val_loss: 2.3012 - val_acc: 0.1152\n",
      "Epoch 2/10\n",
      "54000/54000 [==============================] - 44s 813us/step - loss: 2.3024 - acc: 0.1071 - val_loss: 2.3006 - val_acc: 0.1152\n",
      "Epoch 3/10\n",
      "54000/54000 [==============================] - 44s 812us/step - loss: 2.3023 - acc: 0.1062 - val_loss: 2.3002 - val_acc: 0.1152\n",
      "Epoch 4/10\n",
      "54000/54000 [==============================] - 44s 814us/step - loss: 2.3022 - acc: 0.1082 - val_loss: 2.2989 - val_acc: 0.1152\n",
      "Epoch 5/10\n",
      "54000/54000 [==============================] - 44s 812us/step - loss: 2.3023 - acc: 0.1096 - val_loss: 2.2974 - val_acc: 0.1152\n",
      "Epoch 6/10\n",
      "54000/54000 [==============================] - 44s 816us/step - loss: 2.3015 - acc: 0.1090 - val_loss: 2.2843 - val_acc: 0.1152\n",
      "Epoch 7/10\n",
      "54000/54000 [==============================] - 44s 822us/step - loss: 2.2990 - acc: 0.1100 - val_loss: 2.2024 - val_acc: 0.1152\n",
      "Epoch 8/10\n",
      "54000/54000 [==============================] - 44s 822us/step - loss: 2.0807 - acc: 0.1238 - val_loss: 1.9423 - val_acc: 0.1820\n",
      "Epoch 9/10\n",
      "54000/54000 [==============================] - 44s 821us/step - loss: 1.8223 - acc: 0.1808 - val_loss: 1.7450 - val_acc: 0.2065\n",
      "Epoch 10/10\n",
      "54000/54000 [==============================] - 44s 821us/step - loss: 1.7105 - acc: 0.2019 - val_loss: 1.7135 - val_acc: 0.2068\n",
      "10000/10000 [==============================] - 2s 166us/step\n",
      "10000/10000 [==============================] - 2s 168us/step\n",
      "15 : [1.714672702407837, 0.2069]\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "54000/54000 [==============================] - 61s 1ms/step - loss: 2.3071 - acc: 0.1014 - val_loss: 2.3022 - val_acc: 0.1078\n",
      "Epoch 2/10\n",
      "54000/54000 [==============================] - 55s 1ms/step - loss: 2.3028 - acc: 0.1021 - val_loss: 2.3023 - val_acc: 0.1017\n",
      "Epoch 3/10\n",
      "54000/54000 [==============================] - 55s 1ms/step - loss: 2.3025 - acc: 0.1023 - val_loss: 2.3023 - val_acc: 0.1017\n",
      "Epoch 4/10\n",
      "54000/54000 [==============================] - 55s 1ms/step - loss: 2.3025 - acc: 0.1048 - val_loss: 2.3023 - val_acc: 0.1017\n",
      "Epoch 5/10\n",
      "54000/54000 [==============================] - 55s 1ms/step - loss: 2.3025 - acc: 0.1075 - val_loss: 2.3023 - val_acc: 0.1022\n",
      "Epoch 6/10\n",
      "54000/54000 [==============================] - 55s 1ms/step - loss: 2.3025 - acc: 0.1054 - val_loss: 2.3023 - val_acc: 0.1017\n",
      "Epoch 7/10\n",
      "54000/54000 [==============================] - 55s 1ms/step - loss: 2.3026 - acc: 0.1066 - val_loss: 2.3023 - val_acc: 0.1027\n",
      "Epoch 8/10\n",
      "54000/54000 [==============================] - 55s 1ms/step - loss: 2.3025 - acc: 0.1034 - val_loss: 2.3022 - val_acc: 0.1108\n",
      "Epoch 9/10\n",
      "54000/54000 [==============================] - 55s 1ms/step - loss: 2.3025 - acc: 0.1038 - val_loss: 2.3021 - val_acc: 0.1328\n",
      "Epoch 10/10\n",
      "54000/54000 [==============================] - 55s 1ms/step - loss: 2.3024 - acc: 0.1064 - val_loss: 2.3018 - val_acc: 0.1337\n",
      "10000/10000 [==============================] - 2s 197us/step\n",
      "10000/10000 [==============================] - 2s 197us/step\n",
      "20 : [2.3014014907836913, 0.126]\n",
      "\n",
      "\n",
      "\n",
      "[10, [0.088420898429770023, 0.97629999999999995]]\n",
      "[15, [1.714672702407837, 0.2069]]\n",
      "[20, [2.3014014907836913, 0.126]]\n"
     ]
    }
   ],
   "source": [
    "record = []\n",
    "for n in [10, 15, 20]:\n",
    "    opt = SGD()\n",
    "    # Set a learning rate annealer\n",
    "#         learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=2, verbose=1, factor=0.7, min_lr=0.00001) \n",
    "    # Common params \n",
    "    epochs = 10\n",
    "    batch_size = 100\n",
    "    # Create and compile the model\n",
    "    model = model = create(e_swish_2, n)\n",
    "#     model.summary()\n",
    "    # Compile the model\n",
    "    model.compile(optimizer = opt , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    # Train the model\n",
    "    history = model.fit(X_train,Y_train, epochs = epochs, validation_data = (X_val,Y_val),\n",
    "                        verbose = 1)# , callbacks=[learning_rate_reduction])\n",
    "\n",
    "    record.append([n, model.evaluate(X_test, Y_test)])\n",
    "    print(n, \":\", model.evaluate(X_test, Y_test))\n",
    "    K.get_session().close()\n",
    "    K.set_session(K.tf.Session())\n",
    "\n",
    "    \n",
    "print()\n",
    "print()\n",
    "print()\n",
    "for r in record:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "54000/54000 [==============================] - 33s 614us/step - loss: 2.3019 - acc: 0.1122 - val_loss: 2.3016 - val_acc: 0.1152\n",
      "Epoch 2/10\n",
      "54000/54000 [==============================] - 28s 512us/step - loss: 2.3013 - acc: 0.1121 - val_loss: 2.3013 - val_acc: 0.1152\n",
      "Epoch 3/10\n",
      "54000/54000 [==============================] - 28s 513us/step - loss: 2.3010 - acc: 0.1121 - val_loss: 2.3012 - val_acc: 0.1152\n",
      "Epoch 4/10\n",
      "54000/54000 [==============================] - 29s 533us/step - loss: 2.3009 - acc: 0.1121 - val_loss: 2.3011 - val_acc: 0.1152\n",
      "Epoch 5/10\n",
      "54000/54000 [==============================] - 28s 526us/step - loss: 2.3008 - acc: 0.1121 - val_loss: 2.3010 - val_acc: 0.1152\n",
      "Epoch 6/10\n",
      "54000/54000 [==============================] - 29s 531us/step - loss: 2.3007 - acc: 0.1121 - val_loss: 2.3009 - val_acc: 0.1152\n",
      "Epoch 7/10\n",
      "54000/54000 [==============================] - 29s 531us/step - loss: 2.3006 - acc: 0.1121 - val_loss: 2.3008 - val_acc: 0.1152\n",
      "Epoch 8/10\n",
      "54000/54000 [==============================] - 29s 530us/step - loss: 2.3005 - acc: 0.1121 - val_loss: 2.3007 - val_acc: 0.1152\n",
      "Epoch 9/10\n",
      "54000/54000 [==============================] - 29s 531us/step - loss: 2.3003 - acc: 0.1121 - val_loss: 2.3005 - val_acc: 0.1152\n",
      "Epoch 10/10\n",
      "54000/54000 [==============================] - 29s 534us/step - loss: 2.3001 - acc: 0.1121 - val_loss: 2.3003 - val_acc: 0.1152\n",
      "10000/10000 [==============================] - 1s 118us/step\n",
      "10000/10000 [==============================] - 1s 115us/step\n",
      "10 : [2.2998147857666016, 0.1135]\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "54000/54000 [==============================] - 46s 849us/step - loss: 2.3020 - acc: 0.1119 - val_loss: 2.3016 - val_acc: 0.1152\n",
      "Epoch 2/10\n",
      "54000/54000 [==============================] - 40s 743us/step - loss: 2.3014 - acc: 0.1121 - val_loss: 2.3015 - val_acc: 0.1152\n",
      "Epoch 3/10\n",
      "54000/54000 [==============================] - 40s 746us/step - loss: 2.3012 - acc: 0.1121 - val_loss: 2.3015 - val_acc: 0.1152\n",
      "Epoch 4/10\n",
      "54000/54000 [==============================] - 41s 755us/step - loss: 2.3012 - acc: 0.1121 - val_loss: 2.3014 - val_acc: 0.1152\n",
      "Epoch 5/10\n",
      "54000/54000 [==============================] - 40s 739us/step - loss: 2.3012 - acc: 0.1121 - val_loss: 2.3014 - val_acc: 0.1152\n",
      "Epoch 6/10\n",
      "54000/54000 [==============================] - 40s 740us/step - loss: 2.3012 - acc: 0.1121 - val_loss: 2.3014 - val_acc: 0.1152\n",
      "Epoch 7/10\n",
      "17056/54000 [========>.....................] - ETA: 26s - loss: 2.3013 - acc: 0.1102"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-0b718b854783>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     history = model.fit(X_train,Y_train, epochs = epochs, validation_data = (X_val,Y_val),\n\u001b[1;32m---> 16\u001b[1;33m                         verbose = 1)# , callbacks=[learning_rate_reduction])\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mrecord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 960\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1657\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2357\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "record = []\n",
    "for n in [10, 15, 20]:\n",
    "    opt = SGD()\n",
    "    # Set a learning rate annealer\n",
    "#         learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=2, verbose=1, factor=0.7, min_lr=0.00001) \n",
    "    # Common params \n",
    "    epochs = 10\n",
    "    batch_size = 100\n",
    "    # Create and compile the model\n",
    "    model = model = create(swish, n)\n",
    "#     model.summary()\n",
    "    # Compile the model\n",
    "    model.compile(optimizer = opt , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    # Train the model\n",
    "    history = model.fit(X_train,Y_train, epochs = epochs, validation_data = (X_val,Y_val),\n",
    "                        verbose = 1)# , callbacks=[learning_rate_reduction])\n",
    "\n",
    "    record.append([n, model.evaluate(X_test, Y_test)])\n",
    "    print(n, \":\", model.evaluate(X_test, Y_test))\n",
    "    K.get_session().close()\n",
    "    K.set_session(K.tf.Session())\n",
    "\n",
    "    \n",
    "print()\n",
    "print()\n",
    "print()\n",
    "for r in record:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "54000/54000 [==============================] - 17s 318us/step - loss: 1.3883 - acc: 0.5310 - val_loss: 0.3938 - val_acc: 0.8885\n",
      "Epoch 2/10\n",
      "54000/54000 [==============================] - 16s 298us/step - loss: 0.3980 - acc: 0.8826 - val_loss: 0.2486 - val_acc: 0.9253\n",
      "Epoch 3/10\n",
      "54000/54000 [==============================] - 16s 298us/step - loss: 0.2686 - acc: 0.9203 - val_loss: 0.1871 - val_acc: 0.9410\n",
      "Epoch 4/10\n",
      "54000/54000 [==============================] - 16s 298us/step - loss: 0.2072 - acc: 0.9389 - val_loss: 0.1535 - val_acc: 0.9522\n",
      "Epoch 5/10\n",
      "54000/54000 [==============================] - 16s 298us/step - loss: 0.1737 - acc: 0.9489 - val_loss: 0.1416 - val_acc: 0.9557\n",
      "Epoch 6/10\n",
      "54000/54000 [==============================] - 16s 299us/step - loss: 0.1446 - acc: 0.9590 - val_loss: 0.1265 - val_acc: 0.9610\n",
      "Epoch 7/10\n",
      "54000/54000 [==============================] - 16s 299us/step - loss: 0.1263 - acc: 0.9622 - val_loss: 0.1084 - val_acc: 0.9655\n",
      "Epoch 8/10\n",
      "54000/54000 [==============================] - 16s 299us/step - loss: 0.1103 - acc: 0.9681 - val_loss: 0.1062 - val_acc: 0.9657\n",
      "Epoch 9/10\n",
      "54000/54000 [==============================] - 16s 298us/step - loss: 0.0977 - acc: 0.9713 - val_loss: 0.0949 - val_acc: 0.9712\n",
      "Epoch 10/10\n",
      "54000/54000 [==============================] - 16s 300us/step - loss: 0.0887 - acc: 0.9733 - val_loss: 0.0903 - val_acc: 0.9722\n",
      "10000/10000 [==============================] - 1s 79us/step\n",
      "6 : [0.08236231568926014, 0.97540000000000004]\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "54000/54000 [==============================] - 17s 309us/step - loss: 1.3875 - acc: 0.5261 - val_loss: 0.4243 - val_acc: 0.8598\n",
      "Epoch 2/10\n",
      "54000/54000 [==============================] - 16s 302us/step - loss: 0.3937 - acc: 0.8827 - val_loss: 0.2454 - val_acc: 0.9257\n",
      "Epoch 3/10\n",
      "54000/54000 [==============================] - 16s 303us/step - loss: 0.2684 - acc: 0.9205 - val_loss: 0.1934 - val_acc: 0.9420\n",
      "Epoch 4/10\n",
      "54000/54000 [==============================] - 16s 304us/step - loss: 0.2079 - acc: 0.9393 - val_loss: 0.1508 - val_acc: 0.9527\n",
      "Epoch 5/10\n",
      "54000/54000 [==============================] - 16s 304us/step - loss: 0.1742 - acc: 0.9488 - val_loss: 0.1397 - val_acc: 0.9580\n",
      "Epoch 6/10\n",
      "54000/54000 [==============================] - 16s 301us/step - loss: 0.1462 - acc: 0.9566 - val_loss: 0.1216 - val_acc: 0.9613\n",
      "Epoch 7/10\n",
      "54000/54000 [==============================] - 16s 304us/step - loss: 0.1276 - acc: 0.9624 - val_loss: 0.1115 - val_acc: 0.9652\n",
      "Epoch 8/10\n",
      "54000/54000 [==============================] - 16s 303us/step - loss: 0.1114 - acc: 0.9663 - val_loss: 0.1022 - val_acc: 0.9685\n",
      "Epoch 9/10\n",
      "54000/54000 [==============================] - 16s 301us/step - loss: 0.0986 - acc: 0.9709 - val_loss: 0.0939 - val_acc: 0.9702\n",
      "Epoch 10/10\n",
      "54000/54000 [==============================] - 16s 303us/step - loss: 0.0887 - acc: 0.9724 - val_loss: 0.0929 - val_acc: 0.9710\n",
      "10000/10000 [==============================] - 1s 81us/step\n",
      "8 : [0.085712986452970652, 0.97370000000000001]\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "54000/54000 [==============================] - 17s 313us/step - loss: 1.3841 - acc: 0.5296 - val_loss: 0.3977 - val_acc: 0.8775\n",
      "Epoch 2/10\n",
      "54000/54000 [==============================] - 17s 307us/step - loss: 0.3776 - acc: 0.8876 - val_loss: 0.2317 - val_acc: 0.9308\n",
      "Epoch 3/10\n",
      "54000/54000 [==============================] - 17s 307us/step - loss: 0.2558 - acc: 0.9254 - val_loss: 0.1773 - val_acc: 0.9455\n",
      "Epoch 4/10\n",
      "54000/54000 [==============================] - 17s 308us/step - loss: 0.1984 - acc: 0.9415 - val_loss: 0.1549 - val_acc: 0.9523\n",
      "Epoch 5/10\n",
      "54000/54000 [==============================] - 17s 307us/step - loss: 0.1631 - acc: 0.9516 - val_loss: 0.1368 - val_acc: 0.9592\n",
      "Epoch 6/10\n",
      "54000/54000 [==============================] - 17s 306us/step - loss: 0.1399 - acc: 0.9582 - val_loss: 0.1173 - val_acc: 0.9620\n",
      "Epoch 7/10\n",
      "54000/54000 [==============================] - 17s 308us/step - loss: 0.1203 - acc: 0.9636 - val_loss: 0.1020 - val_acc: 0.9688\n",
      "Epoch 8/10\n",
      "54000/54000 [==============================] - 17s 308us/step - loss: 0.1067 - acc: 0.9683 - val_loss: 0.0972 - val_acc: 0.9693\n",
      "Epoch 9/10\n",
      "54000/54000 [==============================] - 17s 307us/step - loss: 0.0967 - acc: 0.9715 - val_loss: 0.0905 - val_acc: 0.9722\n",
      "Epoch 10/10\n",
      "54000/54000 [==============================] - 17s 308us/step - loss: 0.0851 - acc: 0.9741 - val_loss: 0.0870 - val_acc: 0.9737\n",
      "10000/10000 [==============================] - 1s 80us/step\n",
      "10 : [0.086075643507856875, 0.9728]\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "54000/54000 [==============================] - 17s 313us/step - loss: 1.3505 - acc: 0.5374 - val_loss: 0.3901 - val_acc: 0.8862\n",
      "Epoch 2/10\n",
      "54000/54000 [==============================] - 17s 306us/step - loss: 0.3875 - acc: 0.8827 - val_loss: 0.2472 - val_acc: 0.9263\n",
      "Epoch 3/10\n",
      "54000/54000 [==============================] - 17s 306us/step - loss: 0.2666 - acc: 0.9212 - val_loss: 0.1896 - val_acc: 0.9418\n",
      "Epoch 4/10\n",
      "54000/54000 [==============================] - 17s 307us/step - loss: 0.2049 - acc: 0.9392 - val_loss: 0.1481 - val_acc: 0.9523\n",
      "Epoch 5/10\n",
      "54000/54000 [==============================] - 17s 307us/step - loss: 0.1687 - acc: 0.9492 - val_loss: 0.1352 - val_acc: 0.9573\n",
      "Epoch 6/10\n",
      "54000/54000 [==============================] - 17s 306us/step - loss: 0.1435 - acc: 0.9574 - val_loss: 0.1202 - val_acc: 0.9623\n",
      "Epoch 7/10\n",
      "54000/54000 [==============================] - 17s 306us/step - loss: 0.1277 - acc: 0.9623 - val_loss: 0.1094 - val_acc: 0.9660\n",
      "Epoch 8/10\n",
      "54000/54000 [==============================] - 17s 308us/step - loss: 0.1101 - acc: 0.9669 - val_loss: 0.0987 - val_acc: 0.9685\n",
      "Epoch 9/10\n",
      "54000/54000 [==============================] - 17s 306us/step - loss: 0.0967 - acc: 0.9710 - val_loss: 0.0950 - val_acc: 0.9710\n",
      "Epoch 10/10\n",
      "54000/54000 [==============================] - 17s 307us/step - loss: 0.0866 - acc: 0.9745 - val_loss: 0.0993 - val_acc: 0.9673\n",
      "10000/10000 [==============================] - 1s 77us/step\n",
      "12 : [0.08723578938604333, 0.97340000000000004]\n",
      "\n",
      "\n",
      "\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "54000/54000 [==============================] - 17s 316us/step - loss: 1.4136 - acc: 0.5188 - val_loss: 0.4078 - val_acc: 0.8870\n",
      "Epoch 2/10\n",
      "54000/54000 [==============================] - 17s 310us/step - loss: 0.3974 - acc: 0.8802 - val_loss: 0.2460 - val_acc: 0.9275\n",
      "Epoch 3/10\n",
      "54000/54000 [==============================] - 17s 308us/step - loss: 0.2676 - acc: 0.9209 - val_loss: 0.1851 - val_acc: 0.9448\n",
      "Epoch 4/10\n",
      "54000/54000 [==============================] - 17s 310us/step - loss: 0.2072 - acc: 0.9392 - val_loss: 0.1520 - val_acc: 0.9528\n",
      "Epoch 5/10\n",
      "54000/54000 [==============================] - 17s 308us/step - loss: 0.1680 - acc: 0.9499 - val_loss: 0.1297 - val_acc: 0.9595\n",
      "Epoch 6/10\n",
      "54000/54000 [==============================] - 17s 311us/step - loss: 0.1425 - acc: 0.9571 - val_loss: 0.1199 - val_acc: 0.9623\n",
      "Epoch 7/10\n",
      "54000/54000 [==============================] - 17s 309us/step - loss: 0.1254 - acc: 0.9631 - val_loss: 0.1053 - val_acc: 0.9663\n",
      "Epoch 8/10\n",
      "54000/54000 [==============================] - 17s 311us/step - loss: 0.1087 - acc: 0.9678 - val_loss: 0.1012 - val_acc: 0.9668\n",
      "Epoch 9/10\n",
      "54000/54000 [==============================] - 17s 310us/step - loss: 0.0957 - acc: 0.9715 - val_loss: 0.0955 - val_acc: 0.9698\n",
      "Epoch 10/10\n",
      "54000/54000 [==============================] - 17s 311us/step - loss: 0.0865 - acc: 0.9740 - val_loss: 0.0924 - val_acc: 0.9700\n",
      "10000/10000 [==============================] - 1s 76us/step\n",
      "6 : [0.086314778865128755, 0.97450000000000003]\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "54000/54000 [==============================] - 17s 321us/step - loss: 1.3803 - acc: 0.5260 - val_loss: 0.4145 - val_acc: 0.8832\n",
      "Epoch 2/10\n",
      "54000/54000 [==============================] - 17s 311us/step - loss: 0.3956 - acc: 0.8822 - val_loss: 0.2423 - val_acc: 0.9288\n",
      "Epoch 3/10\n",
      "54000/54000 [==============================] - 17s 310us/step - loss: 0.2680 - acc: 0.9209 - val_loss: 0.1857 - val_acc: 0.9427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "54000/54000 [==============================] - 16s 297us/step - loss: 0.2077 - acc: 0.9397 - val_loss: 0.1505 - val_acc: 0.9535\n",
      "Epoch 5/10\n",
      "54000/54000 [==============================] - 16s 296us/step - loss: 0.1726 - acc: 0.9479 - val_loss: 0.1328 - val_acc: 0.9582\n",
      "Epoch 6/10\n",
      "54000/54000 [==============================] - 16s 297us/step - loss: 0.1489 - acc: 0.9560 - val_loss: 0.1225 - val_acc: 0.9612\n",
      "Epoch 7/10\n",
      "54000/54000 [==============================] - 16s 297us/step - loss: 0.1257 - acc: 0.9632 - val_loss: 0.1154 - val_acc: 0.9637\n",
      "Epoch 8/10\n",
      "54000/54000 [==============================] - 16s 296us/step - loss: 0.1143 - acc: 0.9654 - val_loss: 0.1068 - val_acc: 0.9675\n",
      "Epoch 9/10\n",
      "54000/54000 [==============================] - 16s 298us/step - loss: 0.1007 - acc: 0.9698 - val_loss: 0.0956 - val_acc: 0.9683\n",
      "Epoch 10/10\n",
      "54000/54000 [==============================] - 16s 293us/step - loss: 0.0892 - acc: 0.9732 - val_loss: 0.0937 - val_acc: 0.9707\n",
      "10000/10000 [==============================] - 1s 77us/step\n",
      "8 : [0.08903080306346528, 0.97330000000000005]\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "54000/54000 [==============================] - 16s 301us/step - loss: 1.3818 - acc: 0.5329 - val_loss: 0.3778 - val_acc: 0.8907\n",
      "Epoch 2/10\n",
      "54000/54000 [==============================] - 16s 291us/step - loss: 0.3800 - acc: 0.8865 - val_loss: 0.2344 - val_acc: 0.9312\n",
      "Epoch 3/10\n",
      "54000/54000 [==============================] - 16s 291us/step - loss: 0.2587 - acc: 0.9244 - val_loss: 0.1794 - val_acc: 0.9440\n",
      "Epoch 4/10\n",
      "54000/54000 [==============================] - 16s 304us/step - loss: 0.1993 - acc: 0.9409 - val_loss: 0.1506 - val_acc: 0.9530\n",
      "Epoch 5/10\n",
      "54000/54000 [==============================] - 16s 303us/step - loss: 0.1654 - acc: 0.9507 - val_loss: 0.1361 - val_acc: 0.9590\n",
      "Epoch 6/10\n",
      "54000/54000 [==============================] - 16s 297us/step - loss: 0.1416 - acc: 0.9590 - val_loss: 0.1130 - val_acc: 0.9650\n",
      "Epoch 7/10\n",
      "54000/54000 [==============================] - 17s 307us/step - loss: 0.1228 - acc: 0.9637 - val_loss: 0.1056 - val_acc: 0.9678\n",
      "Epoch 8/10\n",
      "54000/54000 [==============================] - 16s 303us/step - loss: 0.1085 - acc: 0.9680 - val_loss: 0.1026 - val_acc: 0.9682\n",
      "Epoch 9/10\n",
      "54000/54000 [==============================] - 16s 299us/step - loss: 0.0969 - acc: 0.9710 - val_loss: 0.0980 - val_acc: 0.9688\n",
      "Epoch 10/10\n",
      "54000/54000 [==============================] - 16s 297us/step - loss: 0.0869 - acc: 0.9746 - val_loss: 0.0901 - val_acc: 0.9718\n",
      "10000/10000 [==============================] - 1s 78us/step\n",
      "10 : [0.082056643377337604, 0.97430000000000005]\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "54000/54000 [==============================] - 17s 306us/step - loss: 1.3690 - acc: 0.5260 - val_loss: 0.3903 - val_acc: 0.8893\n",
      "Epoch 2/10\n",
      "54000/54000 [==============================] - 16s 297us/step - loss: 0.3929 - acc: 0.8817 - val_loss: 0.2499 - val_acc: 0.9285\n",
      "Epoch 3/10\n",
      "54000/54000 [==============================] - 16s 296us/step - loss: 0.2698 - acc: 0.9203 - val_loss: 0.1857 - val_acc: 0.9443\n",
      "Epoch 4/10\n",
      "54000/54000 [==============================] - 16s 299us/step - loss: 0.2070 - acc: 0.9398 - val_loss: 0.1553 - val_acc: 0.9505\n",
      "Epoch 5/10\n",
      "54000/54000 [==============================] - 16s 299us/step - loss: 0.1724 - acc: 0.9490 - val_loss: 0.1320 - val_acc: 0.9587\n",
      "Epoch 6/10\n",
      "54000/54000 [==============================] - 16s 301us/step - loss: 0.1457 - acc: 0.9574 - val_loss: 0.1229 - val_acc: 0.9608\n",
      "Epoch 7/10\n",
      "54000/54000 [==============================] - 17s 307us/step - loss: 0.1254 - acc: 0.9626 - val_loss: 0.1084 - val_acc: 0.9655\n",
      "Epoch 8/10\n",
      "54000/54000 [==============================] - 16s 297us/step - loss: 0.1125 - acc: 0.9663 - val_loss: 0.1023 - val_acc: 0.9675\n",
      "Epoch 9/10\n",
      "54000/54000 [==============================] - 16s 298us/step - loss: 0.1000 - acc: 0.9699 - val_loss: 0.0951 - val_acc: 0.9695\n",
      "Epoch 10/10\n",
      "54000/54000 [==============================] - 16s 300us/step - loss: 0.0886 - acc: 0.9734 - val_loss: 0.0919 - val_acc: 0.9708\n",
      "10000/10000 [==============================] - 1s 79us/step\n",
      "12 : [0.091209545813687148, 0.97130000000000005]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# results = []\n",
    "# for act in [\"relu\", e_swish_2, swish]:\n",
    "#     record = []\n",
    "#     for n in [15, 20, 25, 30]:\n",
    "#         opt = SGD()\n",
    "#         # Set a learning rate annealer\n",
    "# #         learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=2, verbose=1, factor=0.7, min_lr=0.00001) \n",
    "#         # Common params \n",
    "#         epochs = 10\n",
    "#         batch_size = 100\n",
    "#         # Create and compile the model\n",
    "#         model = model = create(\"relu\", 5)\n",
    "#     #     model.summary()\n",
    "#         # Compile the model\n",
    "#         model.compile(optimizer = opt , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "#         # Train the model\n",
    "#         history = model.fit(X_train,Y_train, epochs = epochs, validation_data = (X_val,Y_val),\n",
    "#                             verbose = 0)# , callbacks=[learning_rate_reduction])\n",
    "\n",
    "#         record.append([n, model.evaluate(X_test, Y_test)])\n",
    "#         print(n, \":\", model.evaluate(X_test, Y_test))\n",
    "        \n",
    "#     results.append(record)\n",
    "    \n",
    "#     print()\n",
    "#     print()\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs_1 = model.predict_proba(X_test)\n",
    "scores = model.evaluate(X_test, Y_test)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
