{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import load_model\n",
    "from keras import regularizers, optimizers\n",
    "from keras.initializers import glorot_normal, RandomNormal, Zeros\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#z-score\n",
    "mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "std = np.std(x_train,axis=(0,1,2,3))\n",
    "x_train = (x_train-mean)/(std+1e-7)\n",
    "x_test = (x_test-mean)/(std+1e-7)\n",
    "\n",
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train,num_classes)# .reshape((y_train.shape[0], 10,1))\n",
    "y_test = np_utils.to_categorical(y_test,num_classes)# .reshape((y_test.shape[0], 10,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False\n",
    "    )\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swish activation function\n",
    "# x*sigmoid(x)\n",
    "def swish(x):\n",
    "    return x*K.sigmoid(x)\n",
    "\n",
    "# Custom activation function 1\n",
    "# mix between relu and positive part of swish mirrored across x=1\n",
    "def e_swish_1(x):\n",
    "    return K.maximum(0.0, x*(2-K.sigmoid(x)))\n",
    "\n",
    "# Custom activation function 2\n",
    "# positive part of swish mirrored across x=1\n",
    "def e_swish_2(x):\n",
    "    return K.maximum(x*K.sigmoid(x), x*(2-K.sigmoid(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e_swish_2', 'relu', 'swish', 'e_swish_1']\n"
     ]
    }
   ],
   "source": [
    "activations = [e_swish_2, \"relu\", swish, e_swish_1]\n",
    "\n",
    "names = activations[:]\n",
    "for i,a in enumerate(names):\n",
    "    if not isinstance(a, str):\n",
    "        names[i] = a.__name__\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(act, act_name):\n",
    "    nn = {\"act\": act, \"act_name\": act_name}\n",
    "    \n",
    "    weight_decay = 1e-2\n",
    "    s = 2\n",
    "    model = Sequential()\n",
    "\n",
    "    # Block 1\n",
    "    model.add(Conv2D(64, (3,3), padding='same', kernel_initializer=glorot_normal(), input_shape=x_train.shape[1:]))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    \n",
    "    # Block 2\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_initializer=glorot_normal()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 3\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_initializer=RandomNormal(stddev=0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 4\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_initializer=RandomNormal(stddev=0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    \n",
    "    # First Maxpooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=s))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    \n",
    "    # Block 5\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_initializer=RandomNormal(stddev=0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 6\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_initializer=glorot_normal()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 7\n",
    "    model.add(Conv2D(256, (3,3), padding='same', kernel_initializer=glorot_normal()))\n",
    "    # Second Maxpooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=s))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    \n",
    "    # Block 8\n",
    "    model.add(Conv2D(256, (3,3), padding='same', kernel_initializer=glorot_normal()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 9\n",
    "    model.add(Conv2D(256, (3,3), padding='same', kernel_initializer=glorot_normal()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    # Third Maxpooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=s))\n",
    "    \n",
    "    \n",
    "    # Block 10\n",
    "    model.add(Conv2D(512, (3,3), padding='same', kernel_initializer=glorot_normal()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Block 11  \n",
    "    model.add(Conv2D(2048, (1,1), padding='same', kernel_initializer=glorot_normal()))\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 12  \n",
    "    model.add(Conv2D(256, (1,1), padding='same', kernel_initializer=glorot_normal()))\n",
    "    model.add(Activation(act))\n",
    "    # Fourth Maxpooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=s))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "    # Block 13\n",
    "    model.add(Conv2D(256, (3,3), padding='same', kernel_initializer=glorot_normal()))\n",
    "    model.add(Activation(act))\n",
    "    # Fifth Maxpooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=s))\n",
    "\n",
    "    # Final Classifier\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    nn[\"model\"] = model\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 4, 4, 2048)        1050624   \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 4, 4, 2048)        0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4, 4, 2048)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 4, 4, 256)         524544    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 2, 2, 256)         590080    \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 5,497,226\n",
      "Trainable params: 5,493,258\n",
      "Non-trainable params: 3,968\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "391/390 [==============================] - 196s 502ms/step - loss: 1.7093 - acc: 0.3781 - val_loss: 1.9259 - val_acc: 0.3806\n",
      "Epoch 2/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 1.2360 - acc: 0.5561 - val_loss: 1.0901 - val_acc: 0.6232\n",
      "Epoch 3/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 1.0355 - acc: 0.6329 - val_loss: 0.8878 - val_acc: 0.6872\n",
      "Epoch 4/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.8990 - acc: 0.6834 - val_loss: 0.8388 - val_acc: 0.7068\n",
      "Epoch 5/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.8080 - acc: 0.7177 - val_loss: 1.0850 - val_acc: 0.6559\n",
      "Epoch 6/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.7409 - acc: 0.7420 - val_loss: 0.6944 - val_acc: 0.7608\n",
      "Epoch 7/50\n",
      "391/390 [==============================] - 189s 482ms/step - loss: 0.6933 - acc: 0.7579 - val_loss: 0.6338 - val_acc: 0.7898\n",
      "Epoch 8/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.6531 - acc: 0.7714 - val_loss: 0.5728 - val_acc: 0.8068\n",
      "Epoch 9/50\n",
      "391/390 [==============================] - 189s 482ms/step - loss: 0.6196 - acc: 0.7855 - val_loss: 0.5284 - val_acc: 0.8211\n",
      "Epoch 10/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.5861 - acc: 0.7966 - val_loss: 0.5290 - val_acc: 0.8224\n",
      "Epoch 11/50\n",
      "391/390 [==============================] - 189s 482ms/step - loss: 0.5597 - acc: 0.8078 - val_loss: 0.5900 - val_acc: 0.8020\n",
      "Epoch 12/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.5352 - acc: 0.8156 - val_loss: 0.5057 - val_acc: 0.8249\n",
      "Epoch 13/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.5170 - acc: 0.8181 - val_loss: 0.4949 - val_acc: 0.8307\n",
      "Epoch 14/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.4989 - acc: 0.8287 - val_loss: 0.5498 - val_acc: 0.8181\n",
      "Epoch 15/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.4775 - acc: 0.8335 - val_loss: 0.4346 - val_acc: 0.8502\n",
      "Epoch 16/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.4620 - acc: 0.8399 - val_loss: 0.4261 - val_acc: 0.8568\n",
      "Epoch 17/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.4539 - acc: 0.8431 - val_loss: 0.4459 - val_acc: 0.8475\n",
      "Epoch 18/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.4347 - acc: 0.8496 - val_loss: 0.4272 - val_acc: 0.8592\n",
      "Epoch 19/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.4190 - acc: 0.8565 - val_loss: 0.3780 - val_acc: 0.8743\n",
      "Epoch 20/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.4131 - acc: 0.8575 - val_loss: 0.3660 - val_acc: 0.8779\n",
      "Epoch 21/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.3972 - acc: 0.8622 - val_loss: 0.4272 - val_acc: 0.8581\n",
      "Epoch 22/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.3885 - acc: 0.8656 - val_loss: 0.4163 - val_acc: 0.8565\n",
      "Epoch 23/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.3769 - acc: 0.8708 - val_loss: 0.3823 - val_acc: 0.8709\n",
      "Epoch 24/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.3702 - acc: 0.8720 - val_loss: 0.4001 - val_acc: 0.8678\n",
      "Epoch 25/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.3602 - acc: 0.8753 - val_loss: 0.3638 - val_acc: 0.8812\n",
      "Epoch 26/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.3546 - acc: 0.8771 - val_loss: 0.3722 - val_acc: 0.8761\n",
      "Epoch 27/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.3471 - acc: 0.8787 - val_loss: 0.3305 - val_acc: 0.8903\n",
      "Epoch 28/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.3355 - acc: 0.8818 - val_loss: 0.3667 - val_acc: 0.8776\n",
      "Epoch 29/50\n",
      "391/390 [==============================] - 189s 482ms/step - loss: 0.3288 - acc: 0.8874 - val_loss: 0.3339 - val_acc: 0.8901\n",
      "Epoch 30/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.3212 - acc: 0.8879 - val_loss: 0.3447 - val_acc: 0.8836\n",
      "Epoch 31/50\n",
      "391/390 [==============================] - 189s 482ms/step - loss: 0.3174 - acc: 0.8903 - val_loss: 0.3644 - val_acc: 0.8800\n",
      "Epoch 32/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.3077 - acc: 0.8943 - val_loss: 0.3667 - val_acc: 0.8791\n",
      "Epoch 33/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.3060 - acc: 0.8932 - val_loss: 0.3666 - val_acc: 0.8772\n",
      "Epoch 34/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.2926 - acc: 0.8974 - val_loss: 0.3051 - val_acc: 0.9015\n",
      "Epoch 35/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.2898 - acc: 0.8984 - val_loss: 0.3253 - val_acc: 0.8964\n",
      "Epoch 36/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.2882 - acc: 0.9005 - val_loss: 0.3173 - val_acc: 0.8954\n",
      "Epoch 37/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.2764 - acc: 0.9030 - val_loss: 0.3083 - val_acc: 0.8986\n",
      "Epoch 38/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.2754 - acc: 0.9048 - val_loss: 0.3171 - val_acc: 0.8951\n",
      "Epoch 39/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.2745 - acc: 0.9048 - val_loss: 0.3057 - val_acc: 0.8995\n",
      "Epoch 40/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.2620 - acc: 0.9092 - val_loss: 0.2987 - val_acc: 0.9030\n",
      "Epoch 41/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.2577 - acc: 0.9109 - val_loss: 0.2827 - val_acc: 0.9047\n",
      "Epoch 42/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.2579 - acc: 0.9110 - val_loss: 0.3121 - val_acc: 0.8999\n",
      "Epoch 43/50\n",
      "391/390 [==============================] - 189s 482ms/step - loss: 0.2527 - acc: 0.9124 - val_loss: 0.3428 - val_acc: 0.8914\n",
      "Epoch 44/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.2467 - acc: 0.9149 - val_loss: 0.3026 - val_acc: 0.9045\n",
      "Epoch 45/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.2410 - acc: 0.9175 - val_loss: 0.2833 - val_acc: 0.9096\n",
      "Epoch 46/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.2380 - acc: 0.9172 - val_loss: 0.3247 - val_acc: 0.8954\n",
      "Epoch 47/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.2371 - acc: 0.9183 - val_loss: 0.2802 - val_acc: 0.9092\n",
      "Epoch 48/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.2343 - acc: 0.9177 - val_loss: 0.2771 - val_acc: 0.9099\n",
      "Epoch 49/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.2322 - acc: 0.9193 - val_loss: 0.2806 - val_acc: 0.9108\n",
      "Epoch 50/50\n",
      "391/390 [==============================] - 189s 483ms/step - loss: 0.2250 - acc: 0.9218 - val_loss: 0.2802 - val_acc: 0.9105\n",
      "{'val_loss': [1.9259260105133056, 1.0901305526733398, 0.88783046112060549, 0.83881420402526852, 1.0849640481948852, 0.69443629646301275, 0.63379247655868531, 0.57275502290725711, 0.5283574045181274, 0.52902177677154538, 0.58998520164489743, 0.50572686433792113, 0.49488935642242432, 0.54980609402656555, 0.43462147550582886, 0.42612168207168577, 0.44591784839630128, 0.42723766434192656, 0.37797044880390168, 0.36600477933883668, 0.42723121271133424, 0.41626608390808106, 0.38229578180313112, 0.40009470009803771, 0.36380790162086485, 0.3722088260650635, 0.33050941390991212, 0.36665001602172853, 0.33393696331977846, 0.34470767796039581, 0.36435041985511779, 0.36665977106094361, 0.36663218288421628, 0.30508174533843996, 0.32527927865982054, 0.31733915472030638, 0.30832635505199435, 0.31709011003971099, 0.30566669864654539, 0.29869229345321657, 0.28273810057640075, 0.31209020211696625, 0.34280241808891299, 0.30264284868240354, 0.2833498553276062, 0.32474232084751131, 0.28015727834701537, 0.27711316957473753, 0.28055761108398436, 0.28018948020935058], 'val_acc': [0.38059999999999999, 0.62319999999999998, 0.68720000000000003, 0.70679999999999998, 0.65590000000000004, 0.76080000000000003, 0.78979999999999995, 0.80679999999999996, 0.82110000000000005, 0.82240000000000002, 0.80200000000000005, 0.82489999999999997, 0.83069999999999999, 0.81810000000000005, 0.85019999999999996, 0.85680000000000001, 0.84750000000000003, 0.85919999999999996, 0.87429999999999997, 0.87790000000000001, 0.85809999999999997, 0.85650000000000004, 0.87090000000000001, 0.86780000000000002, 0.88119999999999998, 0.87609999999999999, 0.89029999999999998, 0.87760000000000005, 0.8901, 0.88360000000000005, 0.88, 0.87909999999999999, 0.87719999999999998, 0.90149999999999997, 0.89639999999999997, 0.89539999999999997, 0.89859999999999995, 0.89510000000000001, 0.89949999999999997, 0.90300000000000002, 0.90469999999999995, 0.89990000000000003, 0.89139999999999997, 0.90449999999999997, 0.90959999999999996, 0.89539999999999997, 0.90920000000000001, 0.90990000000000004, 0.91080000000000005, 0.91049999999999998], 'loss': [1.7104905633544922, 1.2371443897628784, 1.0361795064926147, 0.89935499828338628, 0.80784186296463012, 0.73989788572311399, 0.69324281959533696, 0.65306558881759647, 0.6193849905776978, 0.58602883607864376, 0.55932882328033451, 0.53459473596572871, 0.51689324777603152, 0.49886488525390627, 0.47731620111465456, 0.46204317353248597, 0.45339052130699159, 0.43457750525474548, 0.41935134993553164, 0.41389018691539764, 0.39744072835922239, 0.38844243469238282, 0.37706716281890867, 0.36981279587745669, 0.35957598680496217, 0.35490613488197326, 0.34699017009735106, 0.33563051915168762, 0.32897318202018738, 0.32149987680435183, 0.31693696605682375, 0.30770733855724336, 0.30543176061630251, 0.29290486250877379, 0.28894026743888857, 0.28822809392929077, 0.2763839961671829, 0.27579164357185365, 0.27438088152885437, 0.26253616291046145, 0.25772526176452637, 0.25786722778320315, 0.25262635850906373, 0.24631260746002198, 0.24094112790584565, 0.23779258285522462, 0.23699754324913025, 0.23399410426139833, 0.23190708829879761, 0.22546036017894744], 'acc': [0.37774000001907349, 0.55566000003814697, 0.63254000003814692, 0.68333999996185302, 0.71772000001907343, 0.74228000000000005, 0.75778000001907353, 0.77135999998092653, 0.78575999998092649, 0.79666000000000003, 0.80776000000000003, 0.81572000001907352, 0.81809999998092653, 0.82882000001907352, 0.83358000003814692, 0.83989999996185305, 0.84331999996185303, 0.84949999996185299, 0.85653999998092656, 0.85729999996185302, 0.86212000001907352, 0.86569999996185298, 0.870599999961853, 0.87209999999999999, 0.87564000001907349, 0.87705999999999995, 0.87866000001907352, 0.88183999999999996, 0.88741999999999999, 0.88779999998092651, 0.89034000000000002, 0.8943000000190735, 0.89333999996185298, 0.89732000000000001, 0.89863999999999999, 0.90062000003814702, 0.90302000000000004, 0.90464000001907352, 0.90475999996185308, 0.90903999996185303, 0.91086, 0.91105999996185305, 0.912480000038147, 0.91505999996185305, 0.91751999996185307, 0.91739999996185306, 0.91825999998092656, 0.91780000003814699, 0.91943999998092651, 0.92162000001907352]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "391/390 [==============================] - 193s 493ms/step - loss: 0.1927 - acc: 0.9326 - val_loss: 0.2567 - val_acc: 0.9149\n",
      "Epoch 2/30\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1840 - acc: 0.9351 - val_loss: 0.2507 - val_acc: 0.9156\n",
      "Epoch 3/30\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1774 - acc: 0.9381 - val_loss: 0.2570 - val_acc: 0.9156\n",
      "Epoch 4/30\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1751 - acc: 0.9382 - val_loss: 0.2500 - val_acc: 0.9190\n",
      "Epoch 5/30\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1726 - acc: 0.9394 - val_loss: 0.2479 - val_acc: 0.9217\n",
      "Epoch 6/30\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1717 - acc: 0.9398 - val_loss: 0.2548 - val_acc: 0.9188\n",
      "Epoch 7/30\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1677 - acc: 0.9406 - val_loss: 0.2620 - val_acc: 0.9132\n",
      "Epoch 8/30\n",
      "391/390 [==============================] - 189s 482ms/step - loss: 0.1690 - acc: 0.9405 - val_loss: 0.2558 - val_acc: 0.9182\n",
      "Epoch 9/30\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1636 - acc: 0.9437 - val_loss: 0.2558 - val_acc: 0.9172\n",
      "Epoch 10/30\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1624 - acc: 0.9427 - val_loss: 0.2543 - val_acc: 0.9186\n",
      "Epoch 11/30\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1665 - acc: 0.9422 - val_loss: 0.2542 - val_acc: 0.9172\n",
      "Epoch 12/30\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1593 - acc: 0.9446 - val_loss: 0.2477 - val_acc: 0.9214\n",
      "Epoch 13/30\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1568 - acc: 0.9448 - val_loss: 0.2429 - val_acc: 0.9232\n",
      "Epoch 14/30\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1541 - acc: 0.9454 - val_loss: 0.2528 - val_acc: 0.9211\n",
      "Epoch 15/30\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1530 - acc: 0.9470 - val_loss: 0.2507 - val_acc: 0.9190\n",
      "Epoch 16/30\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1482 - acc: 0.9482 - val_loss: 0.2601 - val_acc: 0.9188\n",
      "Epoch 17/30\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1501 - acc: 0.9469 - val_loss: 0.2587 - val_acc: 0.9170\n",
      "Epoch 18/30\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1474 - acc: 0.9486 - val_loss: 0.2495 - val_acc: 0.9190\n",
      "Epoch 19/30\n",
      "391/390 [==============================] - 188s 481ms/step - loss: 0.1464 - acc: 0.9490 - val_loss: 0.2591 - val_acc: 0.9185\n",
      "Epoch 20/30\n",
      "391/390 [==============================] - 188s 481ms/step - loss: 0.1472 - acc: 0.9492 - val_loss: 0.2545 - val_acc: 0.9169\n",
      "Epoch 21/30\n",
      "391/390 [==============================] - 188s 481ms/step - loss: 0.1445 - acc: 0.9501 - val_loss: 0.2510 - val_acc: 0.9224\n",
      "Epoch 22/30\n",
      "391/390 [==============================] - 188s 481ms/step - loss: 0.1424 - acc: 0.9499 - val_loss: 0.2461 - val_acc: 0.9214\n",
      "Epoch 23/30\n",
      "391/390 [==============================] - 188s 481ms/step - loss: 0.1452 - acc: 0.9494 - val_loss: 0.2658 - val_acc: 0.9160\n",
      "Epoch 24/30\n",
      "391/390 [==============================] - 188s 481ms/step - loss: 0.1388 - acc: 0.9524 - val_loss: 0.2496 - val_acc: 0.9216\n",
      "Epoch 25/30\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1405 - acc: 0.9507 - val_loss: 0.2637 - val_acc: 0.9190\n",
      "Epoch 26/30\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1380 - acc: 0.9523 - val_loss: 0.2453 - val_acc: 0.9234\n",
      "Epoch 27/30\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1332 - acc: 0.9534 - val_loss: 0.2495 - val_acc: 0.9271\n",
      "Epoch 28/30\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1349 - acc: 0.9526 - val_loss: 0.2480 - val_acc: 0.9241\n",
      "Epoch 29/30\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1349 - acc: 0.9527 - val_loss: 0.2477 - val_acc: 0.9228\n",
      "Epoch 30/30\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1355 - acc: 0.9533 - val_loss: 0.2459 - val_acc: 0.9215\n",
      "{'val_loss': [0.25670622930526732, 0.2506920605659485, 0.2570322861671448, 0.24995032482147217, 0.24792020115852356, 0.25483046464920045, 0.26199334192276003, 0.25575393385887146, 0.25578167934417723, 0.25433237800598146, 0.25416022162437441, 0.24770426959991454, 0.2429472041130066, 0.25278541975021362, 0.25071349415779115, 0.26012125740051267, 0.25867460644245149, 0.24947773661613465, 0.25910815315246583, 0.25449635801315307, 0.25102498233318327, 0.24611578054428102, 0.2657999581336975, 0.24955371644496918, 0.26370981616973876, 0.24533061189651489, 0.2494860679626465, 0.2480485725402832, 0.2477382166147232, 0.24590379362106324], 'val_acc': [0.91490000000000005, 0.91559999999999997, 0.91559999999999997, 0.91900000000000004, 0.92169999999999996, 0.91879999999999995, 0.91320000000000001, 0.91820000000000002, 0.91720000000000002, 0.91859999999999997, 0.91720000000000002, 0.9214, 0.92320000000000002, 0.92110000000000003, 0.91900000000000004, 0.91879999999999995, 0.91700000000000004, 0.91900000000000004, 0.91849999999999998, 0.91690000000000005, 0.9224, 0.9214, 0.91600000000000004, 0.92159999999999997, 0.91900000000000004, 0.9234, 0.92710000000000004, 0.92410000000000003, 0.92279999999999995, 0.92149999999999999], 'loss': [0.1927965018749237, 0.18390736070632935, 0.17743734913110734, 0.17496195791244507, 0.17294367081165313, 0.17183155316352844, 0.16739120824813844, 0.16892433804512025, 0.1636318714904785, 0.16241732149600982, 0.16649331682682036, 0.15932307819843292, 0.15669785648345946, 0.15434765105962753, 0.15299657359123231, 0.14834407618045806, 0.15006865736484529, 0.14747976341724395, 0.14606993640899657, 0.1471592768907547, 0.14447358125686646, 0.14238275676727294, 0.14511232049942016, 0.13895736628055572, 0.14047374668359758, 0.13831204537868499, 0.13350434133529662, 0.13500033627271651, 0.13487822558403015, 0.13543127392768861], 'acc': [0.93266000000000004, 0.93519999996185299, 0.93805999996185307, 0.9382600000190735, 0.93921999998092653, 0.93981999999999999, 0.94073999996185298, 0.940539999961853, 0.94367999996185303, 0.94271999996185307, 0.94221999996185302, 0.94464000001907344, 0.94486000001907344, 0.94532000001907346, 0.94701999998092656, 0.94820000001907345, 0.94690000001907348, 0.94859999996185307, 0.94911999999999996, 0.94918000003814695, 0.95004000003814693, 0.94991999998092647, 0.94944000001907347, 0.95241999998092652, 0.95068000003814701, 0.95213999998092647, 0.95321999998092655, 0.95265999996185302, 0.95274000001907344, 0.95341999998092652]}\n",
      "Epoch 1/25\n",
      "391/390 [==============================] - 194s 495ms/step - loss: 0.1259 - acc: 0.9558 - val_loss: 0.2508 - val_acc: 0.9246\n",
      "Epoch 2/25\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1237 - acc: 0.9572 - val_loss: 0.2541 - val_acc: 0.9209\n",
      "Epoch 3/25\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1240 - acc: 0.9572 - val_loss: 0.2420 - val_acc: 0.9241\n",
      "Epoch 4/25\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1189 - acc: 0.9577 - val_loss: 0.2487 - val_acc: 0.9250\n",
      "Epoch 5/25\n",
      "391/390 [==============================] - 189s 482ms/step - loss: 0.1245 - acc: 0.9569 - val_loss: 0.2477 - val_acc: 0.9217\n",
      "Epoch 6/25\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1208 - acc: 0.9574 - val_loss: 0.2458 - val_acc: 0.9230\n",
      "Epoch 7/25\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1207 - acc: 0.9583 - val_loss: 0.2549 - val_acc: 0.9208\n",
      "Epoch 8/25\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1172 - acc: 0.9591 - val_loss: 0.2482 - val_acc: 0.9211\n",
      "Epoch 9/25\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1169 - acc: 0.9592 - val_loss: 0.2494 - val_acc: 0.9227\n",
      "Epoch 10/25\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1181 - acc: 0.9592 - val_loss: 0.2597 - val_acc: 0.9218\n",
      "Epoch 11/25\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1151 - acc: 0.9598 - val_loss: 0.2566 - val_acc: 0.9211\n",
      "Epoch 12/25\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1135 - acc: 0.9602 - val_loss: 0.2457 - val_acc: 0.9224\n",
      "Epoch 13/25\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1132 - acc: 0.9600 - val_loss: 0.2690 - val_acc: 0.9172\n",
      "Epoch 14/25\n",
      "391/390 [==============================] - 189s 482ms/step - loss: 0.1138 - acc: 0.9607 - val_loss: 0.2522 - val_acc: 0.9237\n",
      "Epoch 15/25\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1092 - acc: 0.9621 - val_loss: 0.2557 - val_acc: 0.9235\n",
      "Epoch 16/25\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1106 - acc: 0.9618 - val_loss: 0.2461 - val_acc: 0.9221\n",
      "Epoch 17/25\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1117 - acc: 0.9614 - val_loss: 0.2449 - val_acc: 0.9224\n",
      "Epoch 18/25\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1073 - acc: 0.9628 - val_loss: 0.2505 - val_acc: 0.9204\n",
      "Epoch 19/25\n",
      "391/390 [==============================] - 189s 482ms/step - loss: 0.1092 - acc: 0.9624 - val_loss: 0.2536 - val_acc: 0.9208\n",
      "Epoch 20/25\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1073 - acc: 0.9620 - val_loss: 0.2515 - val_acc: 0.9210\n",
      "Epoch 21/25\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1082 - acc: 0.9631 - val_loss: 0.2494 - val_acc: 0.9244\n",
      "Epoch 22/25\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1073 - acc: 0.9617 - val_loss: 0.2495 - val_acc: 0.9241\n",
      "Epoch 23/25\n",
      "391/390 [==============================] - 189s 482ms/step - loss: 0.1053 - acc: 0.9643 - val_loss: 0.2514 - val_acc: 0.9232\n",
      "Epoch 24/25\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1070 - acc: 0.9626 - val_loss: 0.2541 - val_acc: 0.9229\n",
      "Epoch 25/25\n",
      "391/390 [==============================] - 189s 482ms/step - loss: 0.1023 - acc: 0.9648 - val_loss: 0.2537 - val_acc: 0.9226\n",
      "{'val_loss': [0.25077272162437442, 0.25408797631263735, 0.24202841930389404, 0.24873671188354493, 0.24770552215576172, 0.24584562792778014, 0.25486093645095825, 0.24817476992607115, 0.24937307147979737, 0.25967881631851197, 0.25655857214927674, 0.24572382278442384, 0.26903771076202393, 0.25224345493316652, 0.25565495204925537, 0.2461294063091278, 0.24488090591430664, 0.25052246055603028, 0.25357145376205442, 0.25154114599227906, 0.2493786862373352, 0.24953638563156127, 0.25144477481842042, 0.25410954465866087, 0.2536681616306305], 'val_acc': [0.92459999999999998, 0.92090000000000005, 0.92410000000000003, 0.92500000000000004, 0.92169999999999996, 0.92300000000000004, 0.92079999999999995, 0.92110000000000003, 0.92269999999999996, 0.92179999999999995, 0.92110000000000003, 0.9224, 0.91720000000000002, 0.92369999999999997, 0.92349999999999999, 0.92210000000000003, 0.9224, 0.9204, 0.92079999999999995, 0.92100000000000004, 0.9244, 0.92410000000000003, 0.92320000000000002, 0.92290000000000005, 0.92259999999999998], 'loss': [0.12606821450710295, 0.12377695114135742, 0.12420794323444366, 0.11890301896095276, 0.1245067180776596, 0.12073563578128815, 0.12080706866979599, 0.11711795222759247, 0.11677834965229035, 0.11798877720832825, 0.11523800038814545, 0.11343505038738251, 0.11319916749954223, 0.11422561402559281, 0.10941201115131378, 0.1105710624217987, 0.11157709204196931, 0.10676075511932373, 0.10932339758872986, 0.10757854577064514, 0.10833766788005829, 0.10708009522438049, 0.10522085664749145, 0.10698278599739075, 0.1021695538187027], 'acc': [0.95579999998092646, 0.95723999999999998, 0.95710000003814699, 0.95765999998092655, 0.95692000003814692, 0.95737999996185308, 0.95818000003814696, 0.95914000001907351, 0.95923999996185305, 0.95920000000000005, 0.95986000000000005, 0.96019999998092653, 0.96001999998092646, 0.96058000001907351, 0.96202000003814703, 0.96172000003814695, 0.96139999996185299, 0.96294000003814695, 0.96231999996185302, 0.96191999996185307, 0.96305999998092651, 0.96172000000000002, 0.96427999998092651, 0.96264000000000005, 0.96482000001907353]}\n",
      "Epoch 1/20\n",
      "391/390 [==============================] - 193s 494ms/step - loss: 0.1094 - acc: 0.9623 - val_loss: 0.2531 - val_acc: 0.9243\n",
      "Epoch 2/20\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1054 - acc: 0.9637 - val_loss: 0.2533 - val_acc: 0.9218\n",
      "Epoch 3/20\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1026 - acc: 0.9642 - val_loss: 0.2429 - val_acc: 0.9260\n",
      "Epoch 4/20\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1014 - acc: 0.9650 - val_loss: 0.2493 - val_acc: 0.9244\n",
      "Epoch 5/20\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.1008 - acc: 0.9652 - val_loss: 0.2483 - val_acc: 0.9230\n",
      "Epoch 6/20\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.0987 - acc: 0.9658 - val_loss: 0.2473 - val_acc: 0.9231\n",
      "Epoch 7/20\n",
      "391/390 [==============================] - 189s 482ms/step - loss: 0.0956 - acc: 0.9665 - val_loss: 0.2497 - val_acc: 0.9231\n",
      "Epoch 8/20\n",
      "391/390 [==============================] - 188s 481ms/step - loss: 0.0985 - acc: 0.9659 - val_loss: 0.2496 - val_acc: 0.9235\n",
      "Epoch 9/20\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.0984 - acc: 0.9655 - val_loss: 0.2512 - val_acc: 0.9234\n",
      "Epoch 10/20\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.0949 - acc: 0.9677 - val_loss: 0.2519 - val_acc: 0.9235\n",
      "Epoch 11/20\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.0931 - acc: 0.9674 - val_loss: 0.2427 - val_acc: 0.9250\n",
      "Epoch 12/20\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.0958 - acc: 0.9667 - val_loss: 0.2509 - val_acc: 0.9226\n",
      "Epoch 13/20\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.0936 - acc: 0.9675 - val_loss: 0.2549 - val_acc: 0.9246\n",
      "Epoch 14/20\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.0924 - acc: 0.9677 - val_loss: 0.2527 - val_acc: 0.9229\n",
      "Epoch 15/20\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.0958 - acc: 0.9663 - val_loss: 0.2561 - val_acc: 0.9233\n",
      "Epoch 16/20\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.0921 - acc: 0.9677 - val_loss: 0.2606 - val_acc: 0.9212\n",
      "Epoch 17/20\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.0889 - acc: 0.9695 - val_loss: 0.2578 - val_acc: 0.9198\n",
      "Epoch 18/20\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.0926 - acc: 0.9678 - val_loss: 0.2420 - val_acc: 0.9243\n",
      "Epoch 19/20\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.0903 - acc: 0.9680 - val_loss: 0.2464 - val_acc: 0.9210\n",
      "Epoch 20/20\n",
      "391/390 [==============================] - 188s 482ms/step - loss: 0.0879 - acc: 0.9694 - val_loss: 0.2624 - val_acc: 0.9207\n",
      "{'val_loss': [0.25310274281501771, 0.2533000569343567, 0.24285882992744445, 0.2492559874534607, 0.24830186553001404, 0.2473470950126648, 0.24966820011138915, 0.24963305521011353, 0.25117962703704833, 0.2518593649864197, 0.24270824785232545, 0.25093693437576292, 0.25487520017623899, 0.25273939938545226, 0.25613229565620421, 0.2605855456352234, 0.25775478320121764, 0.24196516962051393, 0.24635256724357604, 0.26240616540908812], 'val_acc': [0.92430000000000001, 0.92179999999999995, 0.92600000000000005, 0.9244, 0.92300000000000004, 0.92310000000000003, 0.92310000000000003, 0.92349999999999999, 0.9234, 0.92349999999999999, 0.92500000000000004, 0.92259999999999998, 0.92459999999999998, 0.92290000000000005, 0.92330000000000001, 0.92120000000000002, 0.91979999999999995, 0.92430000000000001, 0.92100000000000004, 0.92069999999999996], 'loss': [0.10943592274665832, 0.10551368620872498, 0.10268337077617645, 0.10136386226177216, 0.10082431794762611, 0.098556137814521785, 0.095394252073764799, 0.098576354895830151, 0.098505893864631655, 0.095003805017471313, 0.093089251196384434, 0.095875365130901335, 0.093758967475891117, 0.092301570029258734, 0.09585494473099708, 0.091939559845924376, 0.088841701359748837, 0.092297635223865512, 0.090323559484481813, 0.08788797111988067], 'acc': [0.96232000003814699, 0.96360000003814694, 0.96420000003814699, 0.96501999998092647, 0.96518000001907345, 0.96580000003814692, 0.96650000003814696, 0.96592, 0.96541999996185301, 0.96770000001907353, 0.96751999996185301, 0.966739999961853, 0.96744000001907349, 0.96778000003814701, 0.96632000001907348, 0.96775999998092654, 0.96954000001907348, 0.96782000003814694, 0.96807999998092653, 0.9693799999809265]}\n",
      "\n",
      " {'act': <function e_swish_2 at 0x7fd0fcb7a6a8>, 'act_name': 'e_swish_2', 'part_1': {'val_loss': [1.9259260105133056, 1.0901305526733398, 0.88783046112060549, 0.83881420402526852, 1.0849640481948852, 0.69443629646301275, 0.63379247655868531, 0.57275502290725711, 0.5283574045181274, 0.52902177677154538, 0.58998520164489743, 0.50572686433792113, 0.49488935642242432, 0.54980609402656555, 0.43462147550582886, 0.42612168207168577, 0.44591784839630128, 0.42723766434192656, 0.37797044880390168, 0.36600477933883668, 0.42723121271133424, 0.41626608390808106, 0.38229578180313112, 0.40009470009803771, 0.36380790162086485, 0.3722088260650635, 0.33050941390991212, 0.36665001602172853, 0.33393696331977846, 0.34470767796039581, 0.36435041985511779, 0.36665977106094361, 0.36663218288421628, 0.30508174533843996, 0.32527927865982054, 0.31733915472030638, 0.30832635505199435, 0.31709011003971099, 0.30566669864654539, 0.29869229345321657, 0.28273810057640075, 0.31209020211696625, 0.34280241808891299, 0.30264284868240354, 0.2833498553276062, 0.32474232084751131, 0.28015727834701537, 0.27711316957473753, 0.28055761108398436, 0.28018948020935058], 'val_acc': [0.38059999999999999, 0.62319999999999998, 0.68720000000000003, 0.70679999999999998, 0.65590000000000004, 0.76080000000000003, 0.78979999999999995, 0.80679999999999996, 0.82110000000000005, 0.82240000000000002, 0.80200000000000005, 0.82489999999999997, 0.83069999999999999, 0.81810000000000005, 0.85019999999999996, 0.85680000000000001, 0.84750000000000003, 0.85919999999999996, 0.87429999999999997, 0.87790000000000001, 0.85809999999999997, 0.85650000000000004, 0.87090000000000001, 0.86780000000000002, 0.88119999999999998, 0.87609999999999999, 0.89029999999999998, 0.87760000000000005, 0.8901, 0.88360000000000005, 0.88, 0.87909999999999999, 0.87719999999999998, 0.90149999999999997, 0.89639999999999997, 0.89539999999999997, 0.89859999999999995, 0.89510000000000001, 0.89949999999999997, 0.90300000000000002, 0.90469999999999995, 0.89990000000000003, 0.89139999999999997, 0.90449999999999997, 0.90959999999999996, 0.89539999999999997, 0.90920000000000001, 0.90990000000000004, 0.91080000000000005, 0.91049999999999998], 'loss': [1.7104905633544922, 1.2371443897628784, 1.0361795064926147, 0.89935499828338628, 0.80784186296463012, 0.73989788572311399, 0.69324281959533696, 0.65306558881759647, 0.6193849905776978, 0.58602883607864376, 0.55932882328033451, 0.53459473596572871, 0.51689324777603152, 0.49886488525390627, 0.47731620111465456, 0.46204317353248597, 0.45339052130699159, 0.43457750525474548, 0.41935134993553164, 0.41389018691539764, 0.39744072835922239, 0.38844243469238282, 0.37706716281890867, 0.36981279587745669, 0.35957598680496217, 0.35490613488197326, 0.34699017009735106, 0.33563051915168762, 0.32897318202018738, 0.32149987680435183, 0.31693696605682375, 0.30770733855724336, 0.30543176061630251, 0.29290486250877379, 0.28894026743888857, 0.28822809392929077, 0.2763839961671829, 0.27579164357185365, 0.27438088152885437, 0.26253616291046145, 0.25772526176452637, 0.25786722778320315, 0.25262635850906373, 0.24631260746002198, 0.24094112790584565, 0.23779258285522462, 0.23699754324913025, 0.23399410426139833, 0.23190708829879761, 0.22546036017894744], 'acc': [0.37774000001907349, 0.55566000003814697, 0.63254000003814692, 0.68333999996185302, 0.71772000001907343, 0.74228000000000005, 0.75778000001907353, 0.77135999998092653, 0.78575999998092649, 0.79666000000000003, 0.80776000000000003, 0.81572000001907352, 0.81809999998092653, 0.82882000001907352, 0.83358000003814692, 0.83989999996185305, 0.84331999996185303, 0.84949999996185299, 0.85653999998092656, 0.85729999996185302, 0.86212000001907352, 0.86569999996185298, 0.870599999961853, 0.87209999999999999, 0.87564000001907349, 0.87705999999999995, 0.87866000001907352, 0.88183999999999996, 0.88741999999999999, 0.88779999998092651, 0.89034000000000002, 0.8943000000190735, 0.89333999996185298, 0.89732000000000001, 0.89863999999999999, 0.90062000003814702, 0.90302000000000004, 0.90464000001907352, 0.90475999996185308, 0.90903999996185303, 0.91086, 0.91105999996185305, 0.912480000038147, 0.91505999996185305, 0.91751999996185307, 0.91739999996185306, 0.91825999998092656, 0.91780000003814699, 0.91943999998092651, 0.92162000001907352]}, 'part_2': {'val_loss': [0.25670622930526732, 0.2506920605659485, 0.2570322861671448, 0.24995032482147217, 0.24792020115852356, 0.25483046464920045, 0.26199334192276003, 0.25575393385887146, 0.25578167934417723, 0.25433237800598146, 0.25416022162437441, 0.24770426959991454, 0.2429472041130066, 0.25278541975021362, 0.25071349415779115, 0.26012125740051267, 0.25867460644245149, 0.24947773661613465, 0.25910815315246583, 0.25449635801315307, 0.25102498233318327, 0.24611578054428102, 0.2657999581336975, 0.24955371644496918, 0.26370981616973876, 0.24533061189651489, 0.2494860679626465, 0.2480485725402832, 0.2477382166147232, 0.24590379362106324], 'val_acc': [0.91490000000000005, 0.91559999999999997, 0.91559999999999997, 0.91900000000000004, 0.92169999999999996, 0.91879999999999995, 0.91320000000000001, 0.91820000000000002, 0.91720000000000002, 0.91859999999999997, 0.91720000000000002, 0.9214, 0.92320000000000002, 0.92110000000000003, 0.91900000000000004, 0.91879999999999995, 0.91700000000000004, 0.91900000000000004, 0.91849999999999998, 0.91690000000000005, 0.9224, 0.9214, 0.91600000000000004, 0.92159999999999997, 0.91900000000000004, 0.9234, 0.92710000000000004, 0.92410000000000003, 0.92279999999999995, 0.92149999999999999], 'loss': [0.1927965018749237, 0.18390736070632935, 0.17743734913110734, 0.17496195791244507, 0.17294367081165313, 0.17183155316352844, 0.16739120824813844, 0.16892433804512025, 0.1636318714904785, 0.16241732149600982, 0.16649331682682036, 0.15932307819843292, 0.15669785648345946, 0.15434765105962753, 0.15299657359123231, 0.14834407618045806, 0.15006865736484529, 0.14747976341724395, 0.14606993640899657, 0.1471592768907547, 0.14447358125686646, 0.14238275676727294, 0.14511232049942016, 0.13895736628055572, 0.14047374668359758, 0.13831204537868499, 0.13350434133529662, 0.13500033627271651, 0.13487822558403015, 0.13543127392768861], 'acc': [0.93266000000000004, 0.93519999996185299, 0.93805999996185307, 0.9382600000190735, 0.93921999998092653, 0.93981999999999999, 0.94073999996185298, 0.940539999961853, 0.94367999996185303, 0.94271999996185307, 0.94221999996185302, 0.94464000001907344, 0.94486000001907344, 0.94532000001907346, 0.94701999998092656, 0.94820000001907345, 0.94690000001907348, 0.94859999996185307, 0.94911999999999996, 0.94918000003814695, 0.95004000003814693, 0.94991999998092647, 0.94944000001907347, 0.95241999998092652, 0.95068000003814701, 0.95213999998092647, 0.95321999998092655, 0.95265999996185302, 0.95274000001907344, 0.95341999998092652]}, 'part_3': {'val_loss': [0.25077272162437442, 0.25408797631263735, 0.24202841930389404, 0.24873671188354493, 0.24770552215576172, 0.24584562792778014, 0.25486093645095825, 0.24817476992607115, 0.24937307147979737, 0.25967881631851197, 0.25655857214927674, 0.24572382278442384, 0.26903771076202393, 0.25224345493316652, 0.25565495204925537, 0.2461294063091278, 0.24488090591430664, 0.25052246055603028, 0.25357145376205442, 0.25154114599227906, 0.2493786862373352, 0.24953638563156127, 0.25144477481842042, 0.25410954465866087, 0.2536681616306305], 'val_acc': [0.92459999999999998, 0.92090000000000005, 0.92410000000000003, 0.92500000000000004, 0.92169999999999996, 0.92300000000000004, 0.92079999999999995, 0.92110000000000003, 0.92269999999999996, 0.92179999999999995, 0.92110000000000003, 0.9224, 0.91720000000000002, 0.92369999999999997, 0.92349999999999999, 0.92210000000000003, 0.9224, 0.9204, 0.92079999999999995, 0.92100000000000004, 0.9244, 0.92410000000000003, 0.92320000000000002, 0.92290000000000005, 0.92259999999999998], 'loss': [0.12606821450710295, 0.12377695114135742, 0.12420794323444366, 0.11890301896095276, 0.1245067180776596, 0.12073563578128815, 0.12080706866979599, 0.11711795222759247, 0.11677834965229035, 0.11798877720832825, 0.11523800038814545, 0.11343505038738251, 0.11319916749954223, 0.11422561402559281, 0.10941201115131378, 0.1105710624217987, 0.11157709204196931, 0.10676075511932373, 0.10932339758872986, 0.10757854577064514, 0.10833766788005829, 0.10708009522438049, 0.10522085664749145, 0.10698278599739075, 0.1021695538187027], 'acc': [0.95579999998092646, 0.95723999999999998, 0.95710000003814699, 0.95765999998092655, 0.95692000003814692, 0.95737999996185308, 0.95818000003814696, 0.95914000001907351, 0.95923999996185305, 0.95920000000000005, 0.95986000000000005, 0.96019999998092653, 0.96001999998092646, 0.96058000001907351, 0.96202000003814703, 0.96172000003814695, 0.96139999996185299, 0.96294000003814695, 0.96231999996185302, 0.96191999996185307, 0.96305999998092651, 0.96172000000000002, 0.96427999998092651, 0.96264000000000005, 0.96482000001907353]}, 'part_4': {'val_loss': [0.25310274281501771, 0.2533000569343567, 0.24285882992744445, 0.2492559874534607, 0.24830186553001404, 0.2473470950126648, 0.24966820011138915, 0.24963305521011353, 0.25117962703704833, 0.2518593649864197, 0.24270824785232545, 0.25093693437576292, 0.25487520017623899, 0.25273939938545226, 0.25613229565620421, 0.2605855456352234, 0.25775478320121764, 0.24196516962051393, 0.24635256724357604, 0.26240616540908812], 'val_acc': [0.92430000000000001, 0.92179999999999995, 0.92600000000000005, 0.9244, 0.92300000000000004, 0.92310000000000003, 0.92310000000000003, 0.92349999999999999, 0.9234, 0.92349999999999999, 0.92500000000000004, 0.92259999999999998, 0.92459999999999998, 0.92290000000000005, 0.92330000000000001, 0.92120000000000002, 0.91979999999999995, 0.92430000000000001, 0.92100000000000004, 0.92069999999999996], 'loss': [0.10943592274665832, 0.10551368620872498, 0.10268337077617645, 0.10136386226177216, 0.10082431794762611, 0.098556137814521785, 0.095394252073764799, 0.098576354895830151, 0.098505893864631655, 0.095003805017471313, 0.093089251196384434, 0.095875365130901335, 0.093758967475891117, 0.092301570029258734, 0.09585494473099708, 0.091939559845924376, 0.088841701359748837, 0.092297635223865512, 0.090323559484481813, 0.08788797111988067], 'acc': [0.96232000003814699, 0.96360000003814694, 0.96420000003814699, 0.96501999998092647, 0.96518000001907345, 0.96580000003814692, 0.96650000003814696, 0.96592, 0.96541999996185301, 0.96770000001907353, 0.96751999996185301, 0.966739999961853, 0.96744000001907349, 0.96778000003814701, 0.96632000001907348, 0.96775999998092654, 0.96954000001907348, 0.96782000003814694, 0.96807999998092653, 0.9693799999809265]}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_14 (Conv2D)           (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 4, 4, 2048)        1050624   \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 4, 4, 2048)        0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 4, 4, 2048)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 4, 4, 256)         524544    \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 2, 2, 256)         590080    \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 5,497,226\n",
      "Trainable params: 5,493,258\n",
      "Non-trainable params: 3,968\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "391/390 [==============================] - 157s 402ms/step - loss: 1.6582 - acc: 0.3745 - val_loss: 2.2243 - val_acc: 0.3159\n",
      "Epoch 2/50\n",
      "391/390 [==============================] - 151s 386ms/step - loss: 1.1791 - acc: 0.5737 - val_loss: 1.2346 - val_acc: 0.5827\n",
      "Epoch 3/50\n",
      "391/390 [==============================] - 151s 386ms/step - loss: 0.9488 - acc: 0.6614 - val_loss: 0.8211 - val_acc: 0.7068\n",
      "Epoch 4/50\n",
      "391/390 [==============================] - 151s 386ms/step - loss: 0.8179 - acc: 0.7119 - val_loss: 0.7118 - val_acc: 0.7528\n",
      "Epoch 5/50\n",
      "391/390 [==============================] - 151s 386ms/step - loss: 0.7319 - acc: 0.7447 - val_loss: 0.7585 - val_acc: 0.7341\n",
      "Epoch 6/50\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.6714 - acc: 0.7678 - val_loss: 0.7088 - val_acc: 0.7564\n",
      "Epoch 7/50\n",
      "391/390 [==============================] - 151s 386ms/step - loss: 0.6234 - acc: 0.7852 - val_loss: 0.5465 - val_acc: 0.8131\n",
      "Epoch 8/50\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.5859 - acc: 0.7981 - val_loss: 0.5339 - val_acc: 0.8182\n",
      "Epoch 9/50\n",
      "391/390 [==============================] - 151s 386ms/step - loss: 0.5513 - acc: 0.8122 - val_loss: 0.5366 - val_acc: 0.8177\n",
      "Epoch 10/50\n",
      "391/390 [==============================] - 151s 386ms/step - loss: 0.5207 - acc: 0.8208 - val_loss: 0.5037 - val_acc: 0.8242\n",
      "Epoch 11/50\n",
      "391/390 [==============================] - 151s 386ms/step - loss: 0.4967 - acc: 0.8285 - val_loss: 0.4280 - val_acc: 0.8565\n",
      "Epoch 12/50\n",
      "391/390 [==============================] - 151s 386ms/step - loss: 0.4733 - acc: 0.8361 - val_loss: 0.4766 - val_acc: 0.8396\n",
      "Epoch 13/50\n",
      "391/390 [==============================] - 151s 386ms/step - loss: 0.4579 - acc: 0.8422 - val_loss: 0.4803 - val_acc: 0.8374\n",
      "Epoch 14/50\n",
      "391/390 [==============================] - 151s 386ms/step - loss: 0.4399 - acc: 0.8493 - val_loss: 0.4780 - val_acc: 0.8384\n",
      "Epoch 15/50\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.4240 - acc: 0.8537 - val_loss: 0.3682 - val_acc: 0.8787\n",
      "Epoch 16/50\n",
      "391/390 [==============================] - 151s 386ms/step - loss: 0.4090 - acc: 0.8588 - val_loss: 0.3885 - val_acc: 0.8731\n",
      "Epoch 17/50\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.3987 - acc: 0.8624 - val_loss: 0.3536 - val_acc: 0.8785\n",
      "Epoch 18/50\n",
      "391/390 [==============================] - 150s 385ms/step - loss: 0.3828 - acc: 0.8682 - val_loss: 0.4483 - val_acc: 0.8495\n",
      "Epoch 19/50\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.3718 - acc: 0.8714 - val_loss: 0.3408 - val_acc: 0.8885\n",
      "Epoch 20/50\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.3668 - acc: 0.8741 - val_loss: 0.4017 - val_acc: 0.8592\n",
      "Epoch 21/50\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.3485 - acc: 0.8794 - val_loss: 0.4800 - val_acc: 0.8432\n",
      "Epoch 22/50\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.3433 - acc: 0.8824 - val_loss: 0.3298 - val_acc: 0.8881\n",
      "Epoch 23/50\n",
      "391/390 [==============================] - 150s 385ms/step - loss: 0.3347 - acc: 0.8834 - val_loss: 0.3517 - val_acc: 0.8789\n",
      "Epoch 24/50\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.3241 - acc: 0.8881 - val_loss: 0.3318 - val_acc: 0.8896\n",
      "Epoch 25/50\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.3181 - acc: 0.8891 - val_loss: 0.3169 - val_acc: 0.8932\n",
      "Epoch 26/50\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.3071 - acc: 0.8941 - val_loss: 0.3332 - val_acc: 0.8872\n",
      "Epoch 27/50\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.3024 - acc: 0.8960 - val_loss: 0.3794 - val_acc: 0.8702\n",
      "Epoch 28/50\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.2978 - acc: 0.8965 - val_loss: 0.3107 - val_acc: 0.8952\n",
      "Epoch 29/50\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.2899 - acc: 0.8998 - val_loss: 0.3480 - val_acc: 0.8825\n",
      "Epoch 30/50\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.2815 - acc: 0.9016 - val_loss: 0.3169 - val_acc: 0.8921\n",
      "Epoch 31/50\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.2754 - acc: 0.9051 - val_loss: 0.3043 - val_acc: 0.8974\n",
      "Epoch 32/50\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.2755 - acc: 0.9035 - val_loss: 0.2981 - val_acc: 0.9018\n",
      "Epoch 33/50\n",
      "391/390 [==============================] - 151s 386ms/step - loss: 0.2630 - acc: 0.9081 - val_loss: 0.2969 - val_acc: 0.9011\n",
      "Epoch 34/50\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.2646 - acc: 0.9083 - val_loss: 0.2905 - val_acc: 0.9023\n",
      "Epoch 35/50\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.2586 - acc: 0.9106 - val_loss: 0.2864 - val_acc: 0.9058\n",
      "Epoch 36/50\n",
      "391/390 [==============================] - 151s 386ms/step - loss: 0.2483 - acc: 0.9140 - val_loss: 0.3135 - val_acc: 0.8974\n",
      "Epoch 37/50\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.2475 - acc: 0.9125 - val_loss: 0.3046 - val_acc: 0.8994\n",
      "Epoch 38/50\n",
      "391/390 [==============================] - 151s 386ms/step - loss: 0.2448 - acc: 0.9163 - val_loss: 0.2834 - val_acc: 0.9021\n",
      "Epoch 39/50\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.2366 - acc: 0.9161 - val_loss: 0.3087 - val_acc: 0.8969\n",
      "Epoch 40/50\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.2310 - acc: 0.9198 - val_loss: 0.2949 - val_acc: 0.9006\n",
      "Epoch 41/50\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.2287 - acc: 0.9201 - val_loss: 0.2931 - val_acc: 0.9043\n",
      "Epoch 42/50\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.2254 - acc: 0.9226 - val_loss: 0.2738 - val_acc: 0.9081\n",
      "Epoch 43/50\n",
      "391/390 [==============================] - 151s 386ms/step - loss: 0.2207 - acc: 0.9229 - val_loss: 0.2810 - val_acc: 0.9059\n",
      "Epoch 44/50\n",
      "391/390 [==============================] - 150s 385ms/step - loss: 0.2130 - acc: 0.9273 - val_loss: 0.2663 - val_acc: 0.9136\n",
      "Epoch 45/50\n",
      "391/390 [==============================] - 150s 385ms/step - loss: 0.2111 - acc: 0.9266 - val_loss: 0.2767 - val_acc: 0.9057\n",
      "Epoch 46/50\n",
      "391/390 [==============================] - 150s 384ms/step - loss: 0.2098 - acc: 0.9279 - val_loss: 0.2871 - val_acc: 0.9058\n",
      "Epoch 47/50\n",
      "391/390 [==============================] - 150s 384ms/step - loss: 0.2078 - acc: 0.9274 - val_loss: 0.2784 - val_acc: 0.9095\n",
      "Epoch 48/50\n",
      "391/390 [==============================] - 150s 385ms/step - loss: 0.2099 - acc: 0.9267 - val_loss: 0.3061 - val_acc: 0.9017\n",
      "Epoch 49/50\n",
      "391/390 [==============================] - 150s 385ms/step - loss: 0.2025 - acc: 0.9300 - val_loss: 0.2579 - val_acc: 0.9125\n",
      "Epoch 50/50\n",
      "391/390 [==============================] - 150s 385ms/step - loss: 0.1963 - acc: 0.9316 - val_loss: 0.2492 - val_acc: 0.9154\n",
      "{'val_loss': [2.2243431022644042, 1.2346007600784301, 0.82111042938232426, 0.71182781581878662, 0.75847160091400145, 0.7088296018600464, 0.54648199810981746, 0.53386342496871952, 0.53664860897064204, 0.50372551136016841, 0.42798257522583005, 0.47660221729278562, 0.48026397509574892, 0.4779810816526413, 0.36822868156433103, 0.38846632394790648, 0.35355527267456055, 0.44833977727890012, 0.34084624786376955, 0.40170226345062254, 0.48000436544418335, 0.329809424495697, 0.35165872926712038, 0.33176762714385988, 0.31691898698806764, 0.33321869397163389, 0.37942534360885621, 0.31071373696327209, 0.3480212206840515, 0.31687290990352629, 0.30426845693588256, 0.29809732894897462, 0.29692436285018919, 0.29047600855827332, 0.28644314947128297, 0.31353294711112978, 0.30456420373916626, 0.28343406953811645, 0.30874851212501525, 0.29494585843086241, 0.29309044084548952, 0.27384915809631349, 0.28101299848556521, 0.26630651969909669, 0.27674867277145387, 0.28712308645248413, 0.2783505976676941, 0.30614695286750793, 0.25786398377418518, 0.24919021854400636], 'val_acc': [0.31590000000000001, 0.5827, 0.70679999999999998, 0.75280000000000002, 0.73409999999999997, 0.75639999999999996, 0.81310000000000004, 0.81820000000000004, 0.81769999999999998, 0.82420000000000004, 0.85650000000000004, 0.83960000000000001, 0.83740000000000003, 0.83840000000000003, 0.87870000000000004, 0.87309999999999999, 0.87849999999999995, 0.84950000000000003, 0.88849999999999996, 0.85919999999999996, 0.84319999999999995, 0.8881, 0.87890000000000001, 0.88959999999999995, 0.89319999999999999, 0.88719999999999999, 0.87019999999999997, 0.8952, 0.88249999999999995, 0.8921, 0.89739999999999998, 0.90180000000000005, 0.90110000000000001, 0.90229999999999999, 0.90580000000000005, 0.89739999999999998, 0.89939999999999998, 0.90210000000000001, 0.89690000000000003, 0.90059999999999996, 0.90429999999999999, 0.90810000000000002, 0.90590000000000004, 0.91359999999999997, 0.90569999999999995, 0.90580000000000005, 0.90949999999999998, 0.90169999999999995, 0.91249999999999998, 0.91539999999999999], 'loss': [1.659529600906372, 1.1803012458038331, 0.94903196670532231, 0.81813800230026246, 0.73240719829559331, 0.67117280534744261, 0.62335643646240235, 0.58549073767662052, 0.55074725078582765, 0.52116078901290896, 0.49726245031356814, 0.47396393945693971, 0.45798238019943238, 0.43981456901550292, 0.42410997738838196, 0.40864540095329283, 0.39925037778854372, 0.38276374477386477, 0.37145686120986937, 0.36683092349052432, 0.34871869933128358, 0.3437724404478073, 0.33465227394104002, 0.32457722562789915, 0.31790974779129028, 0.30686024953842161, 0.30180474927902223, 0.2977779056072235, 0.28979882848739624, 0.28195438849449156, 0.27548639164924621, 0.27588021730899809, 0.26308416038990023, 0.264097689037323, 0.25852099589347838, 0.24830424611091614, 0.24723213229179383, 0.24469644054412842, 0.23673967043876648, 0.2312675382041931, 0.22842645764350891, 0.22546956903457641, 0.22060722851753234, 0.2130914257478714, 0.21091215568542482, 0.20970408078193664, 0.20752763863563536, 0.20995211159706115, 0.20264985271453859, 0.19627891091823577], 'acc': [0.37393999999046323, 0.57326000003814692, 0.6613199999809265, 0.71190000003814702, 0.74461999998092654, 0.76763999996185306, 0.78524000003814698, 0.79826000001907349, 0.81249999996185307, 0.82053999998092653, 0.82826000001907352, 0.83584000001907344, 0.84217999998092652, 0.84948000000000001, 0.85370000001907353, 0.85906000000000005, 0.86227999998092653, 0.86831999996185305, 0.87142000001907349, 0.87419999996185305, 0.87929999999999997, 0.88229999996185304, 0.88344000003814693, 0.88796000003814701, 0.88936000000000004, 0.89420000003814692, 0.89620000003814693, 0.89631999998092649, 0.89978000003814695, 0.90152000003814692, 0.90508000003814693, 0.90329999996185306, 0.9080600000190735, 0.90840000003814703, 0.91056000003814697, 0.91403999999999996, 0.91253999996185298, 0.91632000003814695, 0.9160999999809265, 0.91966000001907344, 0.92022000001907345, 0.92270000003814701, 0.92293999998092646, 0.92722000003814697, 0.92649999998092647, 0.92788000000000004, 0.92749999999999999, 0.92669999999999997, 0.92991999996185304, 0.93172000003814692]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "391/390 [==============================] - 156s 400ms/step - loss: 0.1706 - acc: 0.9409 - val_loss: 0.2421 - val_acc: 0.9197\n",
      "Epoch 2/30\n",
      "391/390 [==============================] - 150s 385ms/step - loss: 0.1599 - acc: 0.9434 - val_loss: 0.2408 - val_acc: 0.9229\n",
      "Epoch 3/30\n",
      "391/390 [==============================] - 150s 385ms/step - loss: 0.1558 - acc: 0.9457 - val_loss: 0.2502 - val_acc: 0.9190\n",
      "Epoch 4/30\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1530 - acc: 0.9457 - val_loss: 0.2340 - val_acc: 0.9242\n",
      "Epoch 5/30\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1521 - acc: 0.9454 - val_loss: 0.2481 - val_acc: 0.9177\n",
      "Epoch 6/30\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1514 - acc: 0.9468 - val_loss: 0.2463 - val_acc: 0.9212\n",
      "Epoch 7/30\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1493 - acc: 0.9489 - val_loss: 0.2483 - val_acc: 0.9198\n",
      "Epoch 8/30\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1509 - acc: 0.9479 - val_loss: 0.2331 - val_acc: 0.9248\n",
      "Epoch 9/30\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1413 - acc: 0.9513 - val_loss: 0.2314 - val_acc: 0.9243\n",
      "Epoch 10/30\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1474 - acc: 0.9481 - val_loss: 0.2378 - val_acc: 0.9241\n",
      "Epoch 11/30\n",
      "391/390 [==============================] - 150s 385ms/step - loss: 0.1411 - acc: 0.9521 - val_loss: 0.2335 - val_acc: 0.9246\n",
      "Epoch 12/30\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1385 - acc: 0.9525 - val_loss: 0.2336 - val_acc: 0.9202\n",
      "Epoch 13/30\n",
      "391/390 [==============================] - 150s 385ms/step - loss: 0.1445 - acc: 0.9501 - val_loss: 0.2388 - val_acc: 0.9236\n",
      "Epoch 14/30\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1346 - acc: 0.9533 - val_loss: 0.2479 - val_acc: 0.9214\n",
      "Epoch 15/30\n",
      "391/390 [==============================] - 150s 385ms/step - loss: 0.1354 - acc: 0.9535 - val_loss: 0.2359 - val_acc: 0.9231\n",
      "Epoch 16/30\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1343 - acc: 0.9540 - val_loss: 0.2319 - val_acc: 0.9234\n",
      "Epoch 17/30\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1362 - acc: 0.9530 - val_loss: 0.2306 - val_acc: 0.9236\n",
      "Epoch 18/30\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1316 - acc: 0.9541 - val_loss: 0.2426 - val_acc: 0.9206\n",
      "Epoch 19/30\n",
      "391/390 [==============================] - 150s 385ms/step - loss: 0.1327 - acc: 0.9532 - val_loss: 0.2206 - val_acc: 0.9284\n",
      "Epoch 20/30\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1297 - acc: 0.9552 - val_loss: 0.2356 - val_acc: 0.9221\n",
      "Epoch 21/30\n",
      "391/390 [==============================] - 150s 385ms/step - loss: 0.1328 - acc: 0.9542 - val_loss: 0.2301 - val_acc: 0.9225\n",
      "Epoch 22/30\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1314 - acc: 0.9556 - val_loss: 0.2216 - val_acc: 0.9279\n",
      "Epoch 23/30\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1255 - acc: 0.9560 - val_loss: 0.2280 - val_acc: 0.9251\n",
      "Epoch 24/30\n",
      "391/390 [==============================] - 150s 385ms/step - loss: 0.1274 - acc: 0.9563 - val_loss: 0.2524 - val_acc: 0.9219\n",
      "Epoch 25/30\n",
      "391/390 [==============================] - 150s 385ms/step - loss: 0.1247 - acc: 0.9578 - val_loss: 0.2293 - val_acc: 0.9250\n",
      "Epoch 26/30\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1242 - acc: 0.9569 - val_loss: 0.2288 - val_acc: 0.9251\n",
      "Epoch 27/30\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1257 - acc: 0.9565 - val_loss: 0.2352 - val_acc: 0.9250\n",
      "Epoch 28/30\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1217 - acc: 0.9579 - val_loss: 0.2419 - val_acc: 0.9228\n",
      "Epoch 29/30\n",
      "391/390 [==============================] - 150s 385ms/step - loss: 0.1222 - acc: 0.9579 - val_loss: 0.2476 - val_acc: 0.9231\n",
      "Epoch 30/30\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1232 - acc: 0.9586 - val_loss: 0.2408 - val_acc: 0.9222\n",
      "{'val_loss': [0.24208446426391603, 0.24077992639541626, 0.25018725533485414, 0.23401111135482788, 0.24810904757380486, 0.24626069469451906, 0.2483254460811615, 0.23308508322238922, 0.23142802014350891, 0.23782422363758088, 0.23345917816162109, 0.23357124180793762, 0.2387777265071869, 0.24787405748367308, 0.23591543724536895, 0.23190798864364623, 0.23057176761627196, 0.24260770282745361, 0.22055496859550477, 0.2356174066543579, 0.23012672810554505, 0.22161635127067567, 0.22798151364326477, 0.25241847162246706, 0.22933466954231263, 0.22882126305103301, 0.23520413522720338, 0.24185513246059417, 0.24763635144233703, 0.24081270687580109], 'val_acc': [0.91969999999999996, 0.92290000000000005, 0.91900000000000004, 0.92420000000000002, 0.91769999999999996, 0.92120000000000002, 0.91979999999999995, 0.92479999999999996, 0.92430000000000001, 0.92410000000000003, 0.92459999999999998, 0.92020000000000002, 0.92359999999999998, 0.9214, 0.92310000000000003, 0.9234, 0.92359999999999998, 0.92059999999999997, 0.9284, 0.92210000000000003, 0.92249999999999999, 0.92789999999999995, 0.92510000000000003, 0.92190000000000005, 0.92500000000000004, 0.92510000000000003, 0.92500000000000004, 0.92279999999999995, 0.92310000000000003, 0.92220000000000002], 'loss': [0.17082127536296846, 0.16015227002620697, 0.15580480632781982, 0.15330148583054543, 0.15184432391166686, 0.15136846158981324, 0.14959728111743928, 0.15062410049438477, 0.14123302718639374, 0.14723393487453459, 0.14109764512062073, 0.1384378641152382, 0.14457808813810349, 0.13449639783859252, 0.13552219920873643, 0.1338723044347763, 0.13625862650394441, 0.13181392085313798, 0.13279769809722899, 0.12964346807956695, 0.1327502475643158, 0.13125500650405883, 0.12529988319396973, 0.12755709438323976, 0.12468335056781769, 0.12391983367919922, 0.12585994328975678, 0.12191103803873062, 0.12225276635169983, 0.12317001496791839], 'acc': [0.94079999996185304, 0.94332000001907346, 0.94567999996185304, 0.94568000003814701, 0.94549999996185308, 0.94690000003814701, 0.94879999999999998, 0.94794000001907353, 0.95125999999999999, 0.94825999996185306, 0.952079999961853, 0.95250000003814694, 0.95007999996185299, 0.9533999999809265, 0.95350000001907353, 0.95403999996185307, 0.95299999996185303, 0.95401999998092646, 0.95323999999999998, 0.95525999998092648, 0.95423999998092657, 0.95569999999999999, 0.95608000001907345, 0.9562799999809265, 0.95774000003814697, 0.9570599999809265, 0.95648, 0.95779999998092646, 0.95789999998092656, 0.95864000000000005]}\n",
      "Epoch 1/25\n",
      "391/390 [==============================] - 156s 399ms/step - loss: 0.1189 - acc: 0.9592 - val_loss: 0.2227 - val_acc: 0.9273\n",
      "Epoch 2/25\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1171 - acc: 0.9603 - val_loss: 0.2304 - val_acc: 0.9281\n",
      "Epoch 3/25\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1177 - acc: 0.9603 - val_loss: 0.2358 - val_acc: 0.9240\n",
      "Epoch 4/25\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1127 - acc: 0.9606 - val_loss: 0.2396 - val_acc: 0.9202\n",
      "Epoch 5/25\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1107 - acc: 0.9620 - val_loss: 0.2208 - val_acc: 0.9274\n",
      "Epoch 6/25\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1106 - acc: 0.9622 - val_loss: 0.2276 - val_acc: 0.9261\n",
      "Epoch 7/25\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1093 - acc: 0.9629 - val_loss: 0.2241 - val_acc: 0.9294\n",
      "Epoch 8/25\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1099 - acc: 0.9620 - val_loss: 0.2417 - val_acc: 0.9221\n",
      "Epoch 9/25\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1083 - acc: 0.9628 - val_loss: 0.2336 - val_acc: 0.9243\n",
      "Epoch 10/25\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1077 - acc: 0.9628 - val_loss: 0.2250 - val_acc: 0.9309\n",
      "Epoch 11/25\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1088 - acc: 0.9624 - val_loss: 0.2272 - val_acc: 0.9281\n",
      "Epoch 12/25\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1086 - acc: 0.9628 - val_loss: 0.2257 - val_acc: 0.9296\n",
      "Epoch 13/25\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1079 - acc: 0.9643 - val_loss: 0.2319 - val_acc: 0.9269\n",
      "Epoch 14/25\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1048 - acc: 0.9636 - val_loss: 0.2209 - val_acc: 0.9279\n",
      "Epoch 15/25\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1095 - acc: 0.9635 - val_loss: 0.2227 - val_acc: 0.9256\n",
      "Epoch 16/25\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1052 - acc: 0.9654 - val_loss: 0.2383 - val_acc: 0.9272\n",
      "Epoch 17/25\n",
      "391/390 [==============================] - 151s 386ms/step - loss: 0.1070 - acc: 0.9633 - val_loss: 0.2395 - val_acc: 0.9247\n",
      "Epoch 18/25\n",
      "391/390 [==============================] - 151s 386ms/step - loss: 0.1030 - acc: 0.9650 - val_loss: 0.2399 - val_acc: 0.9236\n",
      "Epoch 19/25\n",
      "391/390 [==============================] - 151s 386ms/step - loss: 0.1050 - acc: 0.9654 - val_loss: 0.2359 - val_acc: 0.9285\n",
      "Epoch 20/25\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1023 - acc: 0.9660 - val_loss: 0.2235 - val_acc: 0.9283\n",
      "Epoch 21/25\n",
      "391/390 [==============================] - 150s 385ms/step - loss: 0.1005 - acc: 0.9658 - val_loss: 0.2366 - val_acc: 0.9265\n",
      "Epoch 22/25\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1011 - acc: 0.9661 - val_loss: 0.2253 - val_acc: 0.9276\n",
      "Epoch 23/25\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1023 - acc: 0.9656 - val_loss: 0.2372 - val_acc: 0.9241\n",
      "Epoch 24/25\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1029 - acc: 0.9651 - val_loss: 0.2420 - val_acc: 0.9215\n",
      "Epoch 25/25\n",
      "391/390 [==============================] - 151s 385ms/step - loss: 0.1013 - acc: 0.9653 - val_loss: 0.2256 - val_acc: 0.9299\n",
      "{'val_loss': [0.22267325606346131, 0.23036972253322602, 0.23579582600593568, 0.23960086083412171, 0.22081027193069458, 0.2275534086227417, 0.22408299174308777, 0.24171359729766845, 0.23362920827865599, 0.22500821604728699, 0.22716033687591553, 0.22566603589057924, 0.23186066439151765, 0.22092660958766938, 0.22267607471942902, 0.23826915106773378, 0.23947471456527711, 0.23989546695947647, 0.2359130136013031, 0.22351960487365721, 0.23661012806892395, 0.22526346249580384, 0.23720552511215209, 0.24203000004291533, 0.22558919113874434], 'val_acc': [0.92730000000000001, 0.92810000000000004, 0.92400000000000004, 0.92020000000000002, 0.9274, 0.92610000000000003, 0.9294, 0.92210000000000003, 0.92430000000000001, 0.93089999999999995, 0.92810000000000004, 0.92959999999999998, 0.92689999999999995, 0.92789999999999995, 0.92559999999999998, 0.92720000000000002, 0.92469999999999997, 0.92359999999999998, 0.92849999999999999, 0.92830000000000001, 0.92649999999999999, 0.92759999999999998, 0.92410000000000003, 0.92149999999999999, 0.92989999999999995], 'loss': [0.11883676035404206, 0.11720638843536377, 0.11767813926935196, 0.11246658491969108, 0.11066826381683349, 0.11080864270448684, 0.1096319920039177, 0.10972117706775665, 0.1084241161108017, 0.10779341729164124, 0.10870937635421753, 0.10813221879959106, 0.10788612795352935, 0.10445113217830658, 0.10938031097173691, 0.10526379567623138, 0.10701598144054413, 0.10266165648937225, 0.10520394329786301, 0.1021603741979599, 0.10029212020397187, 0.10095782131433487, 0.10229621625781059, 0.10315327554345131, 0.10119527832508088], 'acc': [0.95920000001907346, 0.96026000003814693, 0.96026000003814693, 0.9607, 0.96207999998092653, 0.96214000003814693, 0.96281999996185308, 0.96211999999999998, 0.96278000003814701, 0.96282000003814694, 0.96235999996185306, 0.96292, 0.96425999996185308, 0.96372000000000002, 0.96334000003814702, 0.96535999996185307, 0.96336000001907351, 0.96513999996185307, 0.96526000001907353, 0.96603999996185308, 0.96591999998092648, 0.96608000001907346, 0.96560000001907353, 0.96500000003814701, 0.96536000001907352]}\n",
      "Epoch 1/20\n",
      "208/390 [===============>..............] - ETA: 1:11 - loss: 0.1073 - acc: 0.9642"
     ]
    }
   ],
   "source": [
    "for i, act in enumerate(activations):\n",
    "    batch_size = 128\n",
    "    nn = create_model(act, names[i])\n",
    "    \n",
    "    nn[\"model\"].summary()\n",
    "    model = nn[\"model\"]\n",
    "\n",
    "    # First training for 50 epochs\n",
    "    epochs = 25*2\n",
    "    opt_adm = keras.optimizers.Adadelta()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt_adm, metrics=['accuracy'])\n",
    "    part_1 = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,epochs=epochs,verbose=1,validation_data=(x_test,y_test))\n",
    "    nn[\"part_1\"] = part_1.history\n",
    "    print(nn[\"part_1\"])\n",
    "    model.save_weights(\"simplenet_generic_\"+nn[\"act_name\"]+\"_\"+str(epochs)+\".h5\")\n",
    "\n",
    "    # Training for 30 epochs more\n",
    "    epochs = 30\n",
    "    opt_adm = keras.optimizers.Adadelta(lr=0.7, rho=0.9)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt_adm, metrics=['accuracy'])\n",
    "    part_2= model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,epochs=epochs,verbose=1,validation_data=(x_test,y_test))\n",
    "    nn[\"part_2\"] = part_2.history\n",
    "    print(nn[\"part_2\"])\n",
    "    model.save_weights(\"simplenet_generic_\"+nn[\"act_name\"]+\"_\"+str(epochs)+\".h5\")\n",
    "\n",
    "    # First training for 25 epochs\n",
    "    epochs = 25\n",
    "    opt_adm = keras.optimizers.Adadelta(lr=0.6, rho=0.90)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt_adm, metrics=['accuracy'])\n",
    "    part_3 = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,epochs=epochs,verbose=1,validation_data=(x_test,y_test))\n",
    "    nn[\"part_3\"] = part_3.history\n",
    "    print(nn[\"part_3\"])\n",
    "    model.save_weights(\"simplenet_generic_\"+nn[\"act_name\"]+\"_\"+str(epochs)+\".h5\")\n",
    "\n",
    "    # First training for 50 epochs\n",
    "    epochs = 20\n",
    "    opt_adm = keras.optimizers.Adadelta(lr=0.5, rho=0.95)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt_adm, metrics=['accuracy'])\n",
    "    part_4 = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,epochs=epochs,verbose=1,validation_data=(x_test,y_test))\n",
    "    nn[\"part_4\"] = part_4.history\n",
    "    print(nn[\"part_4\"])\n",
    "    model.save_weights(\"simplenet_generic_\"+nn[\"act_name\"]+\"_\"+str(epochs)+\".h5\")\n",
    "\n",
    "    del nn[\"model\"]\n",
    "    print(\"\\n\", nn) # Ensure everything's ok x2\n",
    "    models.append(nn)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 4, 4, 2048)        1050624   \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 4, 4, 2048)        0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4, 4, 2048)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 4, 4, 256)         524544    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 2, 2, 256)         590080    \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 5,497,226\n",
      "Trainable params: 5,493,258\n",
      "Non-trainable params: 3,968\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "391/390 [==============================] - 167s 428ms/step - loss: 1.5640 - acc: 0.4142 - val_loss: 1.2702 - val_acc: 0.5552\n",
      "Epoch 2/50\n",
      "391/390 [==============================] - 160s 409ms/step - loss: 1.0431 - acc: 0.6258 - val_loss: 0.9133 - val_acc: 0.6901\n",
      "Epoch 3/50\n",
      "391/390 [==============================] - 160s 410ms/step - loss: 0.8358 - acc: 0.7057 - val_loss: 0.8122 - val_acc: 0.7194\n",
      "Epoch 4/50\n",
      "391/390 [==============================] - 160s 409ms/step - loss: 0.7297 - acc: 0.7438 - val_loss: 0.6388 - val_acc: 0.7833\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/390 [==============================] - 160s 410ms/step - loss: 0.6597 - acc: 0.7696 - val_loss: 0.5688 - val_acc: 0.8058\n",
      "Epoch 6/50\n",
      "391/390 [==============================] - 160s 410ms/step - loss: 0.6042 - acc: 0.7904 - val_loss: 0.5182 - val_acc: 0.8245\n",
      "Epoch 7/50\n",
      "391/390 [==============================] - 160s 410ms/step - loss: 0.5649 - acc: 0.8050 - val_loss: 0.5157 - val_acc: 0.8249\n",
      "Epoch 8/50\n",
      "391/390 [==============================] - 160s 409ms/step - loss: 0.5258 - acc: 0.8155 - val_loss: 0.5515 - val_acc: 0.8138\n",
      "Epoch 9/50\n",
      "391/390 [==============================] - 160s 410ms/step - loss: 0.4970 - acc: 0.8281 - val_loss: 0.4384 - val_acc: 0.8509\n",
      "Epoch 10/50\n",
      "391/390 [==============================] - 160s 409ms/step - loss: 0.4697 - acc: 0.8379 - val_loss: 0.3877 - val_acc: 0.8694\n",
      "Epoch 11/50\n",
      "391/390 [==============================] - 160s 410ms/step - loss: 0.4474 - acc: 0.8457 - val_loss: 0.3947 - val_acc: 0.8660\n",
      "Epoch 12/50\n",
      "391/390 [==============================] - 160s 409ms/step - loss: 0.4239 - acc: 0.8537 - val_loss: 0.4409 - val_acc: 0.8502\n",
      "Epoch 13/50\n",
      "391/390 [==============================] - 160s 409ms/step - loss: 0.4073 - acc: 0.8583 - val_loss: 0.3631 - val_acc: 0.8755\n",
      "Epoch 14/50\n",
      "391/390 [==============================] - 160s 410ms/step - loss: 0.3942 - acc: 0.8615 - val_loss: 0.3597 - val_acc: 0.8783\n",
      "Epoch 15/50\n",
      "391/390 [==============================] - 160s 409ms/step - loss: 0.3785 - acc: 0.8681 - val_loss: 0.3464 - val_acc: 0.8827\n",
      "Epoch 16/50\n",
      "391/390 [==============================] - 160s 409ms/step - loss: 0.3677 - acc: 0.8726 - val_loss: 0.3645 - val_acc: 0.8781\n",
      "Epoch 17/50\n",
      "391/390 [==============================] - 160s 410ms/step - loss: 0.3546 - acc: 0.8751 - val_loss: 0.3350 - val_acc: 0.8859\n",
      "Epoch 18/50\n",
      "391/390 [==============================] - 160s 409ms/step - loss: 0.3418 - acc: 0.8821 - val_loss: 0.3230 - val_acc: 0.8909\n",
      "Epoch 19/50\n",
      "391/390 [==============================] - 160s 409ms/step - loss: 0.3332 - acc: 0.8849 - val_loss: 0.3237 - val_acc: 0.8882\n",
      "Epoch 20/50\n",
      "391/390 [==============================] - 160s 409ms/step - loss: 0.3172 - acc: 0.8891 - val_loss: 0.3238 - val_acc: 0.8922\n",
      "Epoch 21/50\n",
      "391/390 [==============================] - 160s 409ms/step - loss: 0.3129 - acc: 0.8912 - val_loss: 0.3163 - val_acc: 0.8930\n",
      "Epoch 22/50\n",
      "391/390 [==============================] - 160s 410ms/step - loss: 0.3002 - acc: 0.8937 - val_loss: 0.3151 - val_acc: 0.8938\n",
      "Epoch 23/50\n",
      "391/390 [==============================] - 160s 410ms/step - loss: 0.2964 - acc: 0.8968 - val_loss: 0.3039 - val_acc: 0.8964\n",
      "Epoch 24/50\n",
      "391/390 [==============================] - 160s 409ms/step - loss: 0.2894 - acc: 0.8977 - val_loss: 0.3063 - val_acc: 0.8946\n",
      "Epoch 25/50\n",
      "391/390 [==============================] - 160s 410ms/step - loss: 0.2793 - acc: 0.9017 - val_loss: 0.3006 - val_acc: 0.8982\n",
      "Epoch 26/50\n",
      "391/390 [==============================] - 160s 409ms/step - loss: 0.2748 - acc: 0.9039 - val_loss: 0.2802 - val_acc: 0.9040\n",
      "Epoch 27/50\n",
      "391/390 [==============================] - 160s 409ms/step - loss: 0.2680 - acc: 0.9059 - val_loss: 0.3140 - val_acc: 0.8910\n",
      "Epoch 28/50\n",
      "391/390 [==============================] - 160s 409ms/step - loss: 0.2641 - acc: 0.9081 - val_loss: 0.2751 - val_acc: 0.9081\n",
      "Epoch 29/50\n",
      "391/390 [==============================] - 160s 410ms/step - loss: 0.2580 - acc: 0.9096 - val_loss: 0.2891 - val_acc: 0.9017\n",
      "Epoch 30/50\n",
      "391/390 [==============================] - 160s 410ms/step - loss: 0.2486 - acc: 0.9133 - val_loss: 0.2783 - val_acc: 0.9088\n",
      "Epoch 31/50\n",
      "391/390 [==============================] - 160s 410ms/step - loss: 0.2428 - acc: 0.9157 - val_loss: 0.2819 - val_acc: 0.9054\n",
      "Epoch 32/50\n",
      "391/390 [==============================] - 160s 410ms/step - loss: 0.2335 - acc: 0.9190 - val_loss: 0.2783 - val_acc: 0.9058\n",
      "Epoch 33/50\n",
      "391/390 [==============================] - 160s 409ms/step - loss: 0.2309 - acc: 0.9198 - val_loss: 0.2693 - val_acc: 0.9121\n",
      "Epoch 34/50\n",
      "391/390 [==============================] - 160s 410ms/step - loss: 0.2265 - acc: 0.9201 - val_loss: 0.2739 - val_acc: 0.9075\n",
      "Epoch 35/50\n",
      "391/390 [==============================] - 160s 409ms/step - loss: 0.2278 - acc: 0.9218 - val_loss: 0.2697 - val_acc: 0.9127\n",
      "Epoch 36/50\n",
      "391/390 [==============================] - 160s 410ms/step - loss: 0.2206 - acc: 0.9219 - val_loss: 0.2636 - val_acc: 0.9146\n",
      "Epoch 37/50\n",
      "391/390 [==============================] - 160s 410ms/step - loss: 0.2144 - acc: 0.9253 - val_loss: 0.2786 - val_acc: 0.9066\n",
      "Epoch 38/50\n",
      "391/390 [==============================] - 160s 410ms/step - loss: 0.2117 - acc: 0.9257 - val_loss: 0.2669 - val_acc: 0.9122\n",
      "Epoch 39/50\n",
      "391/390 [==============================] - 160s 410ms/step - loss: 0.2045 - acc: 0.9277 - val_loss: 0.2577 - val_acc: 0.9134\n",
      "Epoch 40/50\n",
      "391/390 [==============================] - 160s 410ms/step - loss: 0.2001 - acc: 0.9291 - val_loss: 0.2661 - val_acc: 0.9127\n",
      "Epoch 41/50\n",
      "391/390 [==============================] - 160s 410ms/step - loss: 0.2005 - acc: 0.9305 - val_loss: 0.2658 - val_acc: 0.9121\n",
      "Epoch 42/50\n",
      "391/390 [==============================] - 160s 410ms/step - loss: 0.1937 - acc: 0.9309 - val_loss: 0.2548 - val_acc: 0.9168\n",
      "Epoch 43/50\n",
      "391/390 [==============================] - 160s 410ms/step - loss: 0.1901 - acc: 0.9328 - val_loss: 0.2554 - val_acc: 0.9169\n",
      "Epoch 44/50\n",
      "391/390 [==============================] - 160s 409ms/step - loss: 0.1913 - acc: 0.9324 - val_loss: 0.2443 - val_acc: 0.9199\n",
      "Epoch 45/50\n",
      "391/390 [==============================] - 160s 410ms/step - loss: 0.1862 - acc: 0.9346 - val_loss: 0.2485 - val_acc: 0.9188\n",
      "Epoch 46/50\n",
      "391/390 [==============================] - 160s 409ms/step - loss: 0.1826 - acc: 0.9363 - val_loss: 0.2425 - val_acc: 0.9183\n",
      "Epoch 47/50\n",
      "391/390 [==============================] - 160s 410ms/step - loss: 0.1798 - acc: 0.9375 - val_loss: 0.2556 - val_acc: 0.9171\n",
      "Epoch 48/50\n",
      "391/390 [==============================] - 160s 410ms/step - loss: 0.1760 - acc: 0.9374 - val_loss: 0.2591 - val_acc: 0.9129\n",
      "Epoch 49/50\n",
      "391/390 [==============================] - 160s 410ms/step - loss: 0.1772 - acc: 0.9382 - val_loss: 0.2784 - val_acc: 0.9104\n",
      "Epoch 50/50\n",
      "391/390 [==============================] - 160s 410ms/step - loss: 0.1735 - acc: 0.9394 - val_loss: 0.2377 - val_acc: 0.9220\n",
      "{'val_loss': [1.2701581336975099, 0.91327721881866453, 0.81222170372009272, 0.63881399259567262, 0.56879725151062011, 0.51824476156234744, 0.51574898700714111, 0.5515461269378662, 0.4383820701599121, 0.38769240112304687, 0.39469577107429504, 0.44091899018287661, 0.36313070917129514, 0.35967272009849549, 0.34635223903656004, 0.36449624381065371, 0.33495946302413943, 0.32296953873634338, 0.32365637059211733, 0.32380144996643068, 0.31632360115051267, 0.31512007145881654, 0.30394305286407469, 0.30629115829467773, 0.30061295180320741, 0.28021751627922059, 0.31400390510559084, 0.27507554187774658, 0.28906013638973238, 0.27831463375091553, 0.28190137290954592, 0.27826591367721559, 0.26930967502593994, 0.27388806262016296, 0.26972672195434572, 0.26363159089088439, 0.27860556306838991, 0.26694255905151365, 0.25770274538993837, 0.26614451293945313, 0.26579484412670135, 0.25478542919158936, 0.25541529564857485, 0.24426183116436004, 0.24854100377559663, 0.24249597134590148, 0.25556202796697619, 0.25907791318893431, 0.27836094093322755, 0.23770171065926551], 'val_acc': [0.55520000000000003, 0.69010000000000005, 0.71940000000000004, 0.7833, 0.80579999999999996, 0.82450000000000001, 0.82489999999999997, 0.81379999999999997, 0.85089999999999999, 0.86939999999999995, 0.86599999999999999, 0.85019999999999996, 0.87549999999999994, 0.87829999999999997, 0.88270000000000004, 0.87809999999999999, 0.88590000000000002, 0.89090000000000003, 0.88819999999999999, 0.89219999999999999, 0.89300000000000002, 0.89380000000000004, 0.89639999999999997, 0.89459999999999995, 0.8982, 0.90400000000000003, 0.89100000000000001, 0.90810000000000002, 0.90169999999999995, 0.90880000000000005, 0.90539999999999998, 0.90580000000000005, 0.91210000000000002, 0.90749999999999997, 0.91269999999999996, 0.91459999999999997, 0.90659999999999996, 0.91220000000000001, 0.91339999999999999, 0.91269999999999996, 0.91210000000000002, 0.91679999999999995, 0.91690000000000005, 0.91990000000000005, 0.91879999999999995, 0.91830000000000001, 0.91710000000000003, 0.91290000000000004, 0.91039999999999999, 0.92200000000000004], 'loss': [1.5655533922576905, 1.0437860546875, 0.83588436601638794, 0.72905835903167726, 0.65993852535247799, 0.6041579549026489, 0.56492108266830443, 0.52602087579727175, 0.49708531688690183, 0.46960222829818726, 0.44717367591857909, 0.42371566161155699, 0.4075660409832001, 0.39372524418830873, 0.37844584791183472, 0.3674258336830139, 0.35458202135086059, 0.34180035437583922, 0.33282590187072753, 0.31772252531051637, 0.31266809029579162, 0.3002648435688019, 0.29623459081649778, 0.28935702361583709, 0.27964639157295229, 0.27488453128814699, 0.26766438993453978, 0.26450425645828246, 0.25772493649482725, 0.24895228392601013, 0.24242073478698731, 0.23331637282848358, 0.23107292614936828, 0.2264687239933014, 0.22782241739749909, 0.22069336145401, 0.2144524573469162, 0.21151282247066497, 0.20444761207342149, 0.2002938065814972, 0.20070483724594115, 0.19373612801551818, 0.19001771080970764, 0.19128688548564912, 0.18631839373588563, 0.18240826516151429, 0.17963779053211212, 0.17596865605831147, 0.17681159091949464, 0.17319551641941069], 'acc': [0.41341999998092649, 0.62541999998092657, 0.70547999998092648, 0.74408000001907348, 0.76948000001907346, 0.79042000000000001, 0.80496000001907353, 0.81545999996185303, 0.82816000001907353, 0.837959999961853, 0.84577999998092657, 0.8538, 0.85826000001907343, 0.86174000001907347, 0.86816000003814697, 0.87268000000000001, 0.87508000001907349, 0.88211999996185297, 0.88510000003814693, 0.88887999998092648, 0.89125999998092653, 0.89378000001907354, 0.89688000000000001, 0.89778000001907343, 0.90149999996185304, 0.90403999998092655, 0.90609999996185298, 0.90800000001907344, 0.90957999996185301, 0.9132800000190735, 0.9157599999809265, 0.91897999996185298, 0.91968000003814698, 0.92018000003814693, 0.92179999998092654, 0.92188000001907344, 0.92531999998092651, 0.925659999961853, 0.92789999996185302, 0.92903999998092657, 0.93041999996185298, 0.93090000001907347, 0.93280000001907348, 0.93247999999999998, 0.93452000000000002, 0.93633999998092654, 0.93750000003814693, 0.93735999998092656, 0.93833999999999995, 0.93947999999999998]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "391/390 [==============================] - 165s 421ms/step - loss: 0.1402 - acc: 0.9512 - val_loss: 0.2259 - val_acc: 0.9258\n",
      "Epoch 2/30\n",
      "391/390 [==============================] - 160s 410ms/step - loss: 0.1316 - acc: 0.9533 - val_loss: 0.2298 - val_acc: 0.9282\n",
      "Epoch 3/30\n",
      "391/390 [==============================] - 160s 410ms/step - loss: 0.1281 - acc: 0.9554 - val_loss: 0.2190 - val_acc: 0.9314\n",
      "Epoch 4/30\n",
      "391/390 [==============================] - 160s 410ms/step - loss: 0.1258 - acc: 0.9553 - val_loss: 0.2262 - val_acc: 0.9279\n",
      "Epoch 5/30\n",
      "322/390 [=======================>......] - ETA: 26s - loss: 0.1224 - acc: 0.9569"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e6ef40dbf9a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mopt_adm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdadelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt_adm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mpart_2\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mnn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"part_2\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpart_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"part_2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1221\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2112\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2113\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2114\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1830\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1831\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1832\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1833\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1834\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2350\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2352\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, act in zip([2,3], activations[-2:]):\n",
    "    batch_size = 128\n",
    "    nn = create_model(act, names[i])\n",
    "    \n",
    "    nn[\"model\"].summary()\n",
    "    model = nn[\"model\"]\n",
    "\n",
    "    # First training for 50 epochs\n",
    "    epochs = 25*2\n",
    "    opt_adm = keras.optimizers.Adadelta()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt_adm, metrics=['accuracy'])\n",
    "    part_1 = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,epochs=epochs,verbose=1,validation_data=(x_test,y_test))\n",
    "    nn[\"part_1\"] = part_1.history\n",
    "    print(nn[\"part_1\"])\n",
    "    model.save_weights(\"simplenet_generic_\"+nn[\"act_name\"]+\"_\"+str(epochs)+\".h5\")\n",
    "\n",
    "    # Training for 30 epochs more\n",
    "    epochs = 30\n",
    "    opt_adm = keras.optimizers.Adadelta(lr=0.7, rho=0.9)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt_adm, metrics=['accuracy'])\n",
    "    part_2= model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,epochs=epochs,verbose=1,validation_data=(x_test,y_test))\n",
    "    nn[\"part_2\"] = part_2.history\n",
    "    print(nn[\"part_2\"])\n",
    "    model.save_weights(\"simplenet_generic_\"+nn[\"act_name\"]+\"_\"+str(epochs)+\".h5\")\n",
    "\n",
    "    # First training for 25 epochs\n",
    "    epochs = 25\n",
    "    opt_adm = keras.optimizers.Adadelta(lr=0.6, rho=0.90)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt_adm, metrics=['accuracy'])\n",
    "    part_3 = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,epochs=epochs,verbose=1,validation_data=(x_test,y_test))\n",
    "    nn[\"part_3\"] = part_3.history\n",
    "    print(nn[\"part_3\"])\n",
    "    model.save_weights(\"simplenet_generic_\"+nn[\"act_name\"]+\"_\"+str(epochs)+\".h5\")\n",
    "\n",
    "    # First training for 50 epochs\n",
    "    epochs = 20\n",
    "    opt_adm = keras.optimizers.Adadelta(lr=0.5, rho=0.95)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt_adm, metrics=['accuracy'])\n",
    "    part_4 = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,epochs=epochs,verbose=1,validation_data=(x_test,y_test))\n",
    "    nn[\"part_4\"] = part_4.history\n",
    "    print(nn[\"part_4\"])\n",
    "    model.save_weights(\"simplenet_generic_\"+nn[\"act_name\"]+\"_\"+str(epochs)+\".h5\")\n",
    "\n",
    "    del nn[\"model\"]\n",
    "    print(\"\\n\", nn) # Ensure everything's ok x2\n",
    "    models.append(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
