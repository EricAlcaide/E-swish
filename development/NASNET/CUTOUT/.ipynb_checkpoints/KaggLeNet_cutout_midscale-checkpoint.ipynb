{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" First mid-scale experiment with Cutout to improve results. \n",
    "\n",
    "    Prev steps for further implementation in \n",
    "    NASNet Architectures to try to achieve SOTA results\n",
    "\"\"\"\n",
    "\n",
    "from cutout_eraser import get_random_eraser\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import cifar10\n",
    "from keras import regularizers, optimizers\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "# Create Validation set\n",
    "x_train, x_val = x_train[:45000], x_train[45000:]\n",
    "y_train, y_val = y_train[:45000], y_train[45000:]\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_val = x_val.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#z-score\n",
    "mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "std = np.std(x_train,axis=(0,1,2,3))\n",
    "x_train = (x_train-mean)/(std+1e-7)\n",
    "x_val = (x_val-mean)/(std+1e-7)\n",
    "x_test = (x_test-mean)/(std+1e-7)\n",
    "\n",
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train,num_classes)\n",
    "y_val = np_utils.to_categorical(y_val,num_classes)\n",
    "y_test = np_utils.to_categorical(y_test,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def e_swish_2(x):\n",
    "    return K.maximum(x*K.sigmoid(x), x*(2-K.sigmoid(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create(act):\n",
    "    baseMapNum = 32\n",
    "    weight_decay = 1e-4\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(2*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(2*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(4*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(4*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data augmentation come at me\n",
    "\"\"\" With 50% probability, erase 16*16 pixel squares. \n",
    "    - p=0.5 for 50% probability\n",
    "    - s_l, s_h = 0.5 for only 16*16 (out of 32*32) pixels cut.\n",
    "    - r_1, r_2 = 1 for squares, not rectangles\n",
    "    - v_l, v_h = 0,255 since data is not normalized. Can change to 0,1 if it is.\n",
    "\"\"\"\n",
    "eraser = get_random_eraser(p=0.5, s_l=0.5, s_h = 0.5, r_1=1, r_2=2, v_l=0, v_h=255)\n",
    "#data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    preprocessing_function=eraser\n",
    "    )\n",
    "\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def schedule(x):\n",
    "    if x<20:\n",
    "        return 0.002\n",
    "    elif x<40:\n",
    "        return 0.001\n",
    "    elif x<60:\n",
    "        return 0.0006\n",
    "    elif x<80:\n",
    "        return 0.00025\n",
    "    else:\n",
    "        if x<90:\n",
    "            return 0.000075\n",
    "        elif x<100:\n",
    "            return 0.000025\n",
    "        else: \n",
    "            return 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create(e_swish_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished compiling\n",
      "Gonna fit the model\n",
      "Epoch 1/90\n",
      "351/351 [==============================] - 161s 458ms/step - loss: 2.1940 - acc: 0.3561 - val_loss: 1.5552 - val_acc: 0.4922\n",
      "Epoch 2/90\n",
      "351/351 [==============================] - 154s 440ms/step - loss: 1.6994 - acc: 0.4678 - val_loss: 1.2870 - val_acc: 0.5796\n",
      "Epoch 3/90\n",
      "351/351 [==============================] - 154s 440ms/step - loss: 1.5737 - acc: 0.5160 - val_loss: 1.7705 - val_acc: 0.5676\n",
      "Epoch 4/90\n",
      "351/351 [==============================] - 154s 440ms/step - loss: 1.5166 - acc: 0.5417 - val_loss: 1.1468 - val_acc: 0.6498\n",
      "Epoch 5/90\n",
      "351/351 [==============================] - 154s 440ms/step - loss: 1.5796 - acc: 0.5569 - val_loss: 1.0451 - val_acc: 0.6812\n",
      "Epoch 6/90\n",
      "351/351 [==============================] - 154s 440ms/step - loss: 1.4416 - acc: 0.5814 - val_loss: 1.0763 - val_acc: 0.6706\n",
      "Epoch 7/90\n",
      "351/351 [==============================] - 154s 440ms/step - loss: 1.3612 - acc: 0.6006 - val_loss: 1.0028 - val_acc: 0.7136\n",
      "Epoch 8/90\n",
      "351/351 [==============================] - 154s 440ms/step - loss: 1.3224 - acc: 0.6119 - val_loss: 1.0958 - val_acc: 0.7026\n",
      "Epoch 9/90\n",
      "351/351 [==============================] - 154s 440ms/step - loss: 1.3385 - acc: 0.6150 - val_loss: 1.0293 - val_acc: 0.7022\n",
      "Epoch 10/90\n",
      "351/351 [==============================] - 154s 440ms/step - loss: 1.2824 - acc: 0.6321 - val_loss: 0.8898 - val_acc: 0.7388\n",
      "Epoch 11/90\n",
      "351/351 [==============================] - 154s 440ms/step - loss: 1.2295 - acc: 0.6478 - val_loss: 0.8566 - val_acc: 0.7590\n",
      "Epoch 12/90\n",
      "351/351 [==============================] - 153s 436ms/step - loss: 1.2683 - acc: 0.6464 - val_loss: 0.9280 - val_acc: 0.7414\n",
      "Epoch 13/90\n",
      "351/351 [==============================] - 153s 435ms/step - loss: 1.2777 - acc: 0.6464 - val_loss: 0.8673 - val_acc: 0.7506\n",
      "Epoch 14/90\n",
      "351/351 [==============================] - 153s 436ms/step - loss: 1.2749 - acc: 0.6474 - val_loss: 0.9043 - val_acc: 0.7570\n",
      "Epoch 15/90\n",
      "351/351 [==============================] - 153s 435ms/step - loss: 1.1856 - acc: 0.6613 - val_loss: 0.9001 - val_acc: 0.7498\n",
      "Epoch 16/90\n",
      "351/351 [==============================] - 153s 436ms/step - loss: 1.1644 - acc: 0.6667 - val_loss: 0.8127 - val_acc: 0.7738\n",
      "Epoch 17/90\n",
      "351/351 [==============================] - 153s 436ms/step - loss: 1.1525 - acc: 0.6731 - val_loss: 0.8185 - val_acc: 0.7808\n",
      "Epoch 18/90\n",
      "351/351 [==============================] - 153s 436ms/step - loss: 1.0924 - acc: 0.6784 - val_loss: 0.8884 - val_acc: 0.7632\n",
      "Epoch 19/90\n",
      "351/351 [==============================] - 153s 436ms/step - loss: 1.0573 - acc: 0.6867 - val_loss: 0.7583 - val_acc: 0.7952\n",
      "Epoch 20/90\n",
      "351/351 [==============================] - 153s 436ms/step - loss: 1.0282 - acc: 0.6952 - val_loss: 0.7461 - val_acc: 0.7944\n",
      "Epoch 21/90\n",
      "351/351 [==============================] - 153s 436ms/step - loss: 0.9621 - acc: 0.7179 - val_loss: 0.6830 - val_acc: 0.8190\n",
      "Epoch 22/90\n",
      "351/351 [==============================] - 153s 435ms/step - loss: 0.9338 - acc: 0.7237 - val_loss: 0.6667 - val_acc: 0.8252\n",
      "Epoch 23/90\n",
      "351/351 [==============================] - 153s 436ms/step - loss: 0.9205 - acc: 0.7285 - val_loss: 0.6569 - val_acc: 0.8236\n",
      "Epoch 24/90\n",
      "351/351 [==============================] - 153s 436ms/step - loss: 0.9150 - acc: 0.7278 - val_loss: 0.6589 - val_acc: 0.8288\n",
      "Epoch 25/90\n",
      "351/351 [==============================] - 153s 436ms/step - loss: 0.8992 - acc: 0.7324 - val_loss: 0.6211 - val_acc: 0.8372\n",
      "Epoch 26/90\n",
      "351/351 [==============================] - 153s 436ms/step - loss: 0.8930 - acc: 0.7346 - val_loss: 0.6096 - val_acc: 0.8420\n",
      "Epoch 27/90\n",
      "351/351 [==============================] - 153s 436ms/step - loss: 0.8828 - acc: 0.7386 - val_loss: 0.6133 - val_acc: 0.8386\n",
      "Epoch 28/90\n",
      "351/351 [==============================] - 153s 436ms/step - loss: 0.8786 - acc: 0.7386 - val_loss: 0.6037 - val_acc: 0.8406\n",
      "Epoch 29/90\n",
      "351/351 [==============================] - 153s 436ms/step - loss: 0.8621 - acc: 0.7423 - val_loss: 0.6017 - val_acc: 0.8350\n",
      "Epoch 30/90\n",
      "351/351 [==============================] - 153s 436ms/step - loss: 0.8615 - acc: 0.7425 - val_loss: 0.5992 - val_acc: 0.8400\n",
      "Epoch 31/90\n",
      "351/351 [==============================] - 153s 435ms/step - loss: 0.8465 - acc: 0.7479 - val_loss: 0.6291 - val_acc: 0.8334\n",
      "Epoch 32/90\n",
      "351/351 [==============================] - 153s 436ms/step - loss: 0.8527 - acc: 0.7443 - val_loss: 0.5747 - val_acc: 0.8488\n",
      "Epoch 33/90\n",
      "351/351 [==============================] - 153s 436ms/step - loss: 0.8428 - acc: 0.7492 - val_loss: 0.5904 - val_acc: 0.8416\n",
      "Epoch 34/90\n",
      "351/351 [==============================] - 153s 435ms/step - loss: 0.8389 - acc: 0.7511 - val_loss: 0.5787 - val_acc: 0.8452\n",
      "Epoch 35/90\n",
      "351/351 [==============================] - 153s 435ms/step - loss: 0.8400 - acc: 0.7497 - val_loss: 0.5678 - val_acc: 0.8510\n",
      "Epoch 36/90\n",
      "351/351 [==============================] - 154s 438ms/step - loss: 0.8334 - acc: 0.7490 - val_loss: 0.5716 - val_acc: 0.8440\n",
      "Epoch 37/90\n",
      "351/351 [==============================] - 154s 440ms/step - loss: 0.8280 - acc: 0.7549 - val_loss: 0.5829 - val_acc: 0.8456\n",
      "Epoch 38/90\n",
      "351/351 [==============================] - 154s 440ms/step - loss: 0.8168 - acc: 0.7575 - val_loss: 0.5628 - val_acc: 0.8498\n",
      "Epoch 39/90\n",
      "351/351 [==============================] - 156s 443ms/step - loss: 0.8133 - acc: 0.7591 - val_loss: 0.5570 - val_acc: 0.8488\n",
      "Epoch 40/90\n",
      "351/351 [==============================] - 156s 445ms/step - loss: 0.8165 - acc: 0.7583 - val_loss: 0.5870 - val_acc: 0.8472\n",
      "Epoch 41/90\n",
      "351/351 [==============================] - 156s 444ms/step - loss: 0.7795 - acc: 0.7693 - val_loss: 0.5419 - val_acc: 0.8616\n",
      "Epoch 42/90\n",
      "351/351 [==============================] - 156s 446ms/step - loss: 0.7691 - acc: 0.7738 - val_loss: 0.5451 - val_acc: 0.8556\n",
      "Epoch 43/90\n",
      "351/351 [==============================] - 155s 442ms/step - loss: 0.7644 - acc: 0.7764 - val_loss: 0.5208 - val_acc: 0.8646\n",
      "Epoch 44/90\n",
      "351/351 [==============================] - 156s 444ms/step - loss: 0.7597 - acc: 0.7768 - val_loss: 0.5228 - val_acc: 0.8630\n",
      "Epoch 45/90\n",
      "351/351 [==============================] - 154s 439ms/step - loss: 0.7623 - acc: 0.7737 - val_loss: 0.5139 - val_acc: 0.8642\n",
      "Epoch 46/90\n",
      "351/351 [==============================] - 158s 449ms/step - loss: 0.7500 - acc: 0.7780 - val_loss: 0.5156 - val_acc: 0.8672\n",
      "Epoch 47/90\n",
      "351/351 [==============================] - 156s 445ms/step - loss: 0.7423 - acc: 0.7797 - val_loss: 0.5176 - val_acc: 0.8626\n",
      "Epoch 48/90\n",
      "351/351 [==============================] - 155s 443ms/step - loss: 0.7445 - acc: 0.7779 - val_loss: 0.5119 - val_acc: 0.8672\n",
      "Epoch 49/90\n",
      "351/351 [==============================] - 156s 446ms/step - loss: 0.7410 - acc: 0.7813 - val_loss: 0.4973 - val_acc: 0.8664\n",
      "Epoch 50/90\n",
      "351/351 [==============================] - 156s 443ms/step - loss: 0.7380 - acc: 0.7789 - val_loss: 0.4999 - val_acc: 0.8670\n",
      "Epoch 51/90\n",
      "351/351 [==============================] - 156s 445ms/step - loss: 0.7348 - acc: 0.7832 - val_loss: 0.5016 - val_acc: 0.8688\n",
      "Epoch 52/90\n",
      "351/351 [==============================] - 155s 442ms/step - loss: 0.7289 - acc: 0.7848 - val_loss: 0.5076 - val_acc: 0.8642\n",
      "Epoch 53/90\n",
      "351/351 [==============================] - 155s 441ms/step - loss: 0.7309 - acc: 0.7835 - val_loss: 0.5009 - val_acc: 0.8688\n",
      "Epoch 54/90\n",
      "351/351 [==============================] - 155s 441ms/step - loss: 0.7232 - acc: 0.7844 - val_loss: 0.5081 - val_acc: 0.8650\n",
      "Epoch 55/90\n",
      "351/351 [==============================] - 155s 442ms/step - loss: 0.7285 - acc: 0.7826 - val_loss: 0.4978 - val_acc: 0.8704\n",
      "Epoch 56/90\n",
      "351/351 [==============================] - 156s 445ms/step - loss: 0.7131 - acc: 0.7872 - val_loss: 0.5022 - val_acc: 0.8660\n",
      "Epoch 57/90\n",
      "351/351 [==============================] - 154s 440ms/step - loss: 0.7139 - acc: 0.7882 - val_loss: 0.5037 - val_acc: 0.8680\n",
      "Epoch 58/90\n",
      "351/351 [==============================] - 154s 439ms/step - loss: 0.7149 - acc: 0.7848 - val_loss: 0.4904 - val_acc: 0.8736\n",
      "Epoch 59/90\n",
      "351/351 [==============================] - 154s 438ms/step - loss: 0.7121 - acc: 0.7868 - val_loss: 0.5040 - val_acc: 0.8688\n",
      "Epoch 60/90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351/351 [==============================] - 154s 438ms/step - loss: 0.7166 - acc: 0.7880 - val_loss: 0.4881 - val_acc: 0.8708\n",
      "Epoch 61/90\n",
      "351/351 [==============================] - 153s 435ms/step - loss: 0.6966 - acc: 0.7933 - val_loss: 0.4697 - val_acc: 0.8782\n",
      "Epoch 62/90\n",
      "351/351 [==============================] - 153s 435ms/step - loss: 0.6772 - acc: 0.8011 - val_loss: 0.4660 - val_acc: 0.8786\n",
      "Epoch 63/90\n",
      "351/351 [==============================] - 153s 435ms/step - loss: 0.6721 - acc: 0.8039 - val_loss: 0.4618 - val_acc: 0.8802\n",
      "Epoch 64/90\n",
      "351/351 [==============================] - 153s 435ms/step - loss: 0.6724 - acc: 0.8005 - val_loss: 0.4573 - val_acc: 0.8802\n",
      "Epoch 65/90\n",
      "351/351 [==============================] - 153s 435ms/step - loss: 0.6646 - acc: 0.8047 - val_loss: 0.4611 - val_acc: 0.8764\n",
      "Epoch 66/90\n",
      "351/351 [==============================] - 153s 435ms/step - loss: 0.6635 - acc: 0.8043 - val_loss: 0.4547 - val_acc: 0.8804\n",
      "Epoch 67/90\n",
      "351/351 [==============================] - 152s 434ms/step - loss: 0.6632 - acc: 0.8027 - val_loss: 0.4484 - val_acc: 0.8832\n",
      "Epoch 68/90\n",
      "351/351 [==============================] - 153s 435ms/step - loss: 0.6547 - acc: 0.8072 - val_loss: 0.4530 - val_acc: 0.8820\n",
      "Epoch 69/90\n",
      "351/351 [==============================] - 152s 434ms/step - loss: 0.6582 - acc: 0.8039 - val_loss: 0.4518 - val_acc: 0.8808\n",
      "Epoch 70/90\n",
      "351/351 [==============================] - 153s 435ms/step - loss: 0.6595 - acc: 0.8017 - val_loss: 0.4479 - val_acc: 0.8838\n",
      "Epoch 71/90\n",
      "351/351 [==============================] - 152s 434ms/step - loss: 0.6522 - acc: 0.8050 - val_loss: 0.4486 - val_acc: 0.8826\n",
      "Epoch 72/90\n",
      "351/351 [==============================] - 154s 438ms/step - loss: 0.6467 - acc: 0.8073 - val_loss: 0.4436 - val_acc: 0.8824\n",
      "Epoch 73/90\n",
      "351/351 [==============================] - 154s 439ms/step - loss: 0.6451 - acc: 0.8090 - val_loss: 0.4387 - val_acc: 0.8822\n",
      "Epoch 74/90\n",
      "351/351 [==============================] - 154s 438ms/step - loss: 0.6443 - acc: 0.8108 - val_loss: 0.4372 - val_acc: 0.8840\n",
      "Epoch 75/90\n",
      "351/351 [==============================] - 155s 441ms/step - loss: 0.6437 - acc: 0.8076 - val_loss: 0.4539 - val_acc: 0.8808\n",
      "Epoch 76/90\n",
      "351/351 [==============================] - 155s 441ms/step - loss: 0.6432 - acc: 0.8086 - val_loss: 0.4391 - val_acc: 0.8810\n",
      "Epoch 77/90\n",
      "351/351 [==============================] - 155s 442ms/step - loss: 0.6403 - acc: 0.8087 - val_loss: 0.4424 - val_acc: 0.8828\n",
      "Epoch 78/90\n",
      "351/351 [==============================] - 156s 444ms/step - loss: 0.6294 - acc: 0.8131 - val_loss: 0.4403 - val_acc: 0.8844\n",
      "Epoch 79/90\n",
      "351/351 [==============================] - 155s 441ms/step - loss: 0.6385 - acc: 0.8086 - val_loss: 0.4386 - val_acc: 0.8830\n",
      "Epoch 80/90\n",
      "351/351 [==============================] - 154s 440ms/step - loss: 0.6354 - acc: 0.8097 - val_loss: 0.4363 - val_acc: 0.8826\n",
      "Epoch 81/90\n",
      "351/351 [==============================] - 155s 443ms/step - loss: 0.6246 - acc: 0.8156 - val_loss: 0.4311 - val_acc: 0.8854\n",
      "Epoch 82/90\n",
      "351/351 [==============================] - 157s 447ms/step - loss: 0.6206 - acc: 0.8163 - val_loss: 0.4261 - val_acc: 0.8866\n",
      "Epoch 83/90\n",
      "351/351 [==============================] - 156s 445ms/step - loss: 0.6180 - acc: 0.8164 - val_loss: 0.4247 - val_acc: 0.8854\n",
      "Epoch 84/90\n",
      "351/351 [==============================] - 156s 444ms/step - loss: 0.6159 - acc: 0.8171 - val_loss: 0.4231 - val_acc: 0.8886\n",
      "Epoch 85/90\n",
      "351/351 [==============================] - 155s 441ms/step - loss: 0.6131 - acc: 0.8184 - val_loss: 0.4195 - val_acc: 0.8898\n",
      "Epoch 86/90\n",
      "351/351 [==============================] - 156s 443ms/step - loss: 0.6174 - acc: 0.8163 - val_loss: 0.4197 - val_acc: 0.8870\n",
      "Epoch 87/90\n",
      "351/351 [==============================] - 155s 441ms/step - loss: 0.6090 - acc: 0.8176 - val_loss: 0.4199 - val_acc: 0.8870\n",
      "Epoch 88/90\n",
      "351/351 [==============================] - 155s 443ms/step - loss: 0.6111 - acc: 0.8176 - val_loss: 0.4195 - val_acc: 0.8876\n",
      "Epoch 89/90\n",
      "351/351 [==============================] - 155s 443ms/step - loss: 0.6056 - acc: 0.8186 - val_loss: 0.4201 - val_acc: 0.8894\n",
      "Epoch 90/90\n",
      "351/351 [==============================] - 156s 444ms/step - loss: 0.6123 - acc: 0.8186 - val_loss: 0.4189 - val_acc: 0.8892\n",
      "{'val_loss': [1.5551784242630005, 1.2869540056228637, 1.7705270696640014, 1.1468115957260132, 1.0450788146972656, 1.0762520687103272, 1.0027886987686156, 1.0958253948211669, 1.029307365512848, 0.88984201688766484, 0.85657180957794188, 0.92804006862640376, 0.86725870485305789, 0.90430078887939458, 0.90014740161895757, 0.81274997720718378, 0.81852790317535395, 0.88835911502838139, 0.75825636053085332, 0.74605235519409174, 0.68297518110275268, 0.6666815853118897, 0.65687236604690546, 0.65885198574066162, 0.62113174076080324, 0.60961726818084716, 0.61329304504394533, 0.60365086774826049, 0.60167991838455204, 0.5991581049919128, 0.62906772012710577, 0.57469758653640746, 0.59043770694732667, 0.57866393690109252, 0.56781131448745725, 0.5715912455558777, 0.58292554121017459, 0.56284630870819097, 0.55697985630035396, 0.58698293294906612, 0.54191816654205327, 0.5450530055999756, 0.52079039077758793, 0.52278972864150997, 0.5139030863761902, 0.51562760696411136, 0.51760889205932614, 0.51186376299858094, 0.49727115650177001, 0.49990632848739625, 0.50156373186111447, 0.50764201078414917, 0.50086923551559448, 0.5081172136306763, 0.4977704649925232, 0.50224679522514348, 0.50366667108535768, 0.49036939888000486, 0.50395225830078128, 0.48810527248382568, 0.46968704757690427, 0.46597389440536496, 0.46181428871154784, 0.45729479932785033, 0.46105187568664552, 0.45469140939712527, 0.44836618032455444, 0.45299536600112916, 0.45179630279541017, 0.44793380641937258, 0.44861138472557066, 0.44360010328292848, 0.43871152582168577, 0.43724822807312014, 0.45394362125396731, 0.43913763999938965, 0.44240747361183169, 0.44031143693923952, 0.4386370575904846, 0.43629812450408934, 0.4310791199684143, 0.42614910850524901, 0.4247297857284546, 0.42305786209106444, 0.41951258811950681, 0.41972645173072815, 0.41986088490486145, 0.41948596611022948, 0.42014408087730409, 0.41885962610244754], 'val_acc': [0.49220000000000003, 0.5796, 0.56759999999999999, 0.64980000000000004, 0.68120000000000003, 0.67059999999999997, 0.71360000000000001, 0.7026, 0.70220000000000005, 0.73880000000000001, 0.75900000000000001, 0.74139999999999995, 0.75060000000000004, 0.75700000000000001, 0.74980000000000002, 0.77380000000000004, 0.78080000000000005, 0.76319999999999999, 0.79520000000000002, 0.7944, 0.81899999999999995, 0.82520000000000004, 0.8236, 0.82879999999999998, 0.83720000000000006, 0.84199999999999997, 0.83860000000000001, 0.84060000000000001, 0.83499999999999996, 0.83999999999999997, 0.83340000000000003, 0.8488, 0.84160000000000001, 0.84519999999999995, 0.85099999999999998, 0.84399999999999997, 0.84560000000000002, 0.8498, 0.8488, 0.84719999999999995, 0.86160000000000003, 0.85560000000000003, 0.86460000000000004, 0.86299999999999999, 0.86419999999999997, 0.86719999999999997, 0.86260000000000003, 0.86719999999999997, 0.86639999999999995, 0.86699999999999999, 0.86880000000000002, 0.86419999999999997, 0.86880000000000002, 0.86499999999999999, 0.87039999999999995, 0.86599999999999999, 0.86799999999999999, 0.87360000000000004, 0.86880000000000002, 0.87080000000000002, 0.87819999999999998, 0.87860000000000005, 0.88019999999999998, 0.88019999999999998, 0.87639999999999996, 0.88039999999999996, 0.88319999999999999, 0.88200000000000001, 0.88080000000000003, 0.88380000000000003, 0.88260000000000005, 0.88239999999999996, 0.88219999999999998, 0.88400000000000001, 0.88080000000000003, 0.88100000000000001, 0.88280000000000003, 0.88439999999999996, 0.88300000000000001, 0.88260000000000005, 0.88539999999999996, 0.88660000000000005, 0.88539999999999996, 0.88859999999999995, 0.88980000000000004, 0.88700000000000001, 0.88700000000000001, 0.88759999999999994, 0.88939999999999997, 0.88919999999999999], 'acc': [0.35610180065622943, 0.467797290085471, 0.51593421282966334, 0.5417409520626153, 0.55707345336255898, 0.58143162769862389, 0.60077553929056704, 0.61171777499383051, 0.6152166161313587, 0.63210911036897921, 0.64782046712768926, 0.64643875913710114, 0.64650561593182054, 0.64744161167423619, 0.66130326263490979, 0.66678552325555274, 0.67313692284514504, 0.67848546979133617, 0.68673114632939591, 0.69520121082621078, 0.71786414690675704, 0.72355855053895202, 0.72844981280085574, 0.72784810123394295, 0.73248350868932399, 0.73455607060082007, 0.73861205204856395, 0.73856748082374668, 0.7422446069243035, 0.74264574791577909, 0.74794972363715295, 0.74433945440570182, 0.74915314668596567, 0.7511142806204314, 0.74964343022271518, 0.74901943303276708, 0.75483597793517887, 0.7574656801993972, 0.75920395795664364, 0.75840167584617335, 0.76929934033524605, 0.7736896059691194, 0.77649759313260669, 0.77684294871794868, 0.77374152087475823, 0.77801301478702001, 0.77975129253363984, 0.77794615796042077, 0.78131677350427353, 0.77898518388418592, 0.78316099130654637, 0.78476555539996762, 0.78354255698005693, 0.7842957872188504, 0.78267070776979686, 0.78730611516141835, 0.78819753962588368, 0.7847667378917379, 0.78672668920691413, 0.78804444842192156, 0.79332323056488119, 0.80110090927423949, 0.80408718135824897, 0.80045462648250942, 0.80486717770753813, 0.80433137464387461, 0.80261513748283009, 0.80718037749287752, 0.8041101391721478, 0.80172720797720798, 0.80493573725798095, 0.80729166666666663, 0.80910144411831064, 0.81080417705833463, 0.80751916559479164, 0.80856036324786329, 0.8088405926454838, 0.81295685506500348, 0.80867801746129353, 0.80965858446040628, 0.81565341419832749, 0.81636655377415035, 0.81634426812986194, 0.81712962962962965, 0.81843538022134954, 0.81629969688379145, 0.81750311998573721, 0.81765911928322421, 0.81855413105413111, 0.81859157438450136], 'loss': [2.1934828099390042, 1.6991248158852585, 1.5733377648496569, 1.5162906328567869, 1.5776204782323757, 1.4416738772566549, 1.3610700413512979, 1.3228104173403878, 1.3381413664701343, 1.2823018440908662, 1.2294388891945423, 1.268288380875692, 1.2771184500110917, 1.2737186400089955, 1.1853138080869985, 1.1642528787018844, 1.1525961849901591, 1.0922539140434693, 1.0572618596118961, 1.0281812998643967, 0.96211583241567955, 0.93392521391932937, 0.92037010613684778, 0.91495380380995706, 0.89914619464631129, 0.89290099259599431, 0.88276898973573448, 0.87867934832621397, 0.86222419255896454, 0.86120973704098847, 0.8463953826214502, 0.85262142103992267, 0.84306279944319373, 0.8388752482923133, 0.84013865908574969, 0.83325522217782, 0.82819319653370771, 0.81697817331415423, 0.81317585357095412, 0.81643146586770954, 0.77947126450354154, 0.76923951328619, 0.76425176733508005, 0.75969449848870607, 0.76210792095096824, 0.74989863352929442, 0.74206266371956009, 0.74435455077768631, 0.7409554847285279, 0.73782491452010435, 0.73488912691426456, 0.72884588624985169, 0.73087415396318134, 0.72350250065901245, 0.72825650668097464, 0.71286850461009399, 0.71388598456699848, 0.71494002016181624, 0.71216576849421753, 0.71621477761382679, 0.69671811387381211, 0.67710717124168751, 0.67185240632222287, 0.67241853980334565, 0.66426438924186182, 0.66347640379201989, 0.6632407206226697, 0.65473377466881033, 0.65798267450812709, 0.6594996131383456, 0.65236744886847409, 0.64669728839499319, 0.64492214010836801, 0.64421451566305299, 0.64363800674905369, 0.64316686036919601, 0.63987212510289404, 0.62934283215342024, 0.63833147678427948, 0.63536949321577141, 0.62471480864980344, 0.62053361241246618, 0.61807617187542507, 0.61594149792975506, 0.61300796077256536, 0.61749700723984047, 0.60914518356450764, 0.61099008266078658, 0.60561395149964548, 0.61235589525266354]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 12s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "batch_size  = 128\n",
    "epochs = 90\n",
    "\n",
    "opt = keras.optimizers.rmsprop(lr=0.002,decay=1e-6)\n",
    "lr_1 = keras.callbacks.LearningRateScheduler(schedule)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "print(\"Finished compiling\")\n",
    "\n",
    "####################\n",
    "# Network training #\n",
    "####################\n",
    "                     \n",
    "print(\"Gonna fit the model\")\n",
    "his = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size), steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                          epochs=epochs,verbose=1,validation_data=(x_val,y_val), callbacks=[lr_1])\n",
    "print(his.history)\n",
    "model.evaluate(x_test, y_test)\n",
    "model.save('kaggLeNet_e_swish_2_cutout.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 11s 1ms/step\n",
      "[0.43774211184978484, 0.88380000000000003]\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished compiling\n",
      "Gonna fit the model\n",
      "Epoch 91/110\n",
      "351/351 [==============================] - 156s 443ms/step - loss: 0.6053 - acc: 0.8187 - val_loss: 0.4186 - val_acc: 0.8894\n",
      "Epoch 92/110\n",
      "351/351 [==============================] - 155s 441ms/step - loss: 0.6035 - acc: 0.8199 - val_loss: 0.4181 - val_acc: 0.8908\n",
      "Epoch 93/110\n",
      "351/351 [==============================] - 155s 441ms/step - loss: 0.6066 - acc: 0.8191 - val_loss: 0.4177 - val_acc: 0.8900\n",
      "Epoch 94/110\n",
      "351/351 [==============================] - 155s 440ms/step - loss: 0.6004 - acc: 0.8213 - val_loss: 0.4167 - val_acc: 0.8910\n",
      "Epoch 95/110\n",
      "351/351 [==============================] - 155s 442ms/step - loss: 0.6042 - acc: 0.8199 - val_loss: 0.4177 - val_acc: 0.8880\n",
      "Epoch 96/110\n",
      "351/351 [==============================] - 154s 438ms/step - loss: 0.6004 - acc: 0.8225 - val_loss: 0.4171 - val_acc: 0.8890\n",
      "Epoch 97/110\n",
      "351/351 [==============================] - 154s 438ms/step - loss: 0.5960 - acc: 0.8228 - val_loss: 0.4171 - val_acc: 0.8890\n",
      "Epoch 98/110\n",
      "351/351 [==============================] - 154s 440ms/step - loss: 0.5981 - acc: 0.8218 - val_loss: 0.4149 - val_acc: 0.8914\n",
      "Epoch 99/110\n",
      "351/351 [==============================] - 155s 442ms/step - loss: 0.5984 - acc: 0.8212 - val_loss: 0.4144 - val_acc: 0.8904\n",
      "Epoch 100/110\n",
      "351/351 [==============================] - 155s 441ms/step - loss: 0.5930 - acc: 0.8245 - val_loss: 0.4143 - val_acc: 0.8910\n",
      "Epoch 101/110\n",
      "351/351 [==============================] - 155s 442ms/step - loss: 0.5945 - acc: 0.8240 - val_loss: 0.4150 - val_acc: 0.8908\n",
      "Epoch 102/110\n",
      "351/351 [==============================] - 156s 443ms/step - loss: 0.6004 - acc: 0.8217 - val_loss: 0.4145 - val_acc: 0.8914\n",
      "Epoch 103/110\n",
      "351/351 [==============================] - 156s 443ms/step - loss: 0.5991 - acc: 0.8221 - val_loss: 0.4145 - val_acc: 0.8910\n",
      "Epoch 104/110\n",
      "351/351 [==============================] - 155s 442ms/step - loss: 0.5909 - acc: 0.8257 - val_loss: 0.4139 - val_acc: 0.8910\n",
      "Epoch 105/110\n",
      "351/351 [==============================] - 155s 443ms/step - loss: 0.5948 - acc: 0.8252 - val_loss: 0.4141 - val_acc: 0.8908\n",
      "Epoch 106/110\n",
      "351/351 [==============================] - 155s 442ms/step - loss: 0.5971 - acc: 0.8229 - val_loss: 0.4135 - val_acc: 0.8918\n",
      "Epoch 107/110\n",
      "351/351 [==============================] - 155s 442ms/step - loss: 0.5981 - acc: 0.8212 - val_loss: 0.4146 - val_acc: 0.8912\n",
      "Epoch 108/110\n",
      "351/351 [==============================] - 154s 440ms/step - loss: 0.5982 - acc: 0.8196 - val_loss: 0.4137 - val_acc: 0.8914\n",
      "Epoch 109/110\n",
      "351/351 [==============================] - 154s 440ms/step - loss: 0.5944 - acc: 0.8241 - val_loss: 0.4130 - val_acc: 0.8920\n",
      "Epoch 110/110\n",
      "351/351 [==============================] - 154s 440ms/step - loss: 0.5997 - acc: 0.8216 - val_loss: 0.4135 - val_acc: 0.8914\n",
      "{'val_loss': [0.41864223747253421, 0.41813759369850156, 0.41767448801994322, 0.41671581916809081, 0.41768536043167115, 0.41713269920349122, 0.4170697250366211, 0.41491216320991514, 0.41443265719413758, 0.4142674120426178, 0.41498727226257326, 0.4144924997329712, 0.41451624770164491, 0.41392613649368287, 0.41414773235321045, 0.41350387310981751, 0.41461639862060545, 0.41367182755470278, 0.41298671035766604, 0.41354245967864989], 'val_acc': [0.88939999999999997, 0.89080000000000004, 0.89000000000000001, 0.89100000000000001, 0.88800000000000001, 0.88900000000000001, 0.88900000000000001, 0.89139999999999997, 0.89039999999999997, 0.89100000000000001, 0.89080000000000004, 0.89139999999999997, 0.89100000000000001, 0.89100000000000001, 0.89080000000000004, 0.89180000000000004, 0.89119999999999999, 0.89139999999999997, 0.89200000000000002, 0.89139999999999997], 'acc': [0.81872882868946506, 0.81995453739319279, 0.81921911209869525, 0.82124710284913371, 0.81986734330484334, 0.82249642993160255, 0.8228516669850614, 0.82169281512918602, 0.8213585309749365, 0.82445623103597621, 0.82407737566753603, 0.82173738635400329, 0.82213852732422565, 0.82577108218933859, 0.82528079873760196, 0.8229630950258513, 0.82115796039949918, 0.81955339632733104, 0.82407737561440297, 0.8215368158104458], 'loss': [0.60503705912551087, 0.60362077951643811, 0.60614748045942923, 0.60041307083190409, 0.60420817705640761, 0.60030809022494191, 0.59599142138132077, 0.59827893074979799, 0.59792325868179808, 0.59320821905969234, 0.59433642704020551, 0.60028740277496928, 0.59909952025847923, 0.59068441676846972, 0.59475037780201656, 0.59699490741307049, 0.59800538716628215, 0.59818645614123123, 0.59446757115831472, 0.59974040670984885]}\n",
      "10000/10000 [==============================] - 11s 1ms/step\n",
      "[0.43149707212448118, 0.88649999999999995]\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "batch_size  = 128\n",
    "epochs = 110\n",
    "\n",
    "opt = keras.optimizers.rmsprop(lr=0.000025,decay=1e-8)\n",
    "lr_1 = keras.callbacks.LearningRateScheduler(schedule)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "print(\"Finished compiling\")\n",
    "\n",
    "####################\n",
    "# Network training #\n",
    "####################\n",
    "                     \n",
    "print(\"Gonna fit the model\")\n",
    "his = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size), steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                          epochs=epochs,verbose=1,validation_data=(x_val,y_val), callbacks=[lr_1], initial_epoch=90)\n",
    "print(his.history)\n",
    "print(model.evaluate(x_test, y_test))\n",
    "model.save('kaggLeNet_e_swish_2_cutout.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
