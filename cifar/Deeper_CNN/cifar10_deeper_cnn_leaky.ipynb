{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Performing a large-scale experiment on cifar10 \"\"\"\n",
    "\n",
    "import keras\n",
    "import logging\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import cifar10\n",
    "import keras.regularizers as regularizers\n",
    "from keras.models import load_model\n",
    "\n",
    "# Record settings\n",
    "LOG_FORMAT = \"%(levelname)s %(asctime)s - %(message)s\"\n",
    "logging.basicConfig(filename=\"swish_first_exp_log.txt\",format = LOG_FORMAT, level = logging.DEBUG, filemode = \"a\")\n",
    "logs = logging.getLogger()\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#z-score\n",
    "mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "std = np.std(x_train,axis=(0,1,2,3))\n",
    "x_train = (x_train-mean)/(std+1e-7)\n",
    "x_test = (x_test-mean)/(std+1e-7)\n",
    "\n",
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train,num_classes)\n",
    "y_test = np_utils.to_categorical(y_test,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Swish activation function\n",
    "# x*sigmoid(x)\n",
    "def leaky(x):\n",
    "    return K.maximum(0.3*x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create(act):\n",
    "    baseMapNum = 32\n",
    "    weight_decay = 1e-4\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(2*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(2*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(4*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(4*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False\n",
    "    )\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def schedule(x):\n",
    "    if x < 75: \n",
    "        return 0.001\n",
    "    elif x < 100:\n",
    "        return 0.0005\n",
    "    elif x < 125:\n",
    "        return 0.0003\n",
    "    elif x<150:\n",
    "        return 0.00015\n",
    "    elif x<175:\n",
    "        return 0.000075\n",
    "    elif x<200: \n",
    "        return 0.000035\n",
    "    elif x<225:\n",
    "        return 0.000017\n",
    "    else:\n",
    "        return 0.000012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Epoch 1/250\n",
      "390/390 [==============================] - 160s 411ms/step - loss: 2.0497 - acc: 0.3803 - val_loss: 1.5434 - val_acc: 0.5072\n",
      "Epoch 2/250\n",
      "390/390 [==============================] - 156s 400ms/step - loss: 1.4977 - acc: 0.5185 - val_loss: 1.1258 - val_acc: 0.6378\n",
      "Epoch 3/250\n",
      "390/390 [==============================] - 159s 407ms/step - loss: 1.3323 - acc: 0.5770 - val_loss: 1.0530 - val_acc: 0.6606\n",
      "Epoch 4/250\n",
      "390/390 [==============================] - 157s 401ms/step - loss: 1.2426 - acc: 0.6093 - val_loss: 0.9503 - val_acc: 0.6990\n",
      "Epoch 5/250\n",
      "390/390 [==============================] - 158s 406ms/step - loss: 1.1912 - acc: 0.6337 - val_loss: 0.9501 - val_acc: 0.7043\n",
      "Epoch 6/250\n",
      "390/390 [==============================] - 158s 404ms/step - loss: 1.1404 - acc: 0.6505 - val_loss: 0.9167 - val_acc: 0.7123\n",
      "Epoch 7/250\n",
      "390/390 [==============================] - 156s 399ms/step - loss: 1.1165 - acc: 0.6575 - val_loss: 0.8119 - val_acc: 0.7536\n",
      "Epoch 8/250\n",
      "390/390 [==============================] - 156s 399ms/step - loss: 1.1051 - acc: 0.6693 - val_loss: 1.0560 - val_acc: 0.6871\n",
      "Epoch 9/250\n",
      "390/390 [==============================] - 156s 399ms/step - loss: 1.0505 - acc: 0.6839 - val_loss: 0.7961 - val_acc: 0.7609\n",
      "Epoch 10/250\n",
      "390/390 [==============================] - 156s 399ms/step - loss: 1.0288 - acc: 0.6873 - val_loss: 0.7886 - val_acc: 0.7653\n",
      "Epoch 11/250\n",
      "390/390 [==============================] - 156s 399ms/step - loss: 1.0052 - acc: 0.6981 - val_loss: 0.8151 - val_acc: 0.7677\n",
      "Epoch 12/250\n",
      "390/390 [==============================] - 155s 397ms/step - loss: 0.9966 - acc: 0.7026 - val_loss: 0.7218 - val_acc: 0.7849\n",
      "Epoch 13/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.9767 - acc: 0.7073 - val_loss: 0.7375 - val_acc: 0.7873\n",
      "Epoch 14/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.9539 - acc: 0.7103 - val_loss: 0.7404 - val_acc: 0.7859\n",
      "Epoch 15/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.9500 - acc: 0.7160 - val_loss: 0.7225 - val_acc: 0.7840\n",
      "Epoch 16/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.9283 - acc: 0.7214 - val_loss: 0.7956 - val_acc: 0.7753\n",
      "Epoch 17/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.9043 - acc: 0.7260 - val_loss: 0.7144 - val_acc: 0.8003\n",
      "Epoch 18/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.8811 - acc: 0.7322 - val_loss: 0.7141 - val_acc: 0.7914\n",
      "Epoch 19/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.8693 - acc: 0.7347 - val_loss: 0.6629 - val_acc: 0.8068\n",
      "Epoch 20/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.8460 - acc: 0.7404 - val_loss: 0.6518 - val_acc: 0.8093\n",
      "Epoch 21/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.8437 - acc: 0.7434 - val_loss: 0.6353 - val_acc: 0.8138\n",
      "Epoch 22/250\n",
      "390/390 [==============================] - 155s 396ms/step - loss: 0.8181 - acc: 0.7485 - val_loss: 0.6353 - val_acc: 0.8155\n",
      "Epoch 23/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.8200 - acc: 0.7481 - val_loss: 0.6167 - val_acc: 0.8229\n",
      "Epoch 24/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.8064 - acc: 0.7531 - val_loss: 0.6624 - val_acc: 0.8098\n",
      "Epoch 25/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.7993 - acc: 0.7546 - val_loss: 0.6195 - val_acc: 0.8232\n",
      "Epoch 26/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.7899 - acc: 0.7600 - val_loss: 0.6012 - val_acc: 0.8244\n",
      "Epoch 27/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.7837 - acc: 0.7631 - val_loss: 0.6281 - val_acc: 0.8170\n",
      "Epoch 28/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.7763 - acc: 0.7627 - val_loss: 0.6182 - val_acc: 0.8204\n",
      "Epoch 29/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.7733 - acc: 0.7653 - val_loss: 0.5992 - val_acc: 0.8281\n",
      "Epoch 30/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.7661 - acc: 0.7680 - val_loss: 0.6715 - val_acc: 0.8065\n",
      "Epoch 31/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.7620 - acc: 0.7694 - val_loss: 0.6001 - val_acc: 0.8287\n",
      "Epoch 32/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.7569 - acc: 0.7742 - val_loss: 0.5742 - val_acc: 0.8391\n",
      "Epoch 33/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7549 - acc: 0.7730 - val_loss: 0.6329 - val_acc: 0.8161\n",
      "Epoch 34/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7490 - acc: 0.7738 - val_loss: 0.6141 - val_acc: 0.8216\n",
      "Epoch 35/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7473 - acc: 0.7754 - val_loss: 0.5810 - val_acc: 0.8386\n",
      "Epoch 36/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7389 - acc: 0.7775 - val_loss: 0.5655 - val_acc: 0.8410\n",
      "Epoch 37/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7412 - acc: 0.7783 - val_loss: 0.5700 - val_acc: 0.8421\n",
      "Epoch 38/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7299 - acc: 0.7807 - val_loss: 0.5777 - val_acc: 0.8365\n",
      "Epoch 39/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7374 - acc: 0.7797 - val_loss: 0.5976 - val_acc: 0.8302\n",
      "Epoch 40/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7291 - acc: 0.7819 - val_loss: 0.5783 - val_acc: 0.8350\n",
      "Epoch 41/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7284 - acc: 0.7834 - val_loss: 0.5807 - val_acc: 0.8346\n",
      "Epoch 42/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7224 - acc: 0.7850 - val_loss: 0.5847 - val_acc: 0.8347\n",
      "Epoch 43/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7256 - acc: 0.7839 - val_loss: 0.5669 - val_acc: 0.8382\n",
      "Epoch 44/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7215 - acc: 0.7846 - val_loss: 0.5972 - val_acc: 0.8302\n",
      "Epoch 45/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7183 - acc: 0.7882 - val_loss: 0.5575 - val_acc: 0.8439\n",
      "Epoch 46/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7098 - acc: 0.7892 - val_loss: 0.6131 - val_acc: 0.8286\n",
      "Epoch 47/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7129 - acc: 0.7903 - val_loss: 0.5823 - val_acc: 0.8370\n",
      "Epoch 48/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7086 - acc: 0.7903 - val_loss: 0.5668 - val_acc: 0.8442\n",
      "Epoch 49/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7084 - acc: 0.7891 - val_loss: 0.6080 - val_acc: 0.8273\n",
      "Epoch 50/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7088 - acc: 0.7912 - val_loss: 0.5531 - val_acc: 0.8502\n",
      "Epoch 51/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7033 - acc: 0.7914 - val_loss: 0.5383 - val_acc: 0.8464\n",
      "Epoch 52/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7031 - acc: 0.7916 - val_loss: 0.5640 - val_acc: 0.8417\n",
      "Epoch 53/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6999 - acc: 0.7948 - val_loss: 0.5853 - val_acc: 0.8342\n",
      "Epoch 54/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6932 - acc: 0.7948 - val_loss: 0.5610 - val_acc: 0.8452\n",
      "Epoch 55/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6993 - acc: 0.7955 - val_loss: 0.5654 - val_acc: 0.8394\n",
      "Epoch 56/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6965 - acc: 0.7970 - val_loss: 0.5507 - val_acc: 0.8524\n",
      "Epoch 57/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6930 - acc: 0.7963 - val_loss: 0.5974 - val_acc: 0.8336\n",
      "Epoch 58/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6976 - acc: 0.7943 - val_loss: 0.5457 - val_acc: 0.8537\n",
      "Epoch 59/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6928 - acc: 0.7951 - val_loss: 0.5417 - val_acc: 0.8511\n",
      "Epoch 60/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6883 - acc: 0.8005 - val_loss: 0.5428 - val_acc: 0.8525\n",
      "Epoch 61/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6916 - acc: 0.7971 - val_loss: 0.6210 - val_acc: 0.8244\n",
      "Epoch 62/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6878 - acc: 0.7999 - val_loss: 0.5533 - val_acc: 0.8542\n",
      "Epoch 63/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6809 - acc: 0.7998 - val_loss: 0.5672 - val_acc: 0.8448\n",
      "Epoch 64/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6853 - acc: 0.7992 - val_loss: 0.5813 - val_acc: 0.8398\n",
      "Epoch 65/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6840 - acc: 0.7992 - val_loss: 0.5424 - val_acc: 0.8512\n",
      "Epoch 66/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6845 - acc: 0.8018 - val_loss: 0.5472 - val_acc: 0.8493\n",
      "Epoch 67/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6814 - acc: 0.7999 - val_loss: 0.5660 - val_acc: 0.8428\n",
      "Epoch 68/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6805 - acc: 0.8004 - val_loss: 0.5491 - val_acc: 0.8481\n",
      "Epoch 69/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.6799 - acc: 0.8017 - val_loss: 0.5508 - val_acc: 0.8496\n",
      "Epoch 70/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6801 - acc: 0.8011 - val_loss: 0.5613 - val_acc: 0.8442\n",
      "Epoch 71/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6752 - acc: 0.8051 - val_loss: 0.5512 - val_acc: 0.8497\n",
      "Epoch 72/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.6702 - acc: 0.8039 - val_loss: 0.5882 - val_acc: 0.8393\n",
      "Epoch 73/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6729 - acc: 0.8048 - val_loss: 0.5638 - val_acc: 0.8449\n",
      "Epoch 74/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6745 - acc: 0.8022 - val_loss: 0.5602 - val_acc: 0.8477\n",
      "Epoch 75/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6733 - acc: 0.8029 - val_loss: 0.5571 - val_acc: 0.8484\n",
      "Epoch 76/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6218 - acc: 0.8219 - val_loss: 0.4882 - val_acc: 0.8699\n",
      "Epoch 77/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6121 - acc: 0.8259 - val_loss: 0.4837 - val_acc: 0.8694\n",
      "Epoch 78/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6038 - acc: 0.8279 - val_loss: 0.4912 - val_acc: 0.8667\n",
      "Epoch 79/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6038 - acc: 0.8259 - val_loss: 0.4843 - val_acc: 0.8693\n",
      "Epoch 80/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.5962 - acc: 0.8274 - val_loss: 0.4969 - val_acc: 0.8651\n",
      "Epoch 81/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5985 - acc: 0.8256 - val_loss: 0.4701 - val_acc: 0.8724\n",
      "Epoch 82/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5913 - acc: 0.8292 - val_loss: 0.4709 - val_acc: 0.8696\n",
      "Epoch 83/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5895 - acc: 0.8286 - val_loss: 0.4852 - val_acc: 0.8689\n",
      "Epoch 84/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5889 - acc: 0.8296 - val_loss: 0.4597 - val_acc: 0.8748\n",
      "Epoch 85/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5828 - acc: 0.8312 - val_loss: 0.4769 - val_acc: 0.8712\n",
      "Epoch 86/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5860 - acc: 0.8296 - val_loss: 0.4577 - val_acc: 0.8788\n",
      "Epoch 87/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5836 - acc: 0.8305 - val_loss: 0.4806 - val_acc: 0.8697\n",
      "Epoch 88/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.5811 - acc: 0.8310 - val_loss: 0.4644 - val_acc: 0.8717\n",
      "Epoch 89/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5802 - acc: 0.8324 - val_loss: 0.4677 - val_acc: 0.8730\n",
      "Epoch 90/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5731 - acc: 0.8338 - val_loss: 0.4575 - val_acc: 0.8725\n",
      "Epoch 91/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5746 - acc: 0.8327 - val_loss: 0.4578 - val_acc: 0.8738\n",
      "Epoch 92/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5747 - acc: 0.8312 - val_loss: 0.4679 - val_acc: 0.8708\n",
      "Epoch 93/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.5703 - acc: 0.8350 - val_loss: 0.4547 - val_acc: 0.8726\n",
      "Epoch 94/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5770 - acc: 0.8316 - val_loss: 0.4657 - val_acc: 0.8716\n",
      "Epoch 95/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5697 - acc: 0.8338 - val_loss: 0.4771 - val_acc: 0.8665\n",
      "Epoch 96/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5691 - acc: 0.8330 - val_loss: 0.4679 - val_acc: 0.8702\n",
      "Epoch 97/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5677 - acc: 0.8340 - val_loss: 0.4673 - val_acc: 0.8727\n",
      "Epoch 98/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5696 - acc: 0.8352 - val_loss: 0.4712 - val_acc: 0.8705\n",
      "Epoch 99/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5665 - acc: 0.8351 - val_loss: 0.4713 - val_acc: 0.8710\n",
      "Epoch 100/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5718 - acc: 0.8312 - val_loss: 0.4595 - val_acc: 0.8722\n",
      "Epoch 101/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5464 - acc: 0.8428 - val_loss: 0.4349 - val_acc: 0.8823\n",
      "Epoch 102/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5345 - acc: 0.8466 - val_loss: 0.4318 - val_acc: 0.8816\n",
      "Epoch 103/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.5333 - acc: 0.8465 - val_loss: 0.4305 - val_acc: 0.8823\n",
      "Epoch 104/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5373 - acc: 0.8433 - val_loss: 0.4250 - val_acc: 0.8850\n",
      "Epoch 105/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5330 - acc: 0.8454 - val_loss: 0.4216 - val_acc: 0.8864\n",
      "Epoch 106/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5291 - acc: 0.8457 - val_loss: 0.4333 - val_acc: 0.8831\n",
      "Epoch 107/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5250 - acc: 0.8482 - val_loss: 0.4312 - val_acc: 0.8807\n",
      "Epoch 108/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5216 - acc: 0.8506 - val_loss: 0.4475 - val_acc: 0.8761\n",
      "Epoch 109/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.5268 - acc: 0.8458 - val_loss: 0.4136 - val_acc: 0.8825\n",
      "Epoch 110/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5212 - acc: 0.8499 - val_loss: 0.4281 - val_acc: 0.8834\n",
      "Epoch 111/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5211 - acc: 0.8485 - val_loss: 0.4310 - val_acc: 0.8790\n",
      "Epoch 112/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.5206 - acc: 0.8482 - val_loss: 0.4194 - val_acc: 0.8857\n",
      "Epoch 113/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5214 - acc: 0.8498 - val_loss: 0.4262 - val_acc: 0.8829\n",
      "Epoch 114/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5144 - acc: 0.8496 - val_loss: 0.4336 - val_acc: 0.8832\n",
      "Epoch 115/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5250 - acc: 0.8467 - val_loss: 0.4232 - val_acc: 0.8832\n",
      "Epoch 116/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.5100 - acc: 0.8510 - val_loss: 0.4285 - val_acc: 0.8799\n",
      "Epoch 117/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5151 - acc: 0.8500 - val_loss: 0.4093 - val_acc: 0.8880\n",
      "Epoch 118/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5130 - acc: 0.8497 - val_loss: 0.4226 - val_acc: 0.8857\n",
      "Epoch 119/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5168 - acc: 0.8482 - val_loss: 0.4183 - val_acc: 0.8842\n",
      "Epoch 120/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5160 - acc: 0.8490 - val_loss: 0.4122 - val_acc: 0.8845\n",
      "Epoch 121/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5101 - acc: 0.8500 - val_loss: 0.4230 - val_acc: 0.8843\n",
      "Epoch 122/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5193 - acc: 0.8478 - val_loss: 0.4145 - val_acc: 0.8849\n",
      "Epoch 123/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5075 - acc: 0.8522 - val_loss: 0.4093 - val_acc: 0.8886\n",
      "Epoch 124/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5112 - acc: 0.8506 - val_loss: 0.4175 - val_acc: 0.8877\n",
      "Epoch 125/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5120 - acc: 0.8476 - val_loss: 0.4181 - val_acc: 0.8829\n",
      "Epoch 126/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4906 - acc: 0.8580 - val_loss: 0.3994 - val_acc: 0.8906\n",
      "Epoch 127/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4855 - acc: 0.8599 - val_loss: 0.3992 - val_acc: 0.8919\n",
      "Epoch 128/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4839 - acc: 0.8589 - val_loss: 0.4033 - val_acc: 0.8892\n",
      "Epoch 129/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4827 - acc: 0.8599 - val_loss: 0.3958 - val_acc: 0.8916\n",
      "Epoch 130/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4837 - acc: 0.8607 - val_loss: 0.4038 - val_acc: 0.8915\n",
      "Epoch 131/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4751 - acc: 0.8630 - val_loss: 0.3974 - val_acc: 0.8910\n",
      "Epoch 132/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4779 - acc: 0.8604 - val_loss: 0.4060 - val_acc: 0.8883\n",
      "Epoch 133/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4732 - acc: 0.8625 - val_loss: 0.4154 - val_acc: 0.8871\n",
      "Epoch 134/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4809 - acc: 0.8604 - val_loss: 0.3960 - val_acc: 0.8917\n",
      "Epoch 135/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4771 - acc: 0.8602 - val_loss: 0.3959 - val_acc: 0.8920\n",
      "Epoch 136/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4709 - acc: 0.8624 - val_loss: 0.4099 - val_acc: 0.8864\n",
      "Epoch 137/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4776 - acc: 0.8623 - val_loss: 0.3937 - val_acc: 0.8893\n",
      "Epoch 138/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4697 - acc: 0.8619 - val_loss: 0.4021 - val_acc: 0.8924\n",
      "Epoch 139/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4737 - acc: 0.8611 - val_loss: 0.3951 - val_acc: 0.8919\n",
      "Epoch 140/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4688 - acc: 0.8622 - val_loss: 0.3951 - val_acc: 0.8895\n",
      "Epoch 141/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4694 - acc: 0.8630 - val_loss: 0.3947 - val_acc: 0.8940\n",
      "Epoch 142/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4720 - acc: 0.8620 - val_loss: 0.3951 - val_acc: 0.8918\n",
      "Epoch 143/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4738 - acc: 0.8619 - val_loss: 0.3950 - val_acc: 0.8915\n",
      "Epoch 144/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4642 - acc: 0.8656 - val_loss: 0.3973 - val_acc: 0.8906\n",
      "Epoch 145/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4668 - acc: 0.8630 - val_loss: 0.3923 - val_acc: 0.8917\n",
      "Epoch 146/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4668 - acc: 0.8636 - val_loss: 0.3864 - val_acc: 0.8955\n",
      "Epoch 147/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4589 - acc: 0.8669 - val_loss: 0.3910 - val_acc: 0.8947\n",
      "Epoch 148/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4653 - acc: 0.8638 - val_loss: 0.3927 - val_acc: 0.8927\n",
      "Epoch 149/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4644 - acc: 0.8644 - val_loss: 0.3921 - val_acc: 0.8943\n",
      "Epoch 150/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4658 - acc: 0.8634 - val_loss: 0.3867 - val_acc: 0.8939\n",
      "Epoch 151/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4586 - acc: 0.8655 - val_loss: 0.3817 - val_acc: 0.8969\n",
      "Epoch 152/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4505 - acc: 0.8687 - val_loss: 0.3820 - val_acc: 0.8969\n",
      "Epoch 153/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4513 - acc: 0.8684 - val_loss: 0.3825 - val_acc: 0.8967\n",
      "Epoch 154/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4518 - acc: 0.8685 - val_loss: 0.3760 - val_acc: 0.8980\n",
      "Epoch 155/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4490 - acc: 0.8696 - val_loss: 0.3800 - val_acc: 0.8960\n",
      "Epoch 156/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4472 - acc: 0.8694 - val_loss: 0.3762 - val_acc: 0.8971\n",
      "Epoch 157/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4478 - acc: 0.8706 - val_loss: 0.3755 - val_acc: 0.8989\n",
      "Epoch 158/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4446 - acc: 0.8705 - val_loss: 0.3769 - val_acc: 0.8982\n",
      "Epoch 159/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4482 - acc: 0.8706 - val_loss: 0.3737 - val_acc: 0.8981\n",
      "Epoch 160/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4476 - acc: 0.8693 - val_loss: 0.3771 - val_acc: 0.8990\n",
      "Epoch 161/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4427 - acc: 0.8704 - val_loss: 0.3721 - val_acc: 0.8991\n",
      "Epoch 162/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4460 - acc: 0.8706 - val_loss: 0.3780 - val_acc: 0.8976\n",
      "Epoch 163/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4436 - acc: 0.8711 - val_loss: 0.3802 - val_acc: 0.8969\n",
      "Epoch 164/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4457 - acc: 0.8692 - val_loss: 0.3800 - val_acc: 0.8952\n",
      "Epoch 165/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4442 - acc: 0.8696 - val_loss: 0.3760 - val_acc: 0.8965\n",
      "Epoch 166/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4437 - acc: 0.8700 - val_loss: 0.3773 - val_acc: 0.8981\n",
      "Epoch 167/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4396 - acc: 0.8721 - val_loss: 0.3725 - val_acc: 0.8988\n",
      "Epoch 168/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4454 - acc: 0.8691 - val_loss: 0.3736 - val_acc: 0.8988\n",
      "Epoch 169/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4381 - acc: 0.8728 - val_loss: 0.3793 - val_acc: 0.8970\n",
      "Epoch 170/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4415 - acc: 0.8716 - val_loss: 0.3809 - val_acc: 0.8959\n",
      "Epoch 171/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4403 - acc: 0.8696 - val_loss: 0.3698 - val_acc: 0.9000\n",
      "Epoch 172/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4377 - acc: 0.8718 - val_loss: 0.3771 - val_acc: 0.8992\n",
      "Epoch 173/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4438 - acc: 0.8710 - val_loss: 0.3753 - val_acc: 0.8989\n",
      "Epoch 174/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4383 - acc: 0.8712 - val_loss: 0.3773 - val_acc: 0.8986\n",
      "Epoch 175/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4431 - acc: 0.8702 - val_loss: 0.3864 - val_acc: 0.8962\n",
      "Epoch 176/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4406 - acc: 0.8711 - val_loss: 0.3724 - val_acc: 0.8980\n",
      "Epoch 177/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4233 - acc: 0.8781 - val_loss: 0.3705 - val_acc: 0.9000\n",
      "Epoch 178/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4340 - acc: 0.8719 - val_loss: 0.3716 - val_acc: 0.9001\n",
      "Epoch 179/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4338 - acc: 0.8731 - val_loss: 0.3736 - val_acc: 0.8995\n",
      "Epoch 180/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4312 - acc: 0.8739 - val_loss: 0.3705 - val_acc: 0.8992\n",
      "Epoch 181/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4314 - acc: 0.8749 - val_loss: 0.3699 - val_acc: 0.9005\n",
      "Epoch 182/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4300 - acc: 0.8752 - val_loss: 0.3691 - val_acc: 0.9000\n",
      "Epoch 183/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4260 - acc: 0.8765 - val_loss: 0.3721 - val_acc: 0.8990\n",
      "Epoch 184/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4291 - acc: 0.8751 - val_loss: 0.3723 - val_acc: 0.8997\n",
      "Epoch 185/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4358 - acc: 0.8740 - val_loss: 0.3705 - val_acc: 0.8990\n",
      "Epoch 186/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4223 - acc: 0.8768 - val_loss: 0.3721 - val_acc: 0.8995\n",
      "Epoch 187/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4346 - acc: 0.8747 - val_loss: 0.3759 - val_acc: 0.8986\n",
      "Epoch 188/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4237 - acc: 0.8776 - val_loss: 0.3725 - val_acc: 0.8996\n",
      "Epoch 189/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4359 - acc: 0.8728 - val_loss: 0.3679 - val_acc: 0.9017\n",
      "Epoch 190/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4271 - acc: 0.8758 - val_loss: 0.3704 - val_acc: 0.9005\n",
      "Epoch 191/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4301 - acc: 0.8737 - val_loss: 0.3710 - val_acc: 0.8982\n",
      "Epoch 192/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4285 - acc: 0.8739 - val_loss: 0.3682 - val_acc: 0.9001\n",
      "Epoch 193/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4274 - acc: 0.8754 - val_loss: 0.3689 - val_acc: 0.9019\n",
      "Epoch 194/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4269 - acc: 0.8753 - val_loss: 0.3692 - val_acc: 0.8993\n",
      "Epoch 195/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4304 - acc: 0.8742 - val_loss: 0.3709 - val_acc: 0.8997\n",
      "Epoch 196/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4294 - acc: 0.8746 - val_loss: 0.3681 - val_acc: 0.9011\n",
      "Epoch 197/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4211 - acc: 0.8762 - val_loss: 0.3708 - val_acc: 0.9003\n",
      "Epoch 198/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4310 - acc: 0.8728 - val_loss: 0.3678 - val_acc: 0.9014\n",
      "Epoch 199/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4204 - acc: 0.8765 - val_loss: 0.3702 - val_acc: 0.9012\n",
      "Epoch 200/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4273 - acc: 0.8751 - val_loss: 0.3668 - val_acc: 0.9025\n",
      "Epoch 201/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4240 - acc: 0.8749 - val_loss: 0.3681 - val_acc: 0.9005\n",
      "Epoch 202/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4231 - acc: 0.8781 - val_loss: 0.3656 - val_acc: 0.9013\n",
      "Epoch 203/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4227 - acc: 0.8775 - val_loss: 0.3687 - val_acc: 0.9015\n",
      "Epoch 204/250\n",
      "390/390 [==============================] - 155s 397ms/step - loss: 0.4158 - acc: 0.8796 - val_loss: 0.3684 - val_acc: 0.9012\n",
      "Epoch 205/250\n",
      "390/390 [==============================] - 157s 401ms/step - loss: 0.4217 - acc: 0.8764 - val_loss: 0.3680 - val_acc: 0.9023\n",
      "Epoch 206/250\n",
      "390/390 [==============================] - 156s 400ms/step - loss: 0.4236 - acc: 0.8761 - val_loss: 0.3667 - val_acc: 0.9018\n",
      "Epoch 207/250\n",
      "390/390 [==============================] - 156s 400ms/step - loss: 0.4199 - acc: 0.8769 - val_loss: 0.3662 - val_acc: 0.9010\n",
      "Epoch 208/250\n",
      "390/390 [==============================] - 156s 399ms/step - loss: 0.4253 - acc: 0.8764 - val_loss: 0.3664 - val_acc: 0.9013\n",
      "Epoch 209/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4247 - acc: 0.8758 - val_loss: 0.3660 - val_acc: 0.9031\n",
      "Epoch 210/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4194 - acc: 0.8785 - val_loss: 0.3655 - val_acc: 0.9033\n",
      "Epoch 211/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4193 - acc: 0.8779 - val_loss: 0.3629 - val_acc: 0.9019\n",
      "Epoch 212/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4196 - acc: 0.8778 - val_loss: 0.3645 - val_acc: 0.9022\n",
      "Epoch 213/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4246 - acc: 0.8755 - val_loss: 0.3642 - val_acc: 0.9027\n",
      "Epoch 214/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4190 - acc: 0.8764 - val_loss: 0.3657 - val_acc: 0.9009\n",
      "Epoch 215/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4227 - acc: 0.8781 - val_loss: 0.3676 - val_acc: 0.9025\n",
      "Epoch 216/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4229 - acc: 0.8768 - val_loss: 0.3646 - val_acc: 0.9020\n",
      "Epoch 217/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4231 - acc: 0.8765 - val_loss: 0.3667 - val_acc: 0.9016\n",
      "Epoch 218/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4218 - acc: 0.8792 - val_loss: 0.3687 - val_acc: 0.9016\n",
      "Epoch 219/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4169 - acc: 0.8784 - val_loss: 0.3659 - val_acc: 0.9037\n",
      "Epoch 220/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4213 - acc: 0.8785 - val_loss: 0.3651 - val_acc: 0.9020\n",
      "Epoch 221/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4195 - acc: 0.8782 - val_loss: 0.3643 - val_acc: 0.9025\n",
      "Epoch 222/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4208 - acc: 0.8762 - val_loss: 0.3650 - val_acc: 0.9012\n",
      "Epoch 223/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4188 - acc: 0.8785 - val_loss: 0.3650 - val_acc: 0.9010\n",
      "Epoch 224/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4206 - acc: 0.8769 - val_loss: 0.3642 - val_acc: 0.9022\n",
      "Epoch 225/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4205 - acc: 0.8748 - val_loss: 0.3668 - val_acc: 0.9015\n",
      "Epoch 226/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4227 - acc: 0.8771 - val_loss: 0.3664 - val_acc: 0.9016\n",
      "Epoch 227/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4175 - acc: 0.8768 - val_loss: 0.3649 - val_acc: 0.9018\n",
      "Epoch 228/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4146 - acc: 0.8783 - val_loss: 0.3676 - val_acc: 0.9019\n",
      "Epoch 229/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4170 - acc: 0.8766 - val_loss: 0.3669 - val_acc: 0.9019\n",
      "Epoch 230/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4195 - acc: 0.8782 - val_loss: 0.3663 - val_acc: 0.9020\n",
      "Epoch 231/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4146 - acc: 0.8785 - val_loss: 0.3658 - val_acc: 0.9014\n",
      "Epoch 232/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4188 - acc: 0.8797 - val_loss: 0.3658 - val_acc: 0.9026\n",
      "Epoch 233/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4184 - acc: 0.8780 - val_loss: 0.3656 - val_acc: 0.9027\n",
      "Epoch 234/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4215 - acc: 0.8773 - val_loss: 0.3656 - val_acc: 0.9011\n",
      "Epoch 235/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4186 - acc: 0.8783 - val_loss: 0.3663 - val_acc: 0.9013\n",
      "Epoch 236/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4128 - acc: 0.8797 - val_loss: 0.3658 - val_acc: 0.9013\n",
      "Epoch 237/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4186 - acc: 0.8783 - val_loss: 0.3660 - val_acc: 0.9012\n",
      "Epoch 238/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4155 - acc: 0.8792 - val_loss: 0.3656 - val_acc: 0.9015\n",
      "Epoch 239/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4203 - acc: 0.8767 - val_loss: 0.3668 - val_acc: 0.9011\n",
      "Epoch 240/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4194 - acc: 0.8783 - val_loss: 0.3657 - val_acc: 0.9011\n",
      "Epoch 241/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4211 - acc: 0.8770 - val_loss: 0.3655 - val_acc: 0.9018\n",
      "Epoch 242/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4116 - acc: 0.8804 - val_loss: 0.3649 - val_acc: 0.9025\n",
      "Epoch 243/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4201 - acc: 0.8775 - val_loss: 0.3655 - val_acc: 0.9025\n",
      "Epoch 244/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4179 - acc: 0.8777 - val_loss: 0.3649 - val_acc: 0.9017\n",
      "Epoch 245/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4219 - acc: 0.8759 - val_loss: 0.3649 - val_acc: 0.9017\n",
      "Epoch 246/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4117 - acc: 0.8801 - val_loss: 0.3654 - val_acc: 0.9023\n",
      "Epoch 247/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4194 - acc: 0.8772 - val_loss: 0.3641 - val_acc: 0.9034\n",
      "Epoch 248/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4160 - acc: 0.8783 - val_loss: 0.3630 - val_acc: 0.9038\n",
      "Epoch 249/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4231 - acc: 0.8750 - val_loss: 0.3637 - val_acc: 0.9022\n",
      "Epoch 250/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4183 - acc: 0.8786 - val_loss: 0.3645 - val_acc: 0.9013\n",
      "{'val_loss': [1.5434356340408326, 1.1258149061203002, 1.0529549388885497, 0.95034396286010747, 0.95013308086395265, 0.91671153006553652, 0.81185307016372676, 1.0560010885238647, 0.79613626575469976, 0.78856168498992918, 0.81513289489746099, 0.72178837394714357, 0.73753624334335333, 0.74038853707313534, 0.72249449129104615, 0.79557975053787233, 0.71443974943161015, 0.71406953649520877, 0.66288874888420102, 0.6518380347251892, 0.63528442444801325, 0.63530824193954472, 0.61670734057426457, 0.66243103685379023, 0.61950780920982362, 0.60118625221252442, 0.62814018049240117, 0.61821136865615844, 0.59923357710838321, 0.67147289924621578, 0.60014952917098996, 0.57421350073814392, 0.63293058290481563, 0.61413907151222225, 0.58099061465263369, 0.56548011341094973, 0.57004739937782289, 0.57771571121215826, 0.59757643775939939, 0.57826159958839418, 0.58072964010238648, 0.58469004297256466, 0.56693359718322756, 0.5971707654476166, 0.55753450245857239, 0.61312366456985479, 0.5822628577709198, 0.56680996880531309, 0.60802466936111454, 0.55313376278877258, 0.53825575404167181, 0.56397207117080683, 0.58533357362747196, 0.56104719576835638, 0.56540825471878053, 0.5507257898807526, 0.59740658969879146, 0.54567935528755185, 0.54171613988876344, 0.54284796638488775, 0.62097120099067693, 0.55332985563278203, 0.56718552703857417, 0.58130892634391784, 0.5424428527355194, 0.54723837709426881, 0.56600854806900025, 0.54906285524368281, 0.55083533143997188, 0.56134757313728334, 0.55124365262985231, 0.58820301122665408, 0.56379883742332459, 0.56019655103683474, 0.55710947256088261, 0.48821637854576111, 0.48372525029182434, 0.49116680674552915, 0.48430523519515989, 0.49689690732955932, 0.47010862517356872, 0.47089478068351748, 0.48515863189697267, 0.45965447082519534, 0.47688335685729982, 0.45771792426109315, 0.48058999819755555, 0.46444438052177428, 0.46771104135513308, 0.45745631504058837, 0.45781955022811888, 0.46787357616424563, 0.45470729112625125, 0.46569477994441988, 0.47707840352058412, 0.46785336985588072, 0.46732154793739317, 0.47121453804969787, 0.47130050177574156, 0.45948193016052247, 0.43492195653915405, 0.43177326722145082, 0.43052429351806643, 0.42496807048320773, 0.42155438885688784, 0.43333560509681701, 0.43123701438903811, 0.44745009016990661, 0.41361058254241945, 0.42806454515457154, 0.43101483073234559, 0.41940549087524415, 0.4262335775375366, 0.43364839987754822, 0.42317193126678465, 0.42853009438514711, 0.40932817258834842, 0.42264183459281923, 0.41830561146736145, 0.41220114674568176, 0.42301781759262086, 0.41445427422523501, 0.40926827106475833, 0.41750700540542601, 0.41814418091773986, 0.39939708652496336, 0.39918131394386291, 0.40326075754165647, 0.3958213119506836, 0.40379271006584166, 0.39737285799980165, 0.40599585227966306, 0.41540740904808043, 0.39604961824417112, 0.39588488044738768, 0.40990783562660216, 0.39373813185691836, 0.40209175009727477, 0.39511250252723695, 0.39512256369590759, 0.39472868418693541, 0.39505214676856992, 0.39499689645767211, 0.39727873454093932, 0.39226789908409121, 0.38638600215911867, 0.39102224769592286, 0.39266233901977537, 0.39208719882965087, 0.38669717636108397, 0.3817301601409912, 0.38197511968612669, 0.3824538633823395, 0.37597452206611631, 0.38002386608123778, 0.3761859836578369, 0.37547753796577454, 0.37693296079635619, 0.37372981405258177, 0.37706913981437684, 0.37207537288665771, 0.37802881202697752, 0.38015209035873415, 0.38003891978263854, 0.37601032342910767, 0.37731604347229003, 0.37252567048072815, 0.37357555580139162, 0.3792808657169342, 0.38088427481651305, 0.36979018898010252, 0.37712745752334592, 0.3752868043899536, 0.37729648194313048, 0.38639519715309145, 0.37235339965820313, 0.37054552950859071, 0.37163046483993528, 0.37358409795761111, 0.37054867038726808, 0.36985203876495359, 0.36906854739189149, 0.37213267288208007, 0.37227040519714355, 0.3705091510772705, 0.37213657822608948, 0.37588057050704954, 0.37253608312606812, 0.3679158114910126, 0.37040586853027346, 0.3710382598400116, 0.36819866833686826, 0.36891567130088804, 0.36917122254371643, 0.37089855623245238, 0.36808927783966067, 0.37079372105598452, 0.36775009245872498, 0.37021622991561892, 0.36676858968734743, 0.3681019660949707, 0.36559600906372069, 0.36871289930343626, 0.3684041875362396, 0.36803856663703921, 0.36668211512565613, 0.36622182841300965, 0.36641469836235047, 0.36602188510894773, 0.36547687816619873, 0.36289431390762328, 0.3644535096168518, 0.36420499386787414, 0.36572673015594481, 0.36755600810050965, 0.36459574480056761, 0.36671193099021909, 0.36865636801719665, 0.36589593501091006, 0.36508620319366453, 0.36431653122901919, 0.36504897871017455, 0.36498558797836306, 0.36418798904418948, 0.3667602075099945, 0.36637795553207397, 0.3649035864830017, 0.36762639164924621, 0.3668711437702179, 0.36631480946540834, 0.36584018430709841, 0.36575808463096621, 0.36560468645095823, 0.36555675363540652, 0.36631099228858949, 0.36577963161468507, 0.36602733898162843, 0.36563904824256899, 0.36684866418838502, 0.36574229001998904, 0.36548278121948241, 0.36489976468086244, 0.36546578493118287, 0.36486298713684084, 0.36488206334114076, 0.36539640545845031, 0.36414739561080933, 0.36296739940643308, 0.36374494419097902, 0.36450429015159608], 'acc': [0.38023339748803264, 0.51848732759679472, 0.57701716393968561, 0.60923965349387077, 0.63376243186371806, 0.65034488288110215, 0.65752325950618884, 0.6693334937822295, 0.68399101700352904, 0.68729948666679352, 0.69806705169047456, 0.70259865258877419, 0.70741097205030179, 0.71033846651241872, 0.715972890561691, 0.72140680137336888, 0.72597850499185268, 0.73215431508476392, 0.73476098812961177, 0.7403954122936186, 0.74348331725402328, 0.74861645815194244, 0.74817532888007998, 0.75316811040077292, 0.75465190888675004, 0.75992540900891581, 0.76297321142123842, 0.76273259548258243, 0.76537937122849187, 0.76788578757805281, 0.76942974013474497, 0.77416185430888373, 0.77319939045864761, 0.77382098171318581, 0.77542508818761335, 0.77749037536092391, 0.77831247994866859, 0.78079884508155573, 0.77977622712235972, 0.78196182226499844, 0.78338546679499521, 0.78496952199538161, 0.78396695540583894, 0.78458854664125466, 0.78819778637125737, 0.78922040425396367, 0.79024302215579234, 0.79032322748142592, 0.78910009622726829, 0.7912255373756818, 0.79140599932011246, 0.79162660256410255, 0.79487797047514597, 0.7947345204103976, 0.79549647096567211, 0.79698026945164924, 0.79637872952851951, 0.79427333971151448, 0.79513221153846159, 0.80052935513005952, 0.79712588310854204, 0.79996791785062416, 0.79980750723760174, 0.7991786858974359, 0.79917308923596364, 0.80181264037844235, 0.79986766121270447, 0.80050930383689589, 0.80172275641025637, 0.80095937704534659, 0.80518126399768708, 0.80394631410256412, 0.80484039142111152, 0.8021234746115592, 0.8029355149181906, 0.82174366377927499, 0.82587423804940652, 0.82791947387218623, 0.82593439204363173, 0.82742387820512819, 0.8256061335902376, 0.82922676282051277, 0.82867694278136994, 0.8295837343408391, 0.83124799482861433, 0.8295837343408391, 0.83046599296105383, 0.83102964743589747, 0.83243102338171016, 0.8337949581956281, 0.83277189609214286, 0.83122794349720586, 0.83501602564102562, 0.83158718691702138, 0.83383461665678837, 0.83295235803657364, 0.83399439102564099, 0.83519810717343745, 0.8351196210661529, 0.83118784091087883, 0.84277751040756987, 0.84656721202463614, 0.84645432692307687, 0.8431880218177249, 0.84540423486031591, 0.84568495350003359, 0.84819136992608424, 0.85059752971421532, 0.84575320512820518, 0.84985147719974308, 0.84845203725351592, 0.84823717948717947, 0.84987154781014473, 0.8495548604618558, 0.84664741740763705, 0.85096153846153844, 0.8500321130570343, 0.84973532244453154, 0.84823147260802356, 0.84891321781854201, 0.85001604106512674, 0.84787054862354982, 0.85216153352582613, 0.85057747838280695, 0.84758983000295451, 0.85799278846153848, 0.85988680150314412, 0.8588541666666667, 0.85994701346833502, 0.86074350338774608, 0.86300930377952867, 0.86046278472890603, 0.86250802055168585, 0.86048283604119191, 0.86028232276535277, 0.86246791783150167, 0.86230750721847926, 0.8618663779466168, 0.86107772435897434, 0.86215478486178698, 0.86298076923076927, 0.86196663458453637, 0.86186637792749443, 0.86558686580578337, 0.86302083333333335, 0.86359987154784845, 0.86681905678537052, 0.86377125437304803, 0.86435274306038157, 0.86343038181559495, 0.86551572026294665, 0.86866987179487176, 0.86837668587706685, 0.86846955128205128, 0.86962106617199897, 0.86937099358974357, 0.87058870705190594, 0.87050417469492614, 0.8705686557778648, 0.86942572984934086, 0.87034809115149481, 0.87061298076923077, 0.87104988767429925, 0.86917951182440145, 0.86956608915007727, 0.87001201923076921, 0.87204961460526353, 0.86914501126699051, 0.87281650641025643, 0.87165142767391868, 0.8696009955233156, 0.8718149038461539, 0.87096579959550269, 0.87121394230769234, 0.87018768046198269, 0.87112636484239214, 0.87806785366724116, 0.87191209494398314, 0.87313522619814077, 0.87385817307692304, 0.87491979467436642, 0.87512030802669527, 0.8765454399103374, 0.87510025665704205, 0.87401748479923302, 0.87680461980763402, 0.87473958333333335, 0.8776091843096967, 0.87275425088225855, 0.87578124999999996, 0.8736766121079228, 0.87391618493281653, 0.87542067307692306, 0.87546162488143564, 0.87423804940648064, 0.87457892207866839, 0.8762419871794872, 0.87279222868311157, 0.87656400388810052, 0.87514022435897432, 0.87491979469348879, 0.87812800769971122, 0.87742854843326468, 0.87958733974358971, 0.87636480416789142, 0.87606272060289037, 0.87690304487179482, 0.87644508664777976, 0.87574189922386614, 0.87848893168418496, 0.87784728904087117, 0.87778445512820513, 0.87542107799782143, 0.87642501607565981, 0.87810496794871795, 0.87682467111992002, 0.87648522804085072, 0.87920673076923073, 0.87839193964662965, 0.87852903436612428, 0.87818816169393643, 0.87618302853397345, 0.87850898299647096, 0.87690304487179482, 0.87479929347488461, 0.8771254411484134, 0.87680288461538458, 0.87832852103291781, 0.8766843118191836, 0.87825144508670516, 0.87848557692307694, 0.87969201151132204, 0.87801059728336395, 0.87726362179487183, 0.878308469644142, 0.8797521655437921, 0.878351798330122, 0.87916666666666665, 0.87670600509980434, 0.87834857238344866, 0.87702323717948716, 0.88033365417375831, 0.87746631374411144, 0.87766682709644039, 0.87590230993250073, 0.88015815671162489, 0.87724358974358974, 0.8782915863840719, 0.87495993589743593, 0.87859264607604071], 'loss': [2.0500930321304547, 1.4976031004636763, 1.3322521097047382, 1.2427266239928709, 1.1910975696874575, 1.1408146837601343, 1.1161276076441971, 1.1049077060691508, 1.0504133961007196, 1.0287767725951555, 1.0053653004347685, 0.99668085292091646, 0.97646549316335263, 0.95398846627199618, 0.94987367336896122, 0.92847240066773085, 0.90440205996871037, 0.88115673597554267, 0.86922510312006651, 0.84589120951225105, 0.84352001820308187, 0.81794718291310498, 0.81988012924537013, 0.80634027992948565, 0.79932583908182631, 0.78999661376241459, 0.78393393577120418, 0.77628096478238562, 0.77329277789550677, 0.76627794987580922, 0.76185683564264728, 0.75699695930444244, 0.75475061033488777, 0.74871661458827765, 0.74730557077084747, 0.73904934931841648, 0.7412303638909542, 0.72970465998791867, 0.73729597097564514, 0.72907545348945468, 0.72828077042297867, 0.72238196669230681, 0.7256111024395181, 0.72155473633458078, 0.71815430582583162, 0.70980651907558279, 0.71294381463990442, 0.70862160495431292, 0.70829771817170317, 0.70872124076310283, 0.70328714599355424, 0.70311937446777639, 0.69989626055048615, 0.69330423728066604, 0.69937906890180124, 0.69659313193337447, 0.69297554032482089, 0.69757942199936496, 0.69283582033255164, 0.68824225687085727, 0.69141288519900967, 0.68769579519627377, 0.68080361144910606, 0.68532629074194495, 0.68404378162489998, 0.68443303544971856, 0.6813727039001215, 0.68019050336779019, 0.67986287138400936, 0.68030209403889408, 0.6751017679302288, 0.67023973747705801, 0.67278697946188659, 0.67474586131408876, 0.67316866125630614, 0.6220359365872018, 0.61215748693608762, 0.60362235305398793, 0.6038627377258573, 0.59620493681002884, 0.59853249330817693, 0.59127274812796182, 0.58941676561925838, 0.58900892838744889, 0.58282337226583136, 0.58615248242287366, 0.58359637494113226, 0.58113414897368509, 0.58016810290584742, 0.57298868923288315, 0.57459643983213726, 0.57453052795656145, 0.5703123781161431, 0.5770923207605505, 0.56965662492633362, 0.56920835793611135, 0.56773776381443708, 0.56966768518295818, 0.56616567920321592, 0.5718542139978412, 0.54645646564043371, 0.53441386705489735, 0.53333085087629462, 0.53740659626960452, 0.53286298117960418, 0.52918233761888378, 0.5250945068587467, 0.52163992036716045, 0.5268422603607178, 0.52131586635273541, 0.52113329876605086, 0.5205736680672719, 0.52133197015343991, 0.51448902828134246, 0.52507809600961641, 0.51000975462106557, 0.51514127158590362, 0.51304713465251872, 0.51675886302029506, 0.51614419229144581, 0.51000205580148705, 0.51928128601841561, 0.50756029726834029, 0.51119951913859318, 0.51197630941466254, 0.49055510705862287, 0.48563049917391038, 0.48391140898068746, 0.48268456871913806, 0.4837105608651911, 0.47504157707558864, 0.47784057516756445, 0.47318207340856988, 0.48089303398958416, 0.47702228242784195, 0.47088232270257002, 0.47761299457540868, 0.46969537511061582, 0.47374226427995242, 0.4689312421536706, 0.46944305308354206, 0.47210813093575521, 0.47383986978068887, 0.46408716858620053, 0.46675689571943041, 0.46650452617084515, 0.45908706286491247, 0.46540707114571211, 0.46451400494323086, 0.46582692645473439, 0.45860423250461491, 0.45047901272773744, 0.45130484936401183, 0.45179440891131378, 0.44896797860709997, 0.44715718328952792, 0.44778288711061986, 0.44434762929369415, 0.44829397257758058, 0.44741440760159973, 0.44275556509985081, 0.44601459923462988, 0.44362206905745605, 0.44592669930646417, 0.44419520801751322, 0.44374510806340439, 0.43976887889259048, 0.44536717197319081, 0.4381453255812327, 0.44151981068338231, 0.44032839331094009, 0.43767339694194307, 0.4438495826246498, 0.43834728063681189, 0.4431184863179855, 0.44052599463312764, 0.42328659952509123, 0.4340174231519292, 0.43375465859003776, 0.43120741225205933, 0.4313154271159908, 0.43003183751485652, 0.42587280815123901, 0.4289917024351444, 0.43586203781962585, 0.42237602524700785, 0.43458874026934308, 0.42364402087590752, 0.43593454765401513, 0.42712327425296492, 0.43009577067685428, 0.42862314620344166, 0.42737260842934632, 0.42668627815975468, 0.43036035353417651, 0.42938593242234974, 0.42110521457134148, 0.43099280008920587, 0.42036736703737448, 0.42729021937419209, 0.42401468375190388, 0.42323386391883561, 0.42265328386883438, 0.41580226371685663, 0.42178688885006449, 0.42364652945821701, 0.41991139191847582, 0.42520803632304388, 0.4248107680055932, 0.41931660497727818, 0.41928270360694547, 0.41957767957296127, 0.42471181577621914, 0.41871649166599023, 0.42273363066025271, 0.42277970874695814, 0.42316708876945763, 0.42177404455649548, 0.4168052863408605, 0.42130149548582285, 0.41931043085778108, 0.42093560513602568, 0.41874255832281104, 0.42064506262540818, 0.42054364175159503, 0.42241424285795659, 0.41753028531869252, 0.41470869756066192, 0.41687775237613051, 0.41957811716472637, 0.41464181420130608, 0.41869077269067967, 0.41838675898164224, 0.42153709821211988, 0.41861941265034913, 0.41274301978805183, 0.41855051813021482, 0.41548118736499395, 0.42032517581776158, 0.41934951123496755, 0.42105607233750514, 0.4116980476585152, 0.42009779321370999, 0.41788316102872342, 0.4220052810483228, 0.41171902098484198, 0.41943459384716475, 0.41597282764960652, 0.42307444910208386, 0.41839389100087016], 'val_acc': [0.50719999999999998, 0.63780000000000003, 0.66059999999999997, 0.69899999999999995, 0.70430000000000004, 0.71230000000000004, 0.75360000000000005, 0.68710000000000004, 0.76090000000000002, 0.76529999999999998, 0.76770000000000005, 0.78490000000000004, 0.7873, 0.78590000000000004, 0.78400000000000003, 0.77529999999999999, 0.80030000000000001, 0.79139999999999999, 0.80679999999999996, 0.80930000000000002, 0.81379999999999997, 0.8155, 0.82289999999999996, 0.80979999999999996, 0.82320000000000004, 0.82440000000000002, 0.81699999999999995, 0.82040000000000002, 0.82809999999999995, 0.80649999999999999, 0.82869999999999999, 0.83909999999999996, 0.81610000000000005, 0.8216, 0.83860000000000001, 0.84099999999999997, 0.84209999999999996, 0.83650000000000002, 0.83020000000000005, 0.83499999999999996, 0.83460000000000001, 0.8347, 0.83819999999999995, 0.83020000000000005, 0.84389999999999998, 0.8286, 0.83699999999999997, 0.84419999999999995, 0.82730000000000004, 0.85019999999999996, 0.84640000000000004, 0.8417, 0.83420000000000005, 0.84519999999999995, 0.83940000000000003, 0.85240000000000005, 0.83360000000000001, 0.85370000000000001, 0.85109999999999997, 0.85250000000000004, 0.82440000000000002, 0.85419999999999996, 0.8448, 0.83979999999999999, 0.85119999999999996, 0.84930000000000005, 0.84279999999999999, 0.84809999999999997, 0.84960000000000002, 0.84419999999999995, 0.84970000000000001, 0.83930000000000005, 0.84489999999999998, 0.84770000000000001, 0.84840000000000004, 0.86990000000000001, 0.86939999999999995, 0.86670000000000003, 0.86929999999999996, 0.86509999999999998, 0.87239999999999995, 0.86960000000000004, 0.86890000000000001, 0.87480000000000002, 0.87119999999999997, 0.87880000000000003, 0.86970000000000003, 0.87170000000000003, 0.873, 0.87250000000000005, 0.87380000000000002, 0.87079999637603756, 0.87260000000000004, 0.87160000000000004, 0.86650000000000005, 0.87019999999999997, 0.87270000000000003, 0.87050000000000005, 0.871, 0.87219999999999998, 0.88229999999999997, 0.88160000000000005, 0.88229999999999997, 0.88500000000000001, 0.88639999999999997, 0.8831, 0.88070000000000004, 0.87609999999999999, 0.88249999999999995, 0.88339999999999996, 0.879, 0.88570000000000004, 0.88290000000000002, 0.88319999999999999, 0.88319999999999999, 0.87990000000000002, 0.88800000000000001, 0.88570000000000004, 0.88419999999999999, 0.88449999999999995, 0.88429999999999997, 0.88490000000000002, 0.88859999999999995, 0.88770000000000004, 0.88290000000000002, 0.89059999999999995, 0.89190000000000003, 0.88919999999999999, 0.89159999999999995, 0.89149999999999996, 0.89100000000000001, 0.88829999999999998, 0.8871, 0.89170000000000005, 0.89200000000000002, 0.88639999999999997, 0.88929999999999998, 0.89239999999999997, 0.89190000000000003, 0.88949999999999996, 0.89400000000000002, 0.89180000000000004, 0.89149999999999996, 0.89059999999999995, 0.89170000000000005, 0.89549999999999996, 0.89470000000000005, 0.89270000000000005, 0.89429999999999998, 0.89390000000000003, 0.89690000000000003, 0.89690000000000003, 0.89670000000000005, 0.89800000000000002, 0.89600000000000002, 0.89710000000000001, 0.89890000000000003, 0.8982, 0.89810000000000001, 0.89900000000000002, 0.89910000000000001, 0.89759999999999995, 0.89690000000000003, 0.8952, 0.89649999999999996, 0.89810000000000001, 0.89880000000000004, 0.89880000000000004, 0.89700000000000002, 0.89590000000000003, 0.90000000000000002, 0.8992, 0.89890000000000003, 0.89859999999999995, 0.8962, 0.89800000000000002, 0.90000000000000002, 0.90010000000000001, 0.89949999999999997, 0.8992, 0.90049999999999997, 0.90000000000000002, 0.89900000000000002, 0.89970000000000006, 0.89900000000000002, 0.89949999999999997, 0.89859999999999995, 0.89959999999999996, 0.90169999999999995, 0.90049999999999997, 0.8982, 0.90010000000000001, 0.90190000000000003, 0.89929999999999999, 0.89970000000000006, 0.90110000000000001, 0.90029999999999999, 0.90139999999999998, 0.9012, 0.90249999999999997, 0.90049999999999997, 0.90129999999999999, 0.90149999999999997, 0.9012, 0.90229999999999999, 0.90180000000000005, 0.90100000000000002, 0.90129999999999999, 0.90310000000000001, 0.90329999999999999, 0.90190000000000003, 0.9022, 0.90269999999999995, 0.90090000000000003, 0.90249999999999997, 0.90200000000000002, 0.90159999999999996, 0.90159999999999996, 0.90369999999999995, 0.90200000000000002, 0.90249999999999997, 0.9012, 0.90100000000000002, 0.9022, 0.90149999999999997, 0.90159999999999996, 0.90180000000000005, 0.90190000000000003, 0.90190000000000003, 0.90200000000000002, 0.90139999999999998, 0.90259999999999996, 0.90269999999999995, 0.90110000000000001, 0.90129999999999999, 0.90129999999999999, 0.9012, 0.90149999999999997, 0.90110000000000001, 0.90110000000000001, 0.90180000000000005, 0.90249999999999997, 0.90249999999999997, 0.90169999999999995, 0.90169999999999995, 0.90229999999999999, 0.90339999999999998, 0.90380000000000005, 0.9022, 0.90129999999999999]}\n"
     ]
    }
   ],
   "source": [
    "# Create model and get ready - reproducible cell\n",
    "act, act_name = leaky, \"leaky\"\n",
    "model = create(act)\n",
    "opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "\n",
    "# Training - only for 125 eppochs\n",
    "batch_size = 128\n",
    "epochs=250\n",
    "# lr scheduler\n",
    "lr_1 = keras.callbacks.LearningRateScheduler(schedule)\n",
    "\n",
    "# Fit the model\n",
    "his = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,verbose=1,validation_data=(x_test,y_test), callbacks=[lr_1])\n",
    "# Save the model\n",
    "model.save('models/leaky_relu/leaky_at_2.h5')\n",
    "# Display logs\n",
    "print(his.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Epoch 1/250\n",
      "390/390 [==============================] - 156s 401ms/step - loss: 2.0517 - acc: 0.3842 - val_loss: 1.6512 - val_acc: 0.4332\n",
      "Epoch 2/250\n",
      "390/390 [==============================] - 156s 400ms/step - loss: 1.4624 - acc: 0.5312 - val_loss: 1.1377 - val_acc: 0.6261\n",
      "Epoch 3/250\n",
      "390/390 [==============================] - 155s 399ms/step - loss: 1.3093 - acc: 0.5879 - val_loss: 0.9860 - val_acc: 0.6932\n",
      "Epoch 4/250\n",
      "390/390 [==============================] - 155s 398ms/step - loss: 1.2330 - acc: 0.6169 - val_loss: 0.9871 - val_acc: 0.6955\n",
      "Epoch 5/250\n",
      "390/390 [==============================] - 155s 397ms/step - loss: 1.1651 - acc: 0.6410 - val_loss: 0.9290 - val_acc: 0.7196\n",
      "Epoch 6/250\n",
      "390/390 [==============================] - 155s 397ms/step - loss: 1.1454 - acc: 0.6497 - val_loss: 0.8617 - val_acc: 0.7352\n",
      "Epoch 7/250\n",
      "390/390 [==============================] - 155s 397ms/step - loss: 1.0913 - acc: 0.6610 - val_loss: 0.8567 - val_acc: 0.7433\n",
      "Epoch 8/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 1.0530 - acc: 0.6725 - val_loss: 0.8297 - val_acc: 0.7523\n",
      "Epoch 9/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 1.0310 - acc: 0.6832 - val_loss: 0.7971 - val_acc: 0.7496\n",
      "Epoch 10/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 1.0060 - acc: 0.6936 - val_loss: 0.7897 - val_acc: 0.7611\n",
      "Epoch 11/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.9812 - acc: 0.6996 - val_loss: 0.8121 - val_acc: 0.7618\n",
      "Epoch 12/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.9498 - acc: 0.7072 - val_loss: 0.7681 - val_acc: 0.7731\n",
      "Epoch 13/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.9333 - acc: 0.7147 - val_loss: 0.8011 - val_acc: 0.7685\n",
      "Epoch 14/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.9132 - acc: 0.7174 - val_loss: 0.7029 - val_acc: 0.7947\n",
      "Epoch 15/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.8877 - acc: 0.7245 - val_loss: 0.7311 - val_acc: 0.7845\n",
      "Epoch 16/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.8703 - acc: 0.7323 - val_loss: 0.6652 - val_acc: 0.7999\n",
      "Epoch 17/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.8560 - acc: 0.7352 - val_loss: 0.6734 - val_acc: 0.7975\n",
      "Epoch 18/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.8412 - acc: 0.7410 - val_loss: 0.6960 - val_acc: 0.8017\n",
      "Epoch 19/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.8316 - acc: 0.7447 - val_loss: 0.6563 - val_acc: 0.8047\n",
      "Epoch 20/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.8277 - acc: 0.7462 - val_loss: 0.6512 - val_acc: 0.8133\n",
      "Epoch 21/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.8183 - acc: 0.7492 - val_loss: 0.6650 - val_acc: 0.8048\n",
      "Epoch 22/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.8066 - acc: 0.7531 - val_loss: 0.6515 - val_acc: 0.8058\n",
      "Epoch 23/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.8017 - acc: 0.7544 - val_loss: 0.6207 - val_acc: 0.8173\n",
      "Epoch 24/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7923 - acc: 0.7571 - val_loss: 0.6090 - val_acc: 0.8254\n",
      "Epoch 25/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7888 - acc: 0.7609 - val_loss: 0.6334 - val_acc: 0.8154\n",
      "Epoch 26/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7807 - acc: 0.7629 - val_loss: 0.6163 - val_acc: 0.8206\n",
      "Epoch 27/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7748 - acc: 0.7630 - val_loss: 0.6374 - val_acc: 0.8124\n",
      "Epoch 28/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7741 - acc: 0.7645 - val_loss: 0.5995 - val_acc: 0.8241\n",
      "Epoch 29/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7608 - acc: 0.7704 - val_loss: 0.5946 - val_acc: 0.8315\n",
      "Epoch 30/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7615 - acc: 0.7706 - val_loss: 0.5897 - val_acc: 0.8328\n",
      "Epoch 31/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7569 - acc: 0.7728 - val_loss: 0.5773 - val_acc: 0.8354\n",
      "Epoch 32/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7451 - acc: 0.7754 - val_loss: 0.5677 - val_acc: 0.8408\n",
      "Epoch 33/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7442 - acc: 0.7758 - val_loss: 0.5823 - val_acc: 0.8345\n",
      "Epoch 34/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7432 - acc: 0.7764 - val_loss: 0.5637 - val_acc: 0.8405\n",
      "Epoch 35/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7434 - acc: 0.7775 - val_loss: 0.6304 - val_acc: 0.8189\n",
      "Epoch 36/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7339 - acc: 0.7809 - val_loss: 0.5708 - val_acc: 0.8409\n",
      "Epoch 37/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7331 - acc: 0.7807 - val_loss: 0.5853 - val_acc: 0.8323\n",
      "Epoch 38/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.7338 - acc: 0.7801 - val_loss: 0.5765 - val_acc: 0.8345\n",
      "Epoch 39/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7205 - acc: 0.7846 - val_loss: 0.5989 - val_acc: 0.8280\n",
      "Epoch 40/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7284 - acc: 0.7845 - val_loss: 0.5941 - val_acc: 0.8347\n",
      "Epoch 41/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7202 - acc: 0.7864 - val_loss: 0.5747 - val_acc: 0.8397\n",
      "Epoch 42/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7180 - acc: 0.7854 - val_loss: 0.5711 - val_acc: 0.8371\n",
      "Epoch 43/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.7176 - acc: 0.7874 - val_loss: 0.5665 - val_acc: 0.8426\n",
      "Epoch 44/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7143 - acc: 0.7887 - val_loss: 0.5742 - val_acc: 0.8408\n",
      "Epoch 45/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.7094 - acc: 0.7908 - val_loss: 0.5722 - val_acc: 0.8422\n",
      "Epoch 46/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7166 - acc: 0.7877 - val_loss: 0.6119 - val_acc: 0.8298\n",
      "Epoch 47/250\n",
      "390/390 [==============================] - 156s 399ms/step - loss: 0.7120 - acc: 0.7900 - val_loss: 0.5752 - val_acc: 0.8456\n",
      "Epoch 48/250\n",
      "390/390 [==============================] - 157s 401ms/step - loss: 0.7026 - acc: 0.7907 - val_loss: 0.5925 - val_acc: 0.8358\n",
      "Epoch 49/250\n",
      "390/390 [==============================] - 156s 401ms/step - loss: 0.7063 - acc: 0.7916 - val_loss: 0.5510 - val_acc: 0.8501\n",
      "Epoch 50/250\n",
      "390/390 [==============================] - 157s 402ms/step - loss: 0.7006 - acc: 0.7937 - val_loss: 0.5493 - val_acc: 0.8490\n",
      "Epoch 51/250\n",
      "390/390 [==============================] - 158s 406ms/step - loss: 0.6980 - acc: 0.7946 - val_loss: 0.5480 - val_acc: 0.8511\n",
      "Epoch 52/250\n",
      "390/390 [==============================] - 158s 405ms/step - loss: 0.6966 - acc: 0.7959 - val_loss: 0.5656 - val_acc: 0.8463\n",
      "Epoch 53/250\n",
      "390/390 [==============================] - 157s 404ms/step - loss: 0.6976 - acc: 0.7969 - val_loss: 0.5711 - val_acc: 0.8471\n",
      "Epoch 54/250\n",
      "390/390 [==============================] - 157s 402ms/step - loss: 0.7003 - acc: 0.7949 - val_loss: 0.5691 - val_acc: 0.8420\n",
      "Epoch 55/250\n",
      "390/390 [==============================] - 157s 403ms/step - loss: 0.6942 - acc: 0.7958 - val_loss: 0.5702 - val_acc: 0.8431\n",
      "Epoch 56/250\n",
      "390/390 [==============================] - 157s 402ms/step - loss: 0.6913 - acc: 0.7997 - val_loss: 0.5471 - val_acc: 0.8514\n",
      "Epoch 57/250\n",
      "344/390 [=========================>....] - ETA: 17s - loss: 0.6906 - acc: 0.7984"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m                 \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    607\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mworker\u001b[1;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget_index\u001b[1;34m(uid, i)\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0m_SHARED_SEQUENCES\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_SHARED_SEQUENCES\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    760\u001b[0m                                        self.batch_size * (idx + 1)]\n\u001b[1;32m--> 761\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    762\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[1;34m(self, index_array)\u001b[0m\n\u001b[0;32m    870\u001b[0m         batch_x = np.zeros(tuple([len(index_array)] + list(self.x.shape)[1:]),\n\u001b[1;32m--> 871\u001b[1;33m                            dtype=K.floatx())\n\u001b[0m\u001b[0;32m    872\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-80231796c488>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m his = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,\n\u001b[1;32m---> 15\u001b[1;33m                     epochs=epochs,verbose=1,validation_data=(x_test,y_test), callbacks=[lr_1])\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;31m# Save the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'models/leaky_relu/leaky_at_3.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1225\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1227\u001b[1;33m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1229\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2113\u001b[0m                 \u001b[0mbatch_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2114\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2115\u001b[1;33m                     \u001b[0mgenerator_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2117\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    555\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m             \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_send_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create model and get ready - reproducible cell\n",
    "act, act_name = leaky, \"leaky\"\n",
    "model = create(act)\n",
    "opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "\n",
    "# Training - only for 125 eppochs\n",
    "batch_size = 128\n",
    "epochs=250\n",
    "# lr scheduler\n",
    "lr_1 = keras.callbacks.LearningRateScheduler(schedule)\n",
    "\n",
    "# Fit the model\n",
    "his = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,verbose=1,validation_data=(x_test,y_test), callbacks=[lr_1])\n",
    "# Save the model\n",
    "model.save('models/leaky_relu/leaky_at_3.h5')\n",
    "# Display logs\n",
    "print(his.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_test):\n",
    "    y_hat = np.argmax(y_pred, axis=1)\n",
    "    y = np.argmax(y_test, axis=1)\n",
    "\n",
    "    good = np.sum(np.equal(y, y_hat))\n",
    "    return float(good/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras.regularizers as regularizers\n",
    "ensembler = 0\n",
    "for i in range(3):\n",
    "    if i == 0:\n",
    "        model = create(swish)\n",
    "        model.load_weights('models/leky_relu/leaky_at_'+str(i+1)+'.h5')\n",
    "        model.save('models/leky_relu/leaky_at_'+str(i+1)+'.h5')\n",
    "    else:\n",
    "        model = load_model('models/leky_relu/leaky_at_'+str(i+1)+'.h5')\n",
    "    ensembler += model.predict_proba(x_test)\n",
    "    \n",
    "print(accuracy(ensembler, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Swish activation function\n",
    "# x*sigmoid(x)\n",
    "def swish(x):\n",
    "    return x*K.sigmoid(x)\n",
    "\n",
    "# Custom activation function 1\n",
    "# mix between relu and positive part of swish mirrored across x=1\n",
    "def e_swish_1(x):\n",
    "    return K.maximum(0.0, x*(2-K.sigmoid(x)))\n",
    "\n",
    "# Custom activation function 2\n",
    "# positive part of swish mirrored across x=1\n",
    "def e_swish_2(x):\n",
    "    return K.maximum(x*K.sigmoid(x), x*(2-K.sigmoid(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model and get ready - reproducible cell\n",
    "act, act_name = e_swish_2, \"e_swish_2\"\n",
    "model = create(act)\n",
    "opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "\n",
    "# Training\n",
    "batch_size = 128\n",
    "epochs=250\n",
    "# lr scheduler\n",
    "lr_1 = keras.callbacks.LearningRateScheduler(schedule)\n",
    "\n",
    "# Fit the model\n",
    "his = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,verbose=1,validation_data=(x_test,y_test), callbacks=[lr_1])\n",
    "# Save the model\n",
    "model.save('models/e_swish/e_swish_2_at_4.h5')\n",
    "# Display logs\n",
    "print(his.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
