{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Performing a large-scale experiment on cifar10 \"\"\"\n",
    "\n",
    "import keras\n",
    "import logging\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import cifar10\n",
    "import keras.regularizers as regularizers\n",
    "from keras.models import load_model\n",
    "\n",
    "# Record settings\n",
    "LOG_FORMAT = \"%(levelname)s %(asctime)s - %(message)s\"\n",
    "logging.basicConfig(filename=\"swish_first_exp_log.txt\",format = LOG_FORMAT, level = logging.DEBUG, filemode = \"a\")\n",
    "logs = logging.getLogger()\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#z-score\n",
    "mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "std = np.std(x_train,axis=(0,1,2,3))\n",
    "x_train = (x_train-mean)/(std+1e-7)\n",
    "x_test = (x_test-mean)/(std+1e-7)\n",
    "\n",
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train,num_classes)\n",
    "y_test = np_utils.to_categorical(y_test,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Swish activation function\n",
    "# x*sigmoid(x)\n",
    "def swish(x):\n",
    "    return x*K.sigmoid(x)\n",
    "\n",
    "# Custom activation function 1\n",
    "# mix between relu and positive part of swish mirrored across x=1\n",
    "def e_swish_1(x):\n",
    "    return K.maximum(0.0, x*(2-K.sigmoid(x)))\n",
    "\n",
    "# Custom activation function 2\n",
    "# positive part of swish mirrored across x=1\n",
    "def e_swish_2(x):\n",
    "    return K.maximum(x*K.sigmoid(x), x*(2-K.sigmoid(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create(act):\n",
    "    baseMapNum = 32\n",
    "    weight_decay = 1e-4\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(2*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(2*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(4*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(4*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False\n",
    "    )\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def schedule(x):\n",
    "    if x < 75: \n",
    "        return 0.001\n",
    "    elif x < 100:\n",
    "        return 0.0005\n",
    "    elif x < 125:\n",
    "        return 0.0003\n",
    "    elif x<150:\n",
    "        return 0.00015\n",
    "    elif x<175:\n",
    "        return 0.000075\n",
    "    elif x<200: \n",
    "        return 0.000035\n",
    "    elif x<225:\n",
    "        return 0.000017\n",
    "    else:\n",
    "        return 0.000012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Epoch 1/250\n",
      "224/390 [================>.............] - ETA: 1:02 - loss: 2.3091 - acc: 0.3331"
     ]
    }
   ],
   "source": [
    "# Create model and get ready - reproducible cell\n",
    "act, act_name = \"elu\", \"elu\"\n",
    "model = create(act)\n",
    "opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "\n",
    "# Training - only for 125 eppochs\n",
    "batch_size = 128\n",
    "epochs=250\n",
    "# lr scheduler\n",
    "lr_1 = keras.callbacks.LearningRateScheduler(schedule)\n",
    "\n",
    "# Fit the model\n",
    "his = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,verbose=1,validation_data=(x_test,y_test), callbacks=[lr_1])\n",
    "# Save the model\n",
    "model.save('elu_at_2.h5')\n",
    "# Display logs\n",
    "print(his.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 2.0326 - acc: 0.3921 - val_loss: 1.4817 - val_acc: 0.4821\n",
      "Epoch 2/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 1.4682 - acc: 0.5383 - val_loss: 1.1022 - val_acc: 0.6379\n",
      "Epoch 3/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 1.2809 - acc: 0.6011 - val_loss: 1.0062 - val_acc: 0.6743\n",
      "Epoch 4/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 1.2080 - acc: 0.6357 - val_loss: 0.9366 - val_acc: 0.7097\n",
      "Epoch 5/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 1.1744 - acc: 0.6544 - val_loss: 0.8131 - val_acc: 0.7433\n",
      "Epoch 6/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 1.1130 - acc: 0.6762 - val_loss: 0.8670 - val_acc: 0.7193\n",
      "Epoch 7/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 1.0570 - acc: 0.6917 - val_loss: 0.8936 - val_acc: 0.7333\n",
      "Epoch 8/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 1.0653 - acc: 0.6989 - val_loss: 0.7705 - val_acc: 0.7695\n",
      "Epoch 9/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 1.0051 - acc: 0.7101 - val_loss: 0.8274 - val_acc: 0.7668\n",
      "Epoch 10/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.9963 - acc: 0.7193 - val_loss: 0.7916 - val_acc: 0.7715\n",
      "Epoch 11/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.9746 - acc: 0.7258 - val_loss: 0.6929 - val_acc: 0.7960\n",
      "Epoch 12/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.9489 - acc: 0.7309 - val_loss: 0.6966 - val_acc: 0.7995\n",
      "Epoch 13/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.9129 - acc: 0.7412 - val_loss: 0.7122 - val_acc: 0.7965\n",
      "Epoch 14/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.8949 - acc: 0.7477 - val_loss: 0.8933 - val_acc: 0.7692\n",
      "Epoch 15/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.8833 - acc: 0.7503 - val_loss: 0.6525 - val_acc: 0.8149\n",
      "Epoch 16/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.8926 - acc: 0.7532 - val_loss: 0.7042 - val_acc: 0.8006\n",
      "Epoch 17/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.8922 - acc: 0.7571 - val_loss: 0.6713 - val_acc: 0.8192\n",
      "Epoch 18/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.8520 - acc: 0.7604 - val_loss: 0.6556 - val_acc: 0.8152\n",
      "Epoch 19/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.8443 - acc: 0.7646 - val_loss: 0.6444 - val_acc: 0.8202\n",
      "Epoch 20/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.8402 - acc: 0.7689 - val_loss: 0.6360 - val_acc: 0.8229\n",
      "Epoch 21/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.8367 - acc: 0.7703 - val_loss: 0.6278 - val_acc: 0.8332\n",
      "Epoch 22/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.8195 - acc: 0.7762 - val_loss: 0.6078 - val_acc: 0.8331\n",
      "Epoch 23/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.8249 - acc: 0.7757 - val_loss: 0.6614 - val_acc: 0.8244\n",
      "Epoch 24/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.8130 - acc: 0.7782 - val_loss: 0.6456 - val_acc: 0.8283\n",
      "Epoch 25/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.8167 - acc: 0.7785 - val_loss: 0.6081 - val_acc: 0.8375\n",
      "Epoch 26/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.7999 - acc: 0.7823 - val_loss: 0.6737 - val_acc: 0.8130\n",
      "Epoch 27/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.8070 - acc: 0.7832 - val_loss: 0.6138 - val_acc: 0.8360\n",
      "Epoch 28/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.8108 - acc: 0.7843 - val_loss: 0.7208 - val_acc: 0.8022\n",
      "Epoch 29/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.7996 - acc: 0.7860 - val_loss: 0.6415 - val_acc: 0.8277\n",
      "Epoch 30/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.7843 - acc: 0.7877 - val_loss: 0.6660 - val_acc: 0.8384\n",
      "Epoch 31/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.7805 - acc: 0.7894 - val_loss: 0.6038 - val_acc: 0.8442\n",
      "Epoch 32/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.7730 - acc: 0.7919 - val_loss: 0.5988 - val_acc: 0.8400\n",
      "Epoch 33/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.7532 - acc: 0.7979 - val_loss: 0.6017 - val_acc: 0.8417\n",
      "Epoch 34/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.7705 - acc: 0.7946 - val_loss: 0.6732 - val_acc: 0.8328\n",
      "Epoch 35/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.7517 - acc: 0.7981 - val_loss: 0.6077 - val_acc: 0.8391\n",
      "Epoch 36/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.7458 - acc: 0.7984 - val_loss: 0.5767 - val_acc: 0.8505\n",
      "Epoch 37/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.7471 - acc: 0.7993 - val_loss: 0.6064 - val_acc: 0.8429\n",
      "Epoch 38/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.7408 - acc: 0.8023 - val_loss: 0.6010 - val_acc: 0.8422\n",
      "Epoch 39/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.7437 - acc: 0.8013 - val_loss: 0.5859 - val_acc: 0.8505\n",
      "Epoch 40/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.7425 - acc: 0.8042 - val_loss: 0.6082 - val_acc: 0.8435\n",
      "Epoch 41/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.7278 - acc: 0.8071 - val_loss: 0.5815 - val_acc: 0.8548\n",
      "Epoch 42/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.7393 - acc: 0.8070 - val_loss: 0.5913 - val_acc: 0.8477\n",
      "Epoch 43/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.7344 - acc: 0.8041 - val_loss: 0.5693 - val_acc: 0.8586\n",
      "Epoch 44/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.7297 - acc: 0.8091 - val_loss: 0.5748 - val_acc: 0.8528\n",
      "Epoch 45/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.7233 - acc: 0.8078 - val_loss: 0.5769 - val_acc: 0.8545\n",
      "Epoch 46/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.7283 - acc: 0.8070 - val_loss: 0.5843 - val_acc: 0.8478\n",
      "Epoch 47/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.7184 - acc: 0.8080 - val_loss: 0.5578 - val_acc: 0.8565\n",
      "Epoch 48/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.7086 - acc: 0.8104 - val_loss: 0.5498 - val_acc: 0.8616\n",
      "Epoch 49/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.7126 - acc: 0.8135 - val_loss: 0.6244 - val_acc: 0.8465\n",
      "Epoch 50/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.7045 - acc: 0.8147 - val_loss: 0.5479 - val_acc: 0.8629\n",
      "Epoch 51/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.7065 - acc: 0.8150 - val_loss: 0.5739 - val_acc: 0.8522\n",
      "Epoch 52/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.6940 - acc: 0.8170 - val_loss: 0.5590 - val_acc: 0.8596\n",
      "Epoch 53/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.7054 - acc: 0.8132 - val_loss: 0.6061 - val_acc: 0.8495\n",
      "Epoch 54/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.7009 - acc: 0.8155 - val_loss: 0.5657 - val_acc: 0.8578\n",
      "Epoch 55/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.6990 - acc: 0.8145 - val_loss: 0.5317 - val_acc: 0.8656\n",
      "Epoch 56/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.7062 - acc: 0.8147 - val_loss: 0.6053 - val_acc: 0.8487\n",
      "Epoch 57/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.6888 - acc: 0.8170 - val_loss: 0.6077 - val_acc: 0.8561\n",
      "Epoch 58/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.6931 - acc: 0.8184 - val_loss: 0.5667 - val_acc: 0.8618\n",
      "Epoch 59/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.6816 - acc: 0.8196 - val_loss: 0.5667 - val_acc: 0.8595\n",
      "Epoch 60/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.6856 - acc: 0.8206 - val_loss: 0.5573 - val_acc: 0.8632\n",
      "Epoch 61/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.6886 - acc: 0.8198 - val_loss: 0.5442 - val_acc: 0.8641\n",
      "Epoch 62/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.6821 - acc: 0.8206 - val_loss: 0.5512 - val_acc: 0.8625\n",
      "Epoch 63/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.6800 - acc: 0.8222 - val_loss: 0.5288 - val_acc: 0.8694\n",
      "Epoch 64/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.6731 - acc: 0.8224 - val_loss: 0.5377 - val_acc: 0.8684\n",
      "Epoch 65/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.6754 - acc: 0.8234 - val_loss: 0.5518 - val_acc: 0.8619\n",
      "Epoch 66/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.6725 - acc: 0.8210 - val_loss: 0.5544 - val_acc: 0.8630\n",
      "Epoch 67/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.6695 - acc: 0.8247 - val_loss: 0.5537 - val_acc: 0.8569\n",
      "Epoch 68/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.6611 - acc: 0.8249 - val_loss: 0.5212 - val_acc: 0.8701\n",
      "Epoch 69/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.6665 - acc: 0.8244 - val_loss: 0.5442 - val_acc: 0.8641\n",
      "Epoch 70/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.6557 - acc: 0.8272 - val_loss: 0.5558 - val_acc: 0.8626\n",
      "Epoch 71/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.6600 - acc: 0.8264 - val_loss: 0.5323 - val_acc: 0.8657\n",
      "Epoch 72/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.6543 - acc: 0.8292 - val_loss: 0.5594 - val_acc: 0.8599\n",
      "Epoch 73/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.6602 - acc: 0.8275 - val_loss: 0.5453 - val_acc: 0.8642\n",
      "Epoch 74/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.6552 - acc: 0.8290 - val_loss: 0.5397 - val_acc: 0.8664\n",
      "Epoch 75/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.6478 - acc: 0.8282 - val_loss: 0.5392 - val_acc: 0.8675\n",
      "Epoch 76/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.6064 - acc: 0.8431 - val_loss: 0.5021 - val_acc: 0.8821\n",
      "Epoch 77/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.5945 - acc: 0.8465 - val_loss: 0.4939 - val_acc: 0.8794\n",
      "Epoch 78/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.5821 - acc: 0.8478 - val_loss: 0.5027 - val_acc: 0.8825\n",
      "Epoch 79/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.5862 - acc: 0.8483 - val_loss: 0.4798 - val_acc: 0.8845\n",
      "Epoch 80/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.5698 - acc: 0.8505 - val_loss: 0.4997 - val_acc: 0.8808\n",
      "Epoch 81/200\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.5741 - acc: 0.8499 - val_loss: 0.4760 - val_acc: 0.8859\n",
      "Epoch 82/200\n",
      "390/390 [==============================] - 143s 367ms/step - loss: 0.5663 - acc: 0.8511 - val_loss: 0.4825 - val_acc: 0.8811\n",
      "Epoch 83/200\n",
      "390/390 [==============================] - 143s 368ms/step - loss: 0.5628 - acc: 0.8545 - val_loss: 0.4869 - val_acc: 0.8809\n",
      "Epoch 84/200\n",
      "390/390 [==============================] - 143s 367ms/step - loss: 0.5641 - acc: 0.8530 - val_loss: 0.4695 - val_acc: 0.8848\n",
      "Epoch 85/200\n",
      "390/390 [==============================] - 143s 366ms/step - loss: 0.5537 - acc: 0.8525 - val_loss: 0.4896 - val_acc: 0.8811\n",
      "Epoch 86/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.5576 - acc: 0.8526 - val_loss: 0.4719 - val_acc: 0.8862\n",
      "Epoch 87/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.5509 - acc: 0.8540 - val_loss: 0.4778 - val_acc: 0.8833\n",
      "Epoch 88/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.5570 - acc: 0.8514 - val_loss: 0.4550 - val_acc: 0.8900\n",
      "Epoch 89/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.5473 - acc: 0.8573 - val_loss: 0.4710 - val_acc: 0.8835\n",
      "Epoch 90/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.5494 - acc: 0.8554 - val_loss: 0.4685 - val_acc: 0.8880\n",
      "Epoch 91/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.5463 - acc: 0.8543 - val_loss: 0.4723 - val_acc: 0.8840\n",
      "Epoch 92/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 142s 364ms/step - loss: 0.5442 - acc: 0.8552 - val_loss: 0.4543 - val_acc: 0.8901\n",
      "Epoch 93/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.5357 - acc: 0.8575 - val_loss: 0.4981 - val_acc: 0.8768\n",
      "Epoch 94/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.5400 - acc: 0.8552 - val_loss: 0.4722 - val_acc: 0.8852\n",
      "Epoch 95/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.5358 - acc: 0.8576 - val_loss: 0.4596 - val_acc: 0.8872\n",
      "Epoch 96/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.5374 - acc: 0.8553 - val_loss: 0.4757 - val_acc: 0.8844\n",
      "Epoch 97/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.5378 - acc: 0.8568 - val_loss: 0.4681 - val_acc: 0.8855\n",
      "Epoch 98/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.5306 - acc: 0.8582 - val_loss: 0.4627 - val_acc: 0.8852\n",
      "Epoch 99/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.5301 - acc: 0.8599 - val_loss: 0.4680 - val_acc: 0.8808\n",
      "Epoch 100/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.5248 - acc: 0.8596 - val_loss: 0.4600 - val_acc: 0.8830\n",
      "Epoch 101/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.5085 - acc: 0.8656 - val_loss: 0.4352 - val_acc: 0.8921\n",
      "Epoch 102/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.5019 - acc: 0.8673 - val_loss: 0.4438 - val_acc: 0.8926\n",
      "Epoch 103/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4995 - acc: 0.8670 - val_loss: 0.4378 - val_acc: 0.8933\n",
      "Epoch 104/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4978 - acc: 0.8669 - val_loss: 0.4247 - val_acc: 0.8946\n",
      "Epoch 105/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.4911 - acc: 0.8687 - val_loss: 0.4311 - val_acc: 0.8956\n",
      "Epoch 106/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4940 - acc: 0.8673 - val_loss: 0.4279 - val_acc: 0.8960\n",
      "Epoch 107/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4898 - acc: 0.8695 - val_loss: 0.4480 - val_acc: 0.8925\n",
      "Epoch 108/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.4902 - acc: 0.8697 - val_loss: 0.4291 - val_acc: 0.8935\n",
      "Epoch 109/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4894 - acc: 0.8699 - val_loss: 0.4355 - val_acc: 0.8929\n",
      "Epoch 110/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4804 - acc: 0.8720 - val_loss: 0.4436 - val_acc: 0.8898\n",
      "Epoch 111/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4871 - acc: 0.8690 - val_loss: 0.4317 - val_acc: 0.8931\n",
      "Epoch 112/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.4856 - acc: 0.8698 - val_loss: 0.4276 - val_acc: 0.8955\n",
      "Epoch 113/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.4784 - acc: 0.8699 - val_loss: 0.4421 - val_acc: 0.8893\n",
      "Epoch 114/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4840 - acc: 0.8699 - val_loss: 0.4389 - val_acc: 0.8912\n",
      "Epoch 115/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.4792 - acc: 0.8709 - val_loss: 0.4253 - val_acc: 0.8942\n",
      "Epoch 116/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4818 - acc: 0.8688 - val_loss: 0.4287 - val_acc: 0.8932\n",
      "Epoch 117/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.4731 - acc: 0.8741 - val_loss: 0.4260 - val_acc: 0.8953\n",
      "Epoch 118/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4812 - acc: 0.8696 - val_loss: 0.4195 - val_acc: 0.8971\n",
      "Epoch 119/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.4746 - acc: 0.8734 - val_loss: 0.4200 - val_acc: 0.8967\n",
      "Epoch 120/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.4787 - acc: 0.8691 - val_loss: 0.4251 - val_acc: 0.8956\n",
      "Epoch 121/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4789 - acc: 0.8709 - val_loss: 0.4351 - val_acc: 0.8917\n",
      "Epoch 122/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4668 - acc: 0.8738 - val_loss: 0.4215 - val_acc: 0.8934\n",
      "Epoch 123/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.4745 - acc: 0.8721 - val_loss: 0.4282 - val_acc: 0.8941\n",
      "Epoch 124/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4697 - acc: 0.8724 - val_loss: 0.4236 - val_acc: 0.8956\n",
      "Epoch 125/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.4647 - acc: 0.8734 - val_loss: 0.4166 - val_acc: 0.8981\n",
      "Epoch 126/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.4470 - acc: 0.8802 - val_loss: 0.4197 - val_acc: 0.8976\n",
      "Epoch 127/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.4504 - acc: 0.8784 - val_loss: 0.4146 - val_acc: 0.8977\n",
      "Epoch 128/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.4421 - acc: 0.8825 - val_loss: 0.4006 - val_acc: 0.9012\n",
      "Epoch 129/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4430 - acc: 0.8809 - val_loss: 0.4088 - val_acc: 0.9012\n",
      "Epoch 130/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4426 - acc: 0.8796 - val_loss: 0.3972 - val_acc: 0.9036\n",
      "Epoch 131/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4401 - acc: 0.8820 - val_loss: 0.4090 - val_acc: 0.8994\n",
      "Epoch 132/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4415 - acc: 0.8814 - val_loss: 0.4023 - val_acc: 0.8999\n",
      "Epoch 133/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.4330 - acc: 0.8836 - val_loss: 0.4026 - val_acc: 0.9020\n",
      "Epoch 134/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.4357 - acc: 0.8839 - val_loss: 0.4062 - val_acc: 0.9011\n",
      "Epoch 135/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4370 - acc: 0.8833 - val_loss: 0.4024 - val_acc: 0.9026\n",
      "Epoch 136/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4332 - acc: 0.8837 - val_loss: 0.4024 - val_acc: 0.9018\n",
      "Epoch 137/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.4383 - acc: 0.8818 - val_loss: 0.3979 - val_acc: 0.9018\n",
      "Epoch 138/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4330 - acc: 0.8829 - val_loss: 0.3998 - val_acc: 0.9018\n",
      "Epoch 139/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4335 - acc: 0.8838 - val_loss: 0.4034 - val_acc: 0.9006\n",
      "Epoch 140/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.4290 - acc: 0.8851 - val_loss: 0.4015 - val_acc: 0.9014\n",
      "Epoch 141/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.4363 - acc: 0.8832 - val_loss: 0.3968 - val_acc: 0.9022\n",
      "Epoch 142/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4345 - acc: 0.8837 - val_loss: 0.3999 - val_acc: 0.8991\n",
      "Epoch 143/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.4284 - acc: 0.8850 - val_loss: 0.3960 - val_acc: 0.9024\n",
      "Epoch 144/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4292 - acc: 0.8843 - val_loss: 0.3952 - val_acc: 0.9020\n",
      "Epoch 145/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.4246 - acc: 0.8859 - val_loss: 0.3941 - val_acc: 0.9022\n",
      "Epoch 146/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.4324 - acc: 0.8841 - val_loss: 0.3938 - val_acc: 0.9021\n",
      "Epoch 147/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4269 - acc: 0.8861 - val_loss: 0.3979 - val_acc: 0.9015\n",
      "Epoch 148/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4257 - acc: 0.8854 - val_loss: 0.3911 - val_acc: 0.9040\n",
      "Epoch 149/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4287 - acc: 0.8847 - val_loss: 0.3994 - val_acc: 0.8991\n",
      "Epoch 150/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.4235 - acc: 0.8852 - val_loss: 0.3994 - val_acc: 0.9004\n",
      "Epoch 151/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4195 - acc: 0.8864 - val_loss: 0.3925 - val_acc: 0.9012\n",
      "Epoch 152/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4110 - acc: 0.8892 - val_loss: 0.3940 - val_acc: 0.9026\n",
      "Epoch 153/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4105 - acc: 0.8887 - val_loss: 0.3894 - val_acc: 0.9039\n",
      "Epoch 154/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4097 - acc: 0.8892 - val_loss: 0.3901 - val_acc: 0.9009\n",
      "Epoch 155/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.4117 - acc: 0.8895 - val_loss: 0.3905 - val_acc: 0.9030\n",
      "Epoch 156/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4097 - acc: 0.8905 - val_loss: 0.3883 - val_acc: 0.9034\n",
      "Epoch 157/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4152 - acc: 0.8880 - val_loss: 0.3860 - val_acc: 0.9038\n",
      "Epoch 158/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4096 - acc: 0.8902 - val_loss: 0.3841 - val_acc: 0.9033\n",
      "Epoch 159/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4085 - acc: 0.8896 - val_loss: 0.3871 - val_acc: 0.9028\n",
      "Epoch 160/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4083 - acc: 0.8894 - val_loss: 0.3865 - val_acc: 0.9028\n",
      "Epoch 161/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4077 - acc: 0.8897 - val_loss: 0.3902 - val_acc: 0.9034\n",
      "Epoch 162/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4021 - acc: 0.8926 - val_loss: 0.3883 - val_acc: 0.9036\n",
      "Epoch 163/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4060 - acc: 0.8904 - val_loss: 0.3884 - val_acc: 0.9056\n",
      "Epoch 164/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3993 - acc: 0.8924 - val_loss: 0.3864 - val_acc: 0.9050\n",
      "Epoch 165/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4065 - acc: 0.8905 - val_loss: 0.3847 - val_acc: 0.9048\n",
      "Epoch 166/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4058 - acc: 0.8902 - val_loss: 0.3851 - val_acc: 0.9038\n",
      "Epoch 167/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3974 - acc: 0.8915 - val_loss: 0.3848 - val_acc: 0.9042\n",
      "Epoch 168/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4063 - acc: 0.8911 - val_loss: 0.3877 - val_acc: 0.9023\n",
      "Epoch 169/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4016 - acc: 0.8925 - val_loss: 0.3808 - val_acc: 0.9050\n",
      "Epoch 170/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4053 - acc: 0.8902 - val_loss: 0.3829 - val_acc: 0.9053\n",
      "Epoch 171/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.3988 - acc: 0.8925 - val_loss: 0.3859 - val_acc: 0.9042\n",
      "Epoch 172/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4022 - acc: 0.8913 - val_loss: 0.3841 - val_acc: 0.9045\n",
      "Epoch 173/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.4016 - acc: 0.8918 - val_loss: 0.3855 - val_acc: 0.9035\n",
      "Epoch 174/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3982 - acc: 0.8930 - val_loss: 0.3838 - val_acc: 0.9043\n",
      "Epoch 175/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3952 - acc: 0.8930 - val_loss: 0.3787 - val_acc: 0.9067\n",
      "Epoch 176/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3981 - acc: 0.8924 - val_loss: 0.3826 - val_acc: 0.9041\n",
      "Epoch 177/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3961 - acc: 0.8943 - val_loss: 0.3800 - val_acc: 0.9064\n",
      "Epoch 178/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.3946 - acc: 0.8919 - val_loss: 0.3803 - val_acc: 0.9063\n",
      "Epoch 179/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3942 - acc: 0.8933 - val_loss: 0.3813 - val_acc: 0.9050\n",
      "Epoch 180/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.3902 - acc: 0.8968 - val_loss: 0.3807 - val_acc: 0.9065\n",
      "Epoch 181/200\n",
      "390/390 [==============================] - 143s 366ms/step - loss: 0.3958 - acc: 0.8948 - val_loss: 0.3791 - val_acc: 0.9063\n",
      "Epoch 182/200\n",
      "390/390 [==============================] - 143s 367ms/step - loss: 0.3918 - acc: 0.8952 - val_loss: 0.3788 - val_acc: 0.9065\n",
      "Epoch 183/200\n",
      "390/390 [==============================] - 143s 367ms/step - loss: 0.3884 - acc: 0.8958 - val_loss: 0.3794 - val_acc: 0.9065\n",
      "Epoch 184/200\n",
      "390/390 [==============================] - 143s 367ms/step - loss: 0.3927 - acc: 0.8942 - val_loss: 0.3800 - val_acc: 0.9054\n",
      "Epoch 185/200\n",
      "390/390 [==============================] - 143s 366ms/step - loss: 0.3904 - acc: 0.8956 - val_loss: 0.3766 - val_acc: 0.9073\n",
      "Epoch 186/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.3896 - acc: 0.8947 - val_loss: 0.3790 - val_acc: 0.9069\n",
      "Epoch 187/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3942 - acc: 0.8931 - val_loss: 0.3800 - val_acc: 0.9056\n",
      "Epoch 188/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3892 - acc: 0.8950 - val_loss: 0.3794 - val_acc: 0.9046\n",
      "Epoch 189/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3877 - acc: 0.8960 - val_loss: 0.3780 - val_acc: 0.9055\n",
      "Epoch 190/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3900 - acc: 0.8948 - val_loss: 0.3801 - val_acc: 0.9051\n",
      "Epoch 191/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3933 - acc: 0.8944 - val_loss: 0.3789 - val_acc: 0.9054\n",
      "Epoch 192/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3880 - acc: 0.8963 - val_loss: 0.3795 - val_acc: 0.9067\n",
      "Epoch 193/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3876 - acc: 0.8956 - val_loss: 0.3770 - val_acc: 0.9068\n",
      "Epoch 194/200\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.3868 - acc: 0.8958 - val_loss: 0.3809 - val_acc: 0.9061\n",
      "Epoch 195/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3873 - acc: 0.8954 - val_loss: 0.3792 - val_acc: 0.9072\n",
      "Epoch 196/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3908 - acc: 0.8949 - val_loss: 0.3801 - val_acc: 0.9061\n",
      "Epoch 197/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3882 - acc: 0.8946 - val_loss: 0.3801 - val_acc: 0.9055\n",
      "Epoch 198/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3909 - acc: 0.8946 - val_loss: 0.3779 - val_acc: 0.9064\n",
      "Epoch 199/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3852 - acc: 0.8948 - val_loss: 0.3775 - val_acc: 0.9056\n",
      "Epoch 200/200\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3839 - acc: 0.8976 - val_loss: 0.3770 - val_acc: 0.9053\n",
      "{'val_loss': [1.4817496755599975, 1.1021572984695434, 1.0062085130691529, 0.93657440032958983, 0.81305288915634155, 0.86697609300613399, 0.89362037277221684, 0.77046688032150268, 0.82741076974868777, 0.79155981245040896, 0.69289707584381099, 0.69661957297325139, 0.71221940040588383, 0.89334519205093388, 0.65252266988754271, 0.70419740953445431, 0.67125757265090946, 0.65561524000167848, 0.64435851364135743, 0.63597799119949339, 0.6277854172706604, 0.60776949024200444, 0.66140735301971432, 0.64564204978942874, 0.60809580316543577, 0.67365423765182497, 0.61378504505157472, 0.72083532714843745, 0.64150480251312259, 0.66599054956436154, 0.60379085550308231, 0.59877368283271792, 0.6017335824966431, 0.67324509949684141, 0.60766169342994691, 0.57673200359344479, 0.60642619848251345, 0.60097643842697146, 0.58594126996994023, 0.60818471684455877, 0.58148927526474004, 0.5913311109542847, 0.56925513420104978, 0.57477811393737788, 0.5768910543441772, 0.58425515174865728, 0.55783442945480344, 0.54984763021469119, 0.62444199018478397, 0.54787178106307988, 0.57386580405235288, 0.55904182682037351, 0.60610174884796142, 0.56567408037185674, 0.53166394457817079, 0.60530405349731442, 0.60769472589492801, 0.56673433475494384, 0.56669893016815187, 0.55725846586227412, 0.54417030954360956, 0.55119498319625859, 0.52882104754447934, 0.53771662664413455, 0.55182515392303466, 0.55436164379119868, 0.55365960235595701, 0.52119223604202269, 0.54421504535675047, 0.5557836584091187, 0.53234767560958862, 0.55941797742843624, 0.54532849459648136, 0.53974266681671146, 0.53924129257202147, 0.50211819171905514, 0.49388085575103757, 0.50268162670135497, 0.47982563834190367, 0.49967078080177307, 0.47597259163856509, 0.48254834280014036, 0.48687252273559573, 0.4694694658756256, 0.48963736314773559, 0.47192366690635679, 0.47784577651023863, 0.45498976635932925, 0.47104992651939392, 0.46851047396659851, 0.47230952887535094, 0.4542760464668274, 0.49812923660278319, 0.47221482796669006, 0.45958175144195557, 0.47574987735748292, 0.46811179075241088, 0.46272475061416624, 0.46799402923583983, 0.45995610923767088, 0.43517367820739744, 0.44381564655303957, 0.43780444822311404, 0.42471569395065306, 0.43111538109779357, 0.4278722885131836, 0.44803816041946409, 0.42910966453552246, 0.43554609866142274, 0.44360835876464844, 0.43170892910957337, 0.42756438136100772, 0.44205037279129028, 0.43892042961120603, 0.42526504602432252, 0.42872409148216245, 0.42600816679000852, 0.4195118565559387, 0.42001764616966247, 0.42506560211181643, 0.43512429280281067, 0.4214794222354889, 0.42824076051712034, 0.42362025990486146, 0.41659707417488095, 0.41967057468891145, 0.41464097566604613, 0.40061704442501067, 0.40880785911083223, 0.397179536151886, 0.40902054753303529, 0.40232547793388368, 0.40258280498981475, 0.40621726393699648, 0.4023914182424545, 0.40239552459716799, 0.39787306447029114, 0.39979557123184206, 0.40343179976940158, 0.40152409977912901, 0.39684710388183592, 0.3999343960762024, 0.39603679409027098, 0.39521725730895996, 0.39411117236614229, 0.39380971832275391, 0.39786310186386109, 0.39113569250106811, 0.3994348365306854, 0.39941136746406553, 0.39248738012313844, 0.39404887807369232, 0.38942801561355589, 0.39008951110839846, 0.39045503139495852, 0.38826084995269777, 0.38595053386688233, 0.38407485733032226, 0.38706656997203825, 0.38648124809265139, 0.39024781947135923, 0.38833741927146914, 0.38841075022220611, 0.38635498285293579, 0.38472123012542725, 0.38509224021434785, 0.3847550509929657, 0.38765903272628782, 0.3807973129749298, 0.38288919439315794, 0.38592759981155395, 0.38414815514087675, 0.38549016783237455, 0.38375247445106508, 0.37872273259162903, 0.38255664339065554, 0.38000040328502654, 0.38027688112258912, 0.38131371598243713, 0.38065308430194855, 0.37905781462192534, 0.37883939683437345, 0.37941372690200803, 0.37996632852554324, 0.37660489187240603, 0.37900128149986267, 0.37996638019084933, 0.37937190103530882, 0.37803702409267426, 0.38006933627128603, 0.37893535647392274, 0.37953843231201173, 0.37699624609947202, 0.3808946788787842, 0.37924276509284971, 0.38006131451129915, 0.38005189509391785, 0.37794993853569031, 0.3775156665325165, 0.37704875369071961], 'val_acc': [0.48209999999999997, 0.63790000000000002, 0.67430000000000001, 0.7097, 0.74329999999999996, 0.71930000000000005, 0.73329999999999995, 0.76949999999999996, 0.76680000000000004, 0.77149999999999996, 0.79600000000000004, 0.79949999999999999, 0.79649999999999999, 0.76919999999999999, 0.81489999999999996, 0.80059999999999998, 0.81920000000000004, 0.81520000000000004, 0.82020000000000004, 0.82289999999999996, 0.83320000000000005, 0.83309999999999995, 0.82440000000000002, 0.82830000000000004, 0.83750000000000002, 0.81299999999999994, 0.83599999999999997, 0.80220000000000002, 0.82769999999999999, 0.83840000000000003, 0.84419999999999995, 0.83999999999999997, 0.8417, 0.83279999999999998, 0.83909999999999996, 0.85050000000000003, 0.84289999999999998, 0.84219999999999995, 0.85050000000000003, 0.84350000000000003, 0.8548, 0.84770000000000001, 0.85860000000000003, 0.8528, 0.85450000000000004, 0.8478, 0.85650000000000004, 0.86160000000000003, 0.84650000000000003, 0.8629, 0.85219999999999996, 0.85960000000000003, 0.84950000000000003, 0.85780000000000001, 0.86560000000000004, 0.84870000000000001, 0.85609999999999997, 0.86180000000000001, 0.85950000000000004, 0.86319999999999997, 0.86409999999999998, 0.86250000000000004, 0.86939999999999995, 0.86839999999999995, 0.8619, 0.86299999999999999, 0.8569, 0.87009999999999998, 0.86409999999999998, 0.86260000000000003, 0.86570000000000003, 0.8599, 0.86419999999999997, 0.86639999999999995, 0.86750000000000005, 0.8821, 0.87939999999999996, 0.88249999999999995, 0.88449999999999995, 0.88080000000000003, 0.88590000000000002, 0.88109999999999999, 0.88090000000000002, 0.88480000000000003, 0.88109999999999999, 0.88619999999999999, 0.88329999999999997, 0.89000000000000001, 0.88349999999999995, 0.88800000000000001, 0.88400000000000001, 0.8901, 0.87680000000000002, 0.88519999999999999, 0.88719999999999999, 0.88439999999999996, 0.88549999999999995, 0.88519999999999999, 0.88080000000000003, 0.88300000000000001, 0.8921, 0.89259999999999995, 0.89329999999999998, 0.89459999999999995, 0.89559999999999995, 0.89600000000000002, 0.89249999999999996, 0.89349999999999996, 0.89290000000000003, 0.88980000000000004, 0.8931, 0.89550000095367432, 0.88929999999999998, 0.89119999999999999, 0.89419999999999999, 0.89319999999999999, 0.89529999999999998, 0.89710000000000001, 0.89670000000000005, 0.89559999999999995, 0.89170000000000005, 0.89339999999999997, 0.89410000000000001, 0.89559999999999995, 0.89810000000000001, 0.89759999999999995, 0.89770000000000005, 0.9012, 0.9012, 0.90359999999999996, 0.89939999999999998, 0.89990000000000003, 0.90200000000000002, 0.90110000000000001, 0.90259999999999996, 0.90180000000000005, 0.90180000000000005, 0.90180000000000005, 0.90059999999999996, 0.90139999999999998, 0.9022, 0.89910000000000001, 0.90239999999999998, 0.90200000000000002, 0.9022, 0.90210000000000001, 0.90149999999999997, 0.90400000000000003, 0.89910000000000001, 0.90039999999999998, 0.9012, 0.90259999999999996, 0.90390000000000004, 0.90090000000000003, 0.90300000000000002, 0.90339999999999998, 0.90380000000000005, 0.90329999999999999, 0.90280000000000005, 0.90280000000000005, 0.90339999999999998, 0.90359999999999996, 0.90559999999999996, 0.90500000000000003, 0.90480000000000005, 0.90380000000000005, 0.9042, 0.90229999999999999, 0.90500000000000003, 0.90529999999999999, 0.9042, 0.90449999999999997, 0.90349999999999997, 0.90429999999999999, 0.90669999999999995, 0.90410000000000001, 0.90639999999999998, 0.90629999999999999, 0.90500000000000003, 0.90649999999999997, 0.90629999999999999, 0.90649999999999997, 0.90649999999999997, 0.90539999999999998, 0.9073, 0.90690000000000004, 0.90559999999999996, 0.90459999999999996, 0.90549999999999997, 0.90510000000000002, 0.90539999999999998, 0.90669999999999995, 0.90680000000000005, 0.90610000000000002, 0.90720000000000001, 0.90610000000000002, 0.90549999999999997, 0.90639999999999998, 0.90559999999999996, 0.90529999999999999], 'acc': [0.39208373434083904, 0.53839829964709651, 0.60105871033660874, 0.63558710302188304, 0.65447545718947853, 0.67617099771600597, 0.69177093362823527, 0.69880895093346318, 0.71013795316008987, 0.71925080128205132, 0.72577472700089618, 0.73089108114866708, 0.741297722168752, 0.74767404555662498, 0.75030077000937101, 0.75316811040077292, 0.75707811996804475, 0.76040664098184008, 0.76463747190901354, 0.76896855949322918, 0.77027189603477553, 0.77622195512820513, 0.77571050102081374, 0.77825232599268823, 0.77853304463240591, 0.78226259225524697, 0.78318495351915596, 0.78430782803978183, 0.78603224257914961, 0.78773660569149673, 0.78946102021174203, 0.7919874879309563, 0.79790263075380319, 0.79455405838947701, 0.79810314401051996, 0.79840391398164601, 0.79936637790837195, 0.80229387229399918, 0.80129130574270135, 0.80427895412255379, 0.80707131410256405, 0.80708092483635052, 0.80401828681424448, 0.80905448717948714, 0.80780346820809246, 0.80700593515585206, 0.80794834780853686, 0.81033445624613709, 0.81350256658954267, 0.81468559512351624, 0.81498636509464228, 0.81711180622393331, 0.8132211538461539, 0.81545038533731384, 0.81456528717331067, 0.81478585176143581, 0.81707170359936132, 0.81841514278460203, 0.81963141025641029, 0.82060854208722045, 0.81981169871794868, 0.82060854199151623, 0.82221554487179482, 0.82236525505293556, 0.82337829154810238, 0.82100176451716389, 0.8247112607894741, 0.82487167145986373, 0.82439043952518443, 0.82716346153846154, 0.8264089594992905, 0.82916265636214015, 0.82757860118087612, 0.82894209171664768, 0.82816506410256407, 0.84300738603699721, 0.84654716069322766, 0.84787054858530486, 0.84829162658312629, 0.85047722170664253, 0.84985563045210433, 0.85113891564311983, 0.85448717948717945, 0.85290221579961467, 0.85254250884170824, 0.85258261144715775, 0.85410651263419657, 0.85133942893808146, 0.85731472567866385, 0.85540865384615383, 0.85430702594828067, 0.85521034043648336, 0.85745508503676759, 0.85524943860096547, 0.85759544435662649, 0.85534855769230766, 0.85675328841835097, 0.85819698429887858, 0.85984666024405909, 0.85954042350324178, 0.86563602823227459, 0.86728023740776383, 0.86699951878716863, 0.86693936473557609, 0.86868990384615385, 0.867340391440234, 0.86940028903648192, 0.86976660254065108, 0.86984680778979495, 0.87193214625626903, 0.86904475459082597, 0.8697465511327529, 0.86998716712877622, 0.86996711577824537, 0.87084937437933763, 0.86876403595110829, 0.87413862179487178, 0.86956085422594886, 0.87343749999999998, 0.86906480592223445, 0.87086544633294505, 0.87369666343933117, 0.87207532051282055, 0.87231053315299245, 0.87335579080538828, 0.88017324346512371, 0.87838867498889805, 0.88245909528392685, 0.88083493740160113, 0.87961180622393331, 0.88203801730522791, 0.88143647738209818, 0.88356191849226673, 0.88396294515867968, 0.8832611485020182, 0.88370227786949285, 0.88183750402938876, 0.88286012189297247, 0.8837824831377592, 0.88509615384615381, 0.88328119990991638, 0.88365044958253047, 0.88495592948717949, 0.88429271030211642, 0.8858878729738866, 0.88409455128205128, 0.88601878612716767, 0.88542669229412596, 0.88462463911427958, 0.88512592239948962, 0.88637820512820509, 0.88916987795131519, 0.88868189102564099, 0.88931037247295786, 0.88954326923076921, 0.89057482337829164, 0.88798076923076918, 0.89019348107244856, 0.88959736926531918, 0.88943695857580707, 0.88969762596060609, 0.89254491494410992, 0.89033926852743017, 0.89240455570074084, 0.89039942250253301, 0.89021896053897975, 0.89152229711877107, 0.89108573717948714, 0.89242460699390436, 0.89025369296107604, 0.89248798076923075, 0.89136188642925895, 0.89184311842130548, 0.89300609564299305, 0.8929860442924622, 0.89250160569017645, 0.89432948343945806, 0.89192708333333337, 0.89324421967232004, 0.8968349358974359, 0.89468930637752242, 0.89525184474161201, 0.8957732371794872, 0.89422922685890582, 0.89559248556827376, 0.8947115384615385, 0.89306624957985092, 0.89489092071889342, 0.89597369263406978, 0.89486994221567262, 0.89441105769230766, 0.89629451397484905, 0.89553227360308285, 0.89579326923076918, 0.89541225537375679, 0.89480973021220001, 0.89463025344882896, 0.89461137820512826, 0.89485081809432143, 0.89761790186692625], 'loss': [2.0323343081163756, 1.4680047171369057, 1.2810643562185944, 1.2081885564239276, 1.1741546825372833, 1.1129974321864682, 1.0571497539927801, 1.0653695977571724, 1.0050443218884118, 0.99626552982208061, 0.97475715878305336, 0.9490810845729668, 0.91282666597681472, 0.89473053748920484, 0.8832037594787272, 0.89250248481265848, 0.89229718383245071, 0.85198830669034087, 0.84421289978709635, 0.84005505150619664, 0.83659677661149823, 0.81951346015318849, 0.82477954083631344, 0.81298824377001799, 0.81660855539264987, 0.80001454988962417, 0.80690907636513343, 0.81094424293176615, 0.79947850996383996, 0.78423786457815647, 0.78029820476926059, 0.77256019906352102, 0.75319453315701212, 0.77061647008465084, 0.75154819942109663, 0.74561577423436542, 0.74699194644711364, 0.74088219832180779, 0.74356960529389193, 0.74218362835610874, 0.72775552563178236, 0.73911858328505053, 0.7346018541306687, 0.72971927088040567, 0.72298919205184775, 0.72825331935754067, 0.71843742714273962, 0.70871644453012606, 0.71256585196879652, 0.70458264361599676, 0.70644662668584901, 0.69369572236593768, 0.7053952466218899, 0.70080751932884688, 0.69898119439748296, 0.70607590134188958, 0.68842744460119665, 0.69311360489453955, 0.6816098069533324, 0.68555019147976437, 0.68859109465892498, 0.68233503342899815, 0.6800039383845452, 0.67329090760042543, 0.67543213577628824, 0.67233895261126297, 0.66959970009644365, 0.66104264539673341, 0.66664304209703962, 0.65569109649230273, 0.66000395452432625, 0.65438914228258205, 0.66003518011618167, 0.65531675934906297, 0.64784232530838404, 0.60663613984235676, 0.59456133555631663, 0.58199731797639631, 0.58613084656786074, 0.56977534546544628, 0.57413061851869229, 0.56625569524541663, 0.56278088910457413, 0.56428850457005264, 0.55354384076415553, 0.55757380185847794, 0.55060887256416713, 0.55706227905175831, 0.54733666122667213, 0.54936043062271211, 0.54624029631640691, 0.54431323749213623, 0.53560412924451095, 0.54011868647478078, 0.53584328459209463, 0.53742572825688584, 0.53781803705235642, 0.53056115619563349, 0.53006120586938865, 0.52493203737087879, 0.50858213049997536, 0.5018913566894091, 0.4994951414687151, 0.49776569193358755, 0.49111293768271419, 0.49396926628346238, 0.48977951194394759, 0.49016831193285415, 0.48955884376970438, 0.4804722032257866, 0.48711261945846257, 0.48566715550759221, 0.47834283455771287, 0.48382287395279644, 0.4792837730519543, 0.48189082336341643, 0.47305311896862129, 0.48129720703561718, 0.47461752700499998, 0.47863220040294119, 0.47881670148807531, 0.46700164873629835, 0.47454671492943395, 0.46987089042910107, 0.46471521973533436, 0.44705599218482656, 0.4503957547646591, 0.44206448153191974, 0.44295593003336342, 0.44260313040865207, 0.44014322097996778, 0.44139443007577178, 0.43302044733273587, 0.43566285342905514, 0.43706662774583183, 0.43323128845663378, 0.43805381346865652, 0.43290512355084521, 0.43345491622058024, 0.42902669295286522, 0.43627273158687485, 0.43457051049307821, 0.4284280362801674, 0.42906534213359798, 0.42465361980509214, 0.43238625480578496, 0.4269929829322151, 0.42574685069089674, 0.42877475892002093, 0.42360002167683058, 0.41950963464302893, 0.41112939811472565, 0.41048667801496308, 0.40951016351973007, 0.41170818289885153, 0.40912213240132245, 0.41519828576308032, 0.40953347259174194, 0.40845795197099838, 0.40811921401325235, 0.40763411366637409, 0.40207440326486904, 0.40605073389924412, 0.39931707727170962, 0.40654216537041127, 0.40560802885229325, 0.39722923342142114, 0.40630807578563688, 0.40168734231857983, 0.40522474853481016, 0.39883513286327704, 0.40221846990949417, 0.40152863685351209, 0.39822108840422255, 0.39528482898061079, 0.39784643544014553, 0.39598192749285033, 0.39464093320644816, 0.39428790214165865, 0.39019731432199478, 0.39601451549954192, 0.39174370035973732, 0.38843424878059291, 0.39253979391649363, 0.39033809375058554, 0.38957040963264611, 0.3941452168848486, 0.38929991080357856, 0.38770735796384409, 0.38983274394300554, 0.39331265302040636, 0.38806467128748462, 0.38754030460146044, 0.38675120346821273, 0.3872793245418813, 0.39091524167384717, 0.38823053551515707, 0.39090670820994255, 0.38519010988993641, 0.38385508376549254]}\n"
     ]
    }
   ],
   "source": [
    "# Create model and get ready - reproducible cell\n",
    "act, act_name = \"elu\", \"elu\"\n",
    "model = create(act)\n",
    "opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "\n",
    "# Training - only for 125 eppochs\n",
    "batch_size = 128\n",
    "epochs=200\n",
    "# lr scheduler\n",
    "lr_1 = keras.callbacks.LearningRateScheduler(schedule)\n",
    "\n",
    "# Fit the model\n",
    "his = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,verbose=1,validation_data=(x_test,y_test), callbacks=[lr_1])\n",
    "# Save the model\n",
    "model.save('elu_at_3.h5')\n",
    "# Display logs\n",
    "print(his.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 201/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3826 - acc: 0.8967 - val_loss: 0.3779 - val_acc: 0.9063\n",
      "Epoch 202/250\n",
      "390/390 [==============================] - 143s 366ms/step - loss: 0.3823 - acc: 0.8968 - val_loss: 0.3773 - val_acc: 0.9072\n",
      "Epoch 203/250\n",
      "390/390 [==============================] - 143s 367ms/step - loss: 0.3856 - acc: 0.8974 - val_loss: 0.3766 - val_acc: 0.9069\n",
      "Epoch 204/250\n",
      "390/390 [==============================] - 143s 367ms/step - loss: 0.3803 - acc: 0.8979 - val_loss: 0.3789 - val_acc: 0.9064\n",
      "Epoch 205/250\n",
      "390/390 [==============================] - 143s 368ms/step - loss: 0.3843 - acc: 0.8954 - val_loss: 0.3786 - val_acc: 0.9070\n",
      "Epoch 206/250\n",
      "390/390 [==============================] - 143s 367ms/step - loss: 0.3842 - acc: 0.8968 - val_loss: 0.3771 - val_acc: 0.9068\n",
      "Epoch 207/250\n",
      "390/390 [==============================] - 143s 367ms/step - loss: 0.3845 - acc: 0.8976 - val_loss: 0.3755 - val_acc: 0.9065\n",
      "Epoch 208/250\n",
      "390/390 [==============================] - 143s 368ms/step - loss: 0.3851 - acc: 0.8966 - val_loss: 0.3754 - val_acc: 0.9078\n",
      "Epoch 209/250\n",
      "390/390 [==============================] - 143s 366ms/step - loss: 0.3873 - acc: 0.8942 - val_loss: 0.3758 - val_acc: 0.9072\n",
      "Epoch 210/250\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3837 - acc: 0.8959 - val_loss: 0.3763 - val_acc: 0.9069\n",
      "Epoch 211/250\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3820 - acc: 0.8957 - val_loss: 0.3774 - val_acc: 0.9067\n",
      "Epoch 212/250\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3842 - acc: 0.8968 - val_loss: 0.3767 - val_acc: 0.9061\n",
      "Epoch 213/250\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3859 - acc: 0.8962 - val_loss: 0.3766 - val_acc: 0.9065\n",
      "Epoch 214/250\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3843 - acc: 0.8959 - val_loss: 0.3776 - val_acc: 0.9067\n",
      "Epoch 215/250\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3846 - acc: 0.8970 - val_loss: 0.3769 - val_acc: 0.9072\n",
      "Epoch 216/250\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3800 - acc: 0.8991 - val_loss: 0.3770 - val_acc: 0.9064\n",
      "Epoch 217/250\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3789 - acc: 0.8969 - val_loss: 0.3756 - val_acc: 0.9069\n",
      "Epoch 218/250\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3825 - acc: 0.8966 - val_loss: 0.3777 - val_acc: 0.9066\n",
      "Epoch 219/250\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3831 - acc: 0.8972 - val_loss: 0.3775 - val_acc: 0.9072\n",
      "Epoch 220/250\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3818 - acc: 0.8947 - val_loss: 0.3779 - val_acc: 0.9064\n",
      "Epoch 221/250\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3799 - acc: 0.8964 - val_loss: 0.3785 - val_acc: 0.9076\n",
      "Epoch 222/250\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3795 - acc: 0.8969 - val_loss: 0.3765 - val_acc: 0.9073\n",
      "Epoch 223/250\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.3875 - acc: 0.8962 - val_loss: 0.3764 - val_acc: 0.9063\n",
      "Epoch 224/250\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3825 - acc: 0.8974 - val_loss: 0.3755 - val_acc: 0.9063\n",
      "Epoch 225/250\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3804 - acc: 0.8966 - val_loss: 0.3768 - val_acc: 0.9060\n",
      "Epoch 226/250\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3770 - acc: 0.8976 - val_loss: 0.3766 - val_acc: 0.9070\n",
      "Epoch 227/250\n",
      "390/390 [==============================] - 142s 365ms/step - loss: 0.3802 - acc: 0.8958 - val_loss: 0.3753 - val_acc: 0.9071\n",
      "Epoch 228/250\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3870 - acc: 0.8952 - val_loss: 0.3754 - val_acc: 0.9071\n",
      "Epoch 229/250\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3774 - acc: 0.8971 - val_loss: 0.3733 - val_acc: 0.9075\n",
      "Epoch 230/250\n",
      "390/390 [==============================] - 142s 364ms/step - loss: 0.3805 - acc: 0.8970 - val_loss: 0.3756 - val_acc: 0.9073\n",
      "Epoch 231/250\n",
      "390/390 [==============================] - 143s 366ms/step - loss: 0.3788 - acc: 0.8975 - val_loss: 0.3744 - val_acc: 0.9075\n",
      "Epoch 232/250\n",
      "390/390 [==============================] - 148s 380ms/step - loss: 0.3777 - acc: 0.8977 - val_loss: 0.3760 - val_acc: 0.9066\n",
      "Epoch 233/250\n",
      "390/390 [==============================] - 148s 381ms/step - loss: 0.3792 - acc: 0.8988 - val_loss: 0.3735 - val_acc: 0.9073\n",
      "Epoch 234/250\n",
      "390/390 [==============================] - 152s 391ms/step - loss: 0.3801 - acc: 0.8968 - val_loss: 0.3741 - val_acc: 0.9080\n",
      "Epoch 235/250\n",
      "390/390 [==============================] - 150s 384ms/step - loss: 0.3789 - acc: 0.8979 - val_loss: 0.3754 - val_acc: 0.9071\n",
      "Epoch 236/250\n",
      "390/390 [==============================] - 146s 375ms/step - loss: 0.3765 - acc: 0.8977 - val_loss: 0.3757 - val_acc: 0.9076\n",
      "Epoch 237/250\n",
      "390/390 [==============================] - 151s 387ms/step - loss: 0.3844 - acc: 0.8950 - val_loss: 0.3754 - val_acc: 0.9070\n",
      "Epoch 238/250\n",
      "390/390 [==============================] - 147s 376ms/step - loss: 0.3815 - acc: 0.8966 - val_loss: 0.3750 - val_acc: 0.9068\n",
      "Epoch 239/250\n",
      "390/390 [==============================] - 145s 371ms/step - loss: 0.3807 - acc: 0.8982 - val_loss: 0.3771 - val_acc: 0.9076\n",
      "Epoch 240/250\n",
      "390/390 [==============================] - 147s 376ms/step - loss: 0.3778 - acc: 0.9002 - val_loss: 0.3751 - val_acc: 0.9069\n",
      "Epoch 241/250\n",
      "390/390 [==============================] - 145s 372ms/step - loss: 0.3801 - acc: 0.8978 - val_loss: 0.3748 - val_acc: 0.9075\n",
      "Epoch 242/250\n",
      "390/390 [==============================] - 143s 367ms/step - loss: 0.3804 - acc: 0.8981 - val_loss: 0.3731 - val_acc: 0.9084\n",
      "Epoch 243/250\n",
      "390/390 [==============================] - 144s 368ms/step - loss: 0.3800 - acc: 0.8987 - val_loss: 0.3758 - val_acc: 0.9079\n",
      "Epoch 244/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3797 - acc: 0.8984 - val_loss: 0.3763 - val_acc: 0.9081\n",
      "Epoch 245/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3752 - acc: 0.8986 - val_loss: 0.3758 - val_acc: 0.9079\n",
      "Epoch 246/250\n",
      "390/390 [==============================] - 143s 368ms/step - loss: 0.3762 - acc: 0.8980 - val_loss: 0.3746 - val_acc: 0.9073\n",
      "Epoch 247/250\n",
      "390/390 [==============================] - 145s 371ms/step - loss: 0.3797 - acc: 0.8976 - val_loss: 0.3741 - val_acc: 0.9079\n",
      "Epoch 248/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.3765 - acc: 0.8988 - val_loss: 0.3761 - val_acc: 0.9071\n",
      "Epoch 249/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.3791 - acc: 0.8964 - val_loss: 0.3758 - val_acc: 0.9065\n",
      "Epoch 250/250\n",
      "390/390 [==============================] - 145s 373ms/step - loss: 0.3796 - acc: 0.8989 - val_loss: 0.3770 - val_acc: 0.9069\n",
      "{'val_loss': [0.37785097351074221, 0.37731364712715149, 0.37661438734531405, 0.37886159641742706, 0.37856462571620941, 0.37713949706554412, 0.37550454359054564, 0.37540296201705931, 0.37580586643218994, 0.37626878676414488, 0.3773591064929962, 0.37666828496456145, 0.37660351555347443, 0.37759391674995424, 0.37693574063777924, 0.37701914649009705, 0.37559629876613615, 0.37769697990417478, 0.37752313163280488, 0.37790378603935243, 0.3785025860309601, 0.37651138377189636, 0.37637851953506468, 0.3754533318758011, 0.37683316841125486, 0.37656428048610685, 0.37534701485633848, 0.37539958257675171, 0.37330716643333434, 0.375648184299469, 0.37437661690711976, 0.37600070507526395, 0.37349254970550538, 0.37409095671176912, 0.37544303872585294, 0.37571356678009032, 0.37537829804420469, 0.37495623900890351, 0.37714893469810484, 0.3750675146341324, 0.37481870002746581, 0.37313218743801119, 0.37582568845748904, 0.37627374584674833, 0.37577459394931795, 0.37462998654842378, 0.37410167636871339, 0.37613337566852567, 0.37578551383018494, 0.37696433467864993], 'val_acc': [0.90629999999999999, 0.90720000000000001, 0.90690000000000004, 0.90639999999999998, 0.90700000000000003, 0.90680000000000005, 0.90649999999999997, 0.90780000000000005, 0.90720000000000001, 0.90690000000000004, 0.90669999999999995, 0.90610000000000002, 0.90649999999999997, 0.90669999999999995, 0.90720000000000001, 0.90639999999999998, 0.90690000000000004, 0.90659999999999996, 0.90720000000000001, 0.90639999999999998, 0.90759999999999996, 0.9073, 0.90629999999999999, 0.90629999999999999, 0.90600000000000003, 0.90700000000000003, 0.90710000000000002, 0.90710000000000002, 0.90749999999999997, 0.9073, 0.90749999999999997, 0.90659999999999996, 0.9073, 0.90800000000000003, 0.90710000000000002, 0.90759999999999996, 0.90700000000000003, 0.90680000000000005, 0.90759999999999996, 0.90690000000000004, 0.90749999999999997, 0.90839999999999999, 0.90790000000000004, 0.90810000000000002, 0.90790000000000004, 0.9073, 0.90790000000000004, 0.90710000000000002, 0.90649999999999997, 0.90690000000000004], 'acc': [0.89669554058389478, 0.89681584861059005, 0.89745749117741414, 0.89785851782470472, 0.89541225539287927, 0.89679579722181435, 0.89753769650304782, 0.89659455128205123, 0.89422768143866405, 0.89585338464561926, 0.8957330766954138, 0.89681584859146768, 0.89619425731780711, 0.89601379527776415, 0.89697625926185731, 0.89908164900237264, 0.89693615652255076, 0.89655518128315836, 0.89719682390734978, 0.89475056139903453, 0.8963747192431154, 0.89679579720269187, 0.89621394230769236, 0.89743898522800258, 0.89663538662791442, 0.89751764515251697, 0.89583333333333337, 0.8951910725563248, 0.89703641323696015, 0.89703641317959282, 0.89751764513339449, 0.89765800445325339, 0.89880093042002218, 0.89685595121603956, 0.89791867181892993, 0.8977181585239683, 0.8949911774333027, 0.89649502723156582, 0.89813923644529992, 0.90028472894424427, 0.89781841516188787, 0.89809695512820509, 0.89864322414900455, 0.89837985240307838, 0.89858036577452982, 0.89797882579403276, 0.89763795312184491, 0.89882098173230818, 0.89641426282051284, 0.89886108435688017], 'loss': [0.38247765058082078, 0.38237306306405749, 0.38536368729711001, 0.38022890837099982, 0.38418068871843841, 0.38421753206656917, 0.38468616945566869, 0.38509517396107701, 0.38704997230280497, 0.38390190780334915, 0.38193131318297341, 0.38425878466686303, 0.38578086314118748, 0.38412407446947855, 0.38460791421732077, 0.37987503716300841, 0.378891275389515, 0.38247182903130694, 0.38302359059524721, 0.38172237312598684, 0.38006335567655491, 0.37968259891581757, 0.38745315521955492, 0.38236816986371253, 0.38026291069825335, 0.37708598499616292, 0.38020494740742905, 0.38690472353553651, 0.37743982981518748, 0.38049827865541536, 0.37883050631245324, 0.37772402394877286, 0.3792561863644367, 0.38013887082226278, 0.3788666189347929, 0.37660440887669017, 0.38446455074305103, 0.38157797130323734, 0.38074120329403133, 0.37764427879509116, 0.38006463212809349, 0.38043843278518091, 0.38019017297140512, 0.37979160633116227, 0.37523847440706448, 0.3762182190435836, 0.37966022781733716, 0.37660038402381141, 0.37906198035448024, 0.37974316005068864]}\n"
     ]
    }
   ],
   "source": [
    "epochs = 250\n",
    "model = load_model('relu_at_3.h5')\n",
    "opt_rms = keras.optimizers.rmsprop(lr=0.000017,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "# Fit the model\n",
    "his = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,verbose=1,validation_data=(x_test,y_test), callbacks=[lr_1], initial_epoch=200)\n",
    "# Save the model\n",
    "model.save('relu_at_3.h5')\n",
    "# Display logs\n",
    "print(his.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_test):\n",
    "    y_hat = np.argmax(y_pred, axis=1)\n",
    "    y = np.argmax(y_test, axis=1)\n",
    "\n",
    "    good = np.sum(np.equal(y, y_hat))\n",
    "    return float(good/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "0.9187\n"
     ]
    }
   ],
   "source": [
    "import keras.regularizers as regularizers\n",
    "ensembler = 0\n",
    "for i in range(3):\n",
    "    if i == 0:\n",
    "        model = create(\"relu\")\n",
    "        model.load_weights('relu_at_'+str(i+1)+'.h5')\n",
    "        model.save('relu_at_'+str(i+1)+'.h5')\n",
    "    else:\n",
    "        model = load_model('relu_at_'+str(i+1)+'.h5')\n",
    "    ensembler += model.predict_proba(x_test)\n",
    "    \n",
    "print(accuracy(ensembler, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "390/390 [==============================] - 176s 452ms/step - loss: 2.0201 - acc: 0.3906 - val_loss: 1.3828 - val_acc: 0.5563\n",
      "Epoch 2/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 1.4731 - acc: 0.5308 - val_loss: 1.1149 - val_acc: 0.6337\n",
      "Epoch 3/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 1.2980 - acc: 0.5922 - val_loss: 1.0801 - val_acc: 0.6678\n",
      "Epoch 4/200\n",
      "390/390 [==============================] - 175s 449ms/step - loss: 1.2151 - acc: 0.6298 - val_loss: 0.9754 - val_acc: 0.6856\n",
      "Epoch 5/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 1.1390 - acc: 0.6510 - val_loss: 1.5834 - val_acc: 0.6358\n",
      "Epoch 6/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 1.1029 - acc: 0.6706 - val_loss: 0.7973 - val_acc: 0.7567\n",
      "Epoch 7/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 1.0241 - acc: 0.6928 - val_loss: 0.7856 - val_acc: 0.7628\n",
      "Epoch 8/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.9956 - acc: 0.7041 - val_loss: 1.2215 - val_acc: 0.7283\n",
      "Epoch 9/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 1.0520 - acc: 0.7052 - val_loss: 1.6160 - val_acc: 0.7280\n",
      "Epoch 10/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 1.0199 - acc: 0.7127 - val_loss: 0.7939 - val_acc: 0.7845\n",
      "Epoch 11/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 1.0037 - acc: 0.7170 - val_loss: 0.7445 - val_acc: 0.7803\n",
      "Epoch 12/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.9561 - acc: 0.7259 - val_loss: 0.7351 - val_acc: 0.7896\n",
      "Epoch 13/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.9214 - acc: 0.7366 - val_loss: 0.6897 - val_acc: 0.8018\n",
      "Epoch 14/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.9240 - acc: 0.7409 - val_loss: 0.7333 - val_acc: 0.7836\n",
      "Epoch 15/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.9455 - acc: 0.7394 - val_loss: 0.6761 - val_acc: 0.8085\n",
      "Epoch 16/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.9299 - acc: 0.7462 - val_loss: 0.6588 - val_acc: 0.8135\n",
      "Epoch 17/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.9478 - acc: 0.7482 - val_loss: 0.7012 - val_acc: 0.8151\n",
      "Epoch 18/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.9236 - acc: 0.7535 - val_loss: 0.6909 - val_acc: 0.8094\n",
      "Epoch 19/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.9234 - acc: 0.7549 - val_loss: 0.6782 - val_acc: 0.8189\n",
      "Epoch 20/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.8722 - acc: 0.7605 - val_loss: 0.6582 - val_acc: 0.8214\n",
      "Epoch 21/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.8591 - acc: 0.7642 - val_loss: 0.6625 - val_acc: 0.8172\n",
      "Epoch 22/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.8269 - acc: 0.7725 - val_loss: 0.6077 - val_acc: 0.8343\n",
      "Epoch 23/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.8260 - acc: 0.7745 - val_loss: 0.6269 - val_acc: 0.8280\n",
      "Epoch 24/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.8197 - acc: 0.7769 - val_loss: 0.6442 - val_acc: 0.8197\n",
      "Epoch 25/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.8047 - acc: 0.7790 - val_loss: 0.6241 - val_acc: 0.8293\n",
      "Epoch 26/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.8188 - acc: 0.7806 - val_loss: 0.6076 - val_acc: 0.8352\n",
      "Epoch 27/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.8053 - acc: 0.7823 - val_loss: 0.6519 - val_acc: 0.8270\n",
      "Epoch 28/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.8434 - acc: 0.7808 - val_loss: 0.6330 - val_acc: 0.8318\n",
      "Epoch 29/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7910 - acc: 0.7879 - val_loss: 0.6119 - val_acc: 0.8378\n",
      "Epoch 30/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7811 - acc: 0.7891 - val_loss: 0.5965 - val_acc: 0.8451\n",
      "Epoch 31/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7707 - acc: 0.7902 - val_loss: 0.6395 - val_acc: 0.8271\n",
      "Epoch 32/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7675 - acc: 0.7932 - val_loss: 0.5810 - val_acc: 0.8464\n",
      "Epoch 33/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7828 - acc: 0.7923 - val_loss: 0.6336 - val_acc: 0.8310\n",
      "Epoch 34/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7781 - acc: 0.7948 - val_loss: 0.5732 - val_acc: 0.8480\n",
      "Epoch 35/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7599 - acc: 0.7993 - val_loss: 0.5962 - val_acc: 0.8421\n",
      "Epoch 36/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7571 - acc: 0.7990 - val_loss: 0.6151 - val_acc: 0.8365\n",
      "Epoch 37/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7565 - acc: 0.7981 - val_loss: 0.5864 - val_acc: 0.8464\n",
      "Epoch 38/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7765 - acc: 0.7984 - val_loss: 0.6086 - val_acc: 0.8371\n",
      "Epoch 39/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7530 - acc: 0.7973 - val_loss: 0.5760 - val_acc: 0.8520\n",
      "Epoch 40/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7489 - acc: 0.8030 - val_loss: 0.6164 - val_acc: 0.8407\n",
      "Epoch 41/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7573 - acc: 0.8010 - val_loss: 0.6014 - val_acc: 0.8493\n",
      "Epoch 42/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7443 - acc: 0.8053 - val_loss: 0.5915 - val_acc: 0.8531\n",
      "Epoch 43/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7395 - acc: 0.8055 - val_loss: 0.5853 - val_acc: 0.8527\n",
      "Epoch 44/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7227 - acc: 0.8074 - val_loss: 0.5628 - val_acc: 0.8596\n",
      "Epoch 45/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7235 - acc: 0.8086 - val_loss: 0.5740 - val_acc: 0.8546\n",
      "Epoch 46/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7244 - acc: 0.8098 - val_loss: 0.5851 - val_acc: 0.8470\n",
      "Epoch 47/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7256 - acc: 0.8081 - val_loss: 0.5751 - val_acc: 0.8535\n",
      "Epoch 48/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7238 - acc: 0.8093 - val_loss: 0.5957 - val_acc: 0.8468\n",
      "Epoch 49/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7146 - acc: 0.8112 - val_loss: 0.5766 - val_acc: 0.8543\n",
      "Epoch 50/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7047 - acc: 0.8129 - val_loss: 0.5858 - val_acc: 0.8507\n",
      "Epoch 51/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7186 - acc: 0.8132 - val_loss: 0.5443 - val_acc: 0.8671\n",
      "Epoch 52/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7213 - acc: 0.8124 - val_loss: 0.5546 - val_acc: 0.8629\n",
      "Epoch 53/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7064 - acc: 0.8124 - val_loss: 0.5631 - val_acc: 0.8606\n",
      "Epoch 54/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7087 - acc: 0.8195 - val_loss: 0.5347 - val_acc: 0.8653\n",
      "Epoch 55/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7108 - acc: 0.8152 - val_loss: 0.5700 - val_acc: 0.8593\n",
      "Epoch 56/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7082 - acc: 0.8148 - val_loss: 0.5746 - val_acc: 0.8537\n",
      "Epoch 57/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6987 - acc: 0.8188 - val_loss: 0.5549 - val_acc: 0.8612\n",
      "Epoch 58/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6983 - acc: 0.8193 - val_loss: 0.5453 - val_acc: 0.8599\n",
      "Epoch 59/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7052 - acc: 0.8172 - val_loss: 0.5706 - val_acc: 0.8608\n",
      "Epoch 60/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6905 - acc: 0.8199 - val_loss: 0.5770 - val_acc: 0.8546\n",
      "Epoch 61/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6847 - acc: 0.8224 - val_loss: 0.5566 - val_acc: 0.8653\n",
      "Epoch 62/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7043 - acc: 0.8194 - val_loss: 0.5629 - val_acc: 0.8581\n",
      "Epoch 63/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6949 - acc: 0.8193 - val_loss: 0.5522 - val_acc: 0.8611\n",
      "Epoch 64/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6857 - acc: 0.8212 - val_loss: 0.5427 - val_acc: 0.8663\n",
      "Epoch 65/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6768 - acc: 0.8262 - val_loss: 0.5607 - val_acc: 0.8605\n",
      "Epoch 66/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6842 - acc: 0.8230 - val_loss: 0.5553 - val_acc: 0.8618\n",
      "Epoch 67/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6684 - acc: 0.8280 - val_loss: 0.5316 - val_acc: 0.8725\n",
      "Epoch 68/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6789 - acc: 0.8240 - val_loss: 0.5659 - val_acc: 0.8556\n",
      "Epoch 69/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6752 - acc: 0.8236 - val_loss: 0.5545 - val_acc: 0.8627\n",
      "Epoch 70/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6796 - acc: 0.8253 - val_loss: 0.5433 - val_acc: 0.8695\n",
      "Epoch 71/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6776 - acc: 0.8265 - val_loss: 0.5272 - val_acc: 0.8718\n",
      "Epoch 72/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6764 - acc: 0.8266 - val_loss: 0.5555 - val_acc: 0.8612\n",
      "Epoch 73/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6706 - acc: 0.8262 - val_loss: 0.5707 - val_acc: 0.8638\n",
      "Epoch 74/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6767 - acc: 0.8271 - val_loss: 0.5402 - val_acc: 0.8703\n",
      "Epoch 75/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6697 - acc: 0.8265 - val_loss: 0.5153 - val_acc: 0.8720\n",
      "Epoch 76/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6226 - acc: 0.8408 - val_loss: 0.4931 - val_acc: 0.8828\n",
      "Epoch 77/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6095 - acc: 0.8447 - val_loss: 0.4853 - val_acc: 0.8847\n",
      "Epoch 78/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5955 - acc: 0.8486 - val_loss: 0.4940 - val_acc: 0.8829\n",
      "Epoch 79/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5962 - acc: 0.8474 - val_loss: 0.4920 - val_acc: 0.8826\n",
      "Epoch 80/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5897 - acc: 0.8493 - val_loss: 0.4860 - val_acc: 0.8858\n",
      "Epoch 81/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5869 - acc: 0.8470 - val_loss: 0.4761 - val_acc: 0.8886\n",
      "Epoch 82/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5713 - acc: 0.8523 - val_loss: 0.4877 - val_acc: 0.8820\n",
      "Epoch 83/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5799 - acc: 0.8501 - val_loss: 0.4911 - val_acc: 0.8843\n",
      "Epoch 84/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5708 - acc: 0.8521 - val_loss: 0.4748 - val_acc: 0.8885\n",
      "Epoch 85/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5667 - acc: 0.8513 - val_loss: 0.4805 - val_acc: 0.8842\n",
      "Epoch 86/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5677 - acc: 0.8520 - val_loss: 0.4675 - val_acc: 0.8858\n",
      "Epoch 87/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5537 - acc: 0.8549 - val_loss: 0.4713 - val_acc: 0.8847\n",
      "Epoch 88/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5667 - acc: 0.8520 - val_loss: 0.4694 - val_acc: 0.8878\n",
      "Epoch 89/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5627 - acc: 0.8526 - val_loss: 0.4810 - val_acc: 0.8814\n",
      "Epoch 90/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5565 - acc: 0.8538 - val_loss: 0.4680 - val_acc: 0.8862\n",
      "Epoch 91/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5535 - acc: 0.8546 - val_loss: 0.4712 - val_acc: 0.8847\n",
      "Epoch 92/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5423 - acc: 0.8576 - val_loss: 0.4586 - val_acc: 0.8902\n",
      "Epoch 93/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5474 - acc: 0.8566 - val_loss: 0.4638 - val_acc: 0.8858\n",
      "Epoch 94/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5499 - acc: 0.8535 - val_loss: 0.4542 - val_acc: 0.8877\n",
      "Epoch 95/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5362 - acc: 0.8586 - val_loss: 0.4561 - val_acc: 0.8869\n",
      "Epoch 96/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5433 - acc: 0.8543 - val_loss: 0.4426 - val_acc: 0.8915\n",
      "Epoch 97/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5360 - acc: 0.8568 - val_loss: 0.4597 - val_acc: 0.8866\n",
      "Epoch 98/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5389 - acc: 0.8566 - val_loss: 0.4537 - val_acc: 0.8865\n",
      "Epoch 99/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5435 - acc: 0.8535 - val_loss: 0.4490 - val_acc: 0.8913\n",
      "Epoch 100/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5295 - acc: 0.8574 - val_loss: 0.4588 - val_acc: 0.8837\n",
      "Epoch 101/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5126 - acc: 0.8649 - val_loss: 0.4349 - val_acc: 0.8947\n",
      "Epoch 102/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5037 - acc: 0.8662 - val_loss: 0.4344 - val_acc: 0.8927\n",
      "Epoch 103/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5049 - acc: 0.8671 - val_loss: 0.4230 - val_acc: 0.8981\n",
      "Epoch 104/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4946 - acc: 0.8680 - val_loss: 0.4370 - val_acc: 0.8915\n",
      "Epoch 105/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4985 - acc: 0.8676 - val_loss: 0.4267 - val_acc: 0.8978\n",
      "Epoch 106/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4964 - acc: 0.8688 - val_loss: 0.4269 - val_acc: 0.8958\n",
      "Epoch 107/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4917 - acc: 0.8703 - val_loss: 0.4228 - val_acc: 0.8984\n",
      "Epoch 108/200\n",
      "390/390 [==============================] - 176s 452ms/step - loss: 0.4929 - acc: 0.8693 - val_loss: 0.4239 - val_acc: 0.8957\n",
      "Epoch 109/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.4857 - acc: 0.8693 - val_loss: 0.4249 - val_acc: 0.8974\n",
      "Epoch 110/200\n",
      "390/390 [==============================] - 176s 452ms/step - loss: 0.4896 - acc: 0.8700 - val_loss: 0.4235 - val_acc: 0.8971\n",
      "Epoch 111/200\n",
      "390/390 [==============================] - 176s 452ms/step - loss: 0.4832 - acc: 0.8697 - val_loss: 0.4233 - val_acc: 0.8974\n",
      "Epoch 112/200\n",
      "390/390 [==============================] - 175s 448ms/step - loss: 0.4821 - acc: 0.8717 - val_loss: 0.4369 - val_acc: 0.8953\n",
      "Epoch 113/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4878 - acc: 0.8698 - val_loss: 0.4254 - val_acc: 0.8970\n",
      "Epoch 114/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4785 - acc: 0.8715 - val_loss: 0.4166 - val_acc: 0.8970\n",
      "Epoch 115/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4844 - acc: 0.8698 - val_loss: 0.4253 - val_acc: 0.8967\n",
      "Epoch 116/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4799 - acc: 0.8711 - val_loss: 0.4380 - val_acc: 0.8919\n",
      "Epoch 117/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4786 - acc: 0.8706 - val_loss: 0.4211 - val_acc: 0.8948\n",
      "Epoch 118/200\n",
      "390/390 [==============================] - 175s 447ms/step - loss: 0.4725 - acc: 0.8729 - val_loss: 0.4296 - val_acc: 0.8936\n",
      "Epoch 119/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4752 - acc: 0.8712 - val_loss: 0.4253 - val_acc: 0.8922\n",
      "Epoch 120/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4740 - acc: 0.8720 - val_loss: 0.4342 - val_acc: 0.8906\n",
      "Epoch 121/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4714 - acc: 0.8740 - val_loss: 0.4373 - val_acc: 0.8936\n",
      "Epoch 122/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4708 - acc: 0.8729 - val_loss: 0.4101 - val_acc: 0.8962\n",
      "Epoch 123/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4622 - acc: 0.8761 - val_loss: 0.4203 - val_acc: 0.8945\n",
      "Epoch 124/200\n",
      "390/390 [==============================] - 175s 447ms/step - loss: 0.4684 - acc: 0.8729 - val_loss: 0.4186 - val_acc: 0.8958\n",
      "Epoch 125/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4613 - acc: 0.8746 - val_loss: 0.4081 - val_acc: 0.8997\n",
      "Epoch 126/200\n",
      "390/390 [==============================] - 177s 455ms/step - loss: 0.4521 - acc: 0.8776 - val_loss: 0.4075 - val_acc: 0.8980\n",
      "Epoch 127/200\n",
      "390/390 [==============================] - 177s 453ms/step - loss: 0.4516 - acc: 0.8782 - val_loss: 0.4002 - val_acc: 0.8997\n",
      "Epoch 128/200\n",
      "390/390 [==============================] - 181s 464ms/step - loss: 0.4431 - acc: 0.8817 - val_loss: 0.4046 - val_acc: 0.9000\n",
      "Epoch 129/200\n",
      "390/390 [==============================] - 179s 458ms/step - loss: 0.4360 - acc: 0.8839 - val_loss: 0.3950 - val_acc: 0.9007\n",
      "Epoch 130/200\n",
      "390/390 [==============================] - 179s 458ms/step - loss: 0.4390 - acc: 0.8811 - val_loss: 0.3987 - val_acc: 0.9011\n",
      "Epoch 131/200\n",
      "390/390 [==============================] - 178s 457ms/step - loss: 0.4398 - acc: 0.8813 - val_loss: 0.3989 - val_acc: 0.8995\n",
      "Epoch 132/200\n",
      "390/390 [==============================] - 177s 455ms/step - loss: 0.4388 - acc: 0.8825 - val_loss: 0.4080 - val_acc: 0.8963\n",
      "Epoch 133/200\n",
      "390/390 [==============================] - 176s 452ms/step - loss: 0.4423 - acc: 0.8800 - val_loss: 0.4072 - val_acc: 0.8986\n",
      "Epoch 134/200\n",
      "390/390 [==============================] - 176s 450ms/step - loss: 0.4373 - acc: 0.8807 - val_loss: 0.4090 - val_acc: 0.8962\n",
      "Epoch 135/200\n",
      "390/390 [==============================] - 177s 455ms/step - loss: 0.4330 - acc: 0.8823 - val_loss: 0.4040 - val_acc: 0.8990\n",
      "Epoch 136/200\n",
      "390/390 [==============================] - 177s 454ms/step - loss: 0.4277 - acc: 0.8842 - val_loss: 0.3929 - val_acc: 0.9014\n",
      "Epoch 137/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.4341 - acc: 0.8825 - val_loss: 0.4060 - val_acc: 0.9006\n",
      "Epoch 138/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.4285 - acc: 0.8839 - val_loss: 0.3915 - val_acc: 0.9013\n",
      "Epoch 139/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.4296 - acc: 0.8833 - val_loss: 0.4029 - val_acc: 0.9015\n",
      "Epoch 140/200\n",
      "390/390 [==============================] - 176s 450ms/step - loss: 0.4305 - acc: 0.8839 - val_loss: 0.4011 - val_acc: 0.9009\n",
      "Epoch 141/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4266 - acc: 0.8849 - val_loss: 0.3881 - val_acc: 0.9016\n",
      "Epoch 142/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4307 - acc: 0.8837 - val_loss: 0.3954 - val_acc: 0.9019\n",
      "Epoch 143/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4255 - acc: 0.8869 - val_loss: 0.3977 - val_acc: 0.9002\n",
      "Epoch 144/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4284 - acc: 0.8838 - val_loss: 0.4010 - val_acc: 0.9011\n",
      "Epoch 145/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4236 - acc: 0.8841 - val_loss: 0.3886 - val_acc: 0.9028\n",
      "Epoch 146/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4223 - acc: 0.8849 - val_loss: 0.3961 - val_acc: 0.9010\n",
      "Epoch 147/200\n",
      "390/390 [==============================] - 175s 447ms/step - loss: 0.4214 - acc: 0.8852 - val_loss: 0.3977 - val_acc: 0.9002\n",
      "Epoch 148/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4208 - acc: 0.8858 - val_loss: 0.3911 - val_acc: 0.9024\n",
      "Epoch 149/200\n",
      "390/390 [==============================] - 175s 447ms/step - loss: 0.4228 - acc: 0.8851 - val_loss: 0.3914 - val_acc: 0.9003\n",
      "Epoch 150/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4217 - acc: 0.8861 - val_loss: 0.3957 - val_acc: 0.9004\n",
      "Epoch 151/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4115 - acc: 0.8871 - val_loss: 0.3882 - val_acc: 0.9029\n",
      "Epoch 152/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4149 - acc: 0.8877 - val_loss: 0.3919 - val_acc: 0.9037\n",
      "Epoch 153/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4113 - acc: 0.8888 - val_loss: 0.3920 - val_acc: 0.9013\n",
      "Epoch 154/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4110 - acc: 0.8882 - val_loss: 0.3838 - val_acc: 0.9035\n",
      "Epoch 155/200\n",
      "390/390 [==============================] - 176s 452ms/step - loss: 0.4095 - acc: 0.8899 - val_loss: 0.3948 - val_acc: 0.9024\n",
      "Epoch 156/200\n",
      "390/390 [==============================] - 176s 452ms/step - loss: 0.4084 - acc: 0.8893 - val_loss: 0.3909 - val_acc: 0.9042\n",
      "Epoch 157/200\n",
      "390/390 [==============================] - 176s 452ms/step - loss: 0.4045 - acc: 0.8903 - val_loss: 0.3851 - val_acc: 0.9046\n",
      "Epoch 158/200\n",
      "390/390 [==============================] - 176s 450ms/step - loss: 0.4040 - acc: 0.8916 - val_loss: 0.3865 - val_acc: 0.9032\n",
      "Epoch 159/200\n",
      "390/390 [==============================] - 175s 449ms/step - loss: 0.4052 - acc: 0.8896 - val_loss: 0.3857 - val_acc: 0.9046\n",
      "Epoch 160/200\n",
      "390/390 [==============================] - 175s 450ms/step - loss: 0.4019 - acc: 0.8913 - val_loss: 0.3871 - val_acc: 0.9056\n",
      "Epoch 161/200\n",
      "390/390 [==============================] - 175s 449ms/step - loss: 0.4055 - acc: 0.8913 - val_loss: 0.3808 - val_acc: 0.9062\n",
      "Epoch 162/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4025 - acc: 0.8923 - val_loss: 0.3901 - val_acc: 0.9044\n",
      "Epoch 163/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4029 - acc: 0.8910 - val_loss: 0.3853 - val_acc: 0.9044\n",
      "Epoch 164/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4027 - acc: 0.8910 - val_loss: 0.3823 - val_acc: 0.9061\n",
      "Epoch 165/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3982 - acc: 0.8928 - val_loss: 0.3831 - val_acc: 0.9062\n",
      "Epoch 166/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4029 - acc: 0.8903 - val_loss: 0.3881 - val_acc: 0.9056\n",
      "Epoch 167/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3985 - acc: 0.8912 - val_loss: 0.3836 - val_acc: 0.9070\n",
      "Epoch 168/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3989 - acc: 0.8925 - val_loss: 0.3874 - val_acc: 0.9035\n",
      "Epoch 169/200\n",
      "390/390 [==============================] - 177s 453ms/step - loss: 0.4048 - acc: 0.8895 - val_loss: 0.3850 - val_acc: 0.9034\n",
      "Epoch 170/200\n",
      "390/390 [==============================] - 177s 454ms/step - loss: 0.3951 - acc: 0.8937 - val_loss: 0.3849 - val_acc: 0.9045\n",
      "Epoch 171/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.3978 - acc: 0.8920 - val_loss: 0.3835 - val_acc: 0.9052\n",
      "Epoch 172/200\n",
      "390/390 [==============================] - 177s 453ms/step - loss: 0.3997 - acc: 0.8901 - val_loss: 0.3865 - val_acc: 0.9044\n",
      "Epoch 173/200\n",
      "390/390 [==============================] - 177s 453ms/step - loss: 0.3989 - acc: 0.8913 - val_loss: 0.3794 - val_acc: 0.9049\n",
      "Epoch 174/200\n",
      "390/390 [==============================] - 176s 450ms/step - loss: 0.3992 - acc: 0.8915 - val_loss: 0.3835 - val_acc: 0.9040\n",
      "Epoch 175/200\n",
      "390/390 [==============================] - 175s 449ms/step - loss: 0.3927 - acc: 0.8924 - val_loss: 0.3854 - val_acc: 0.9039\n",
      "Epoch 176/200\n",
      "390/390 [==============================] - 175s 450ms/step - loss: 0.3928 - acc: 0.8938 - val_loss: 0.3804 - val_acc: 0.9056\n",
      "Epoch 177/200\n",
      "390/390 [==============================] - 175s 449ms/step - loss: 0.3919 - acc: 0.8931 - val_loss: 0.3817 - val_acc: 0.9057\n",
      "Epoch 178/200\n",
      "390/390 [==============================] - 175s 450ms/step - loss: 0.3892 - acc: 0.8952 - val_loss: 0.3801 - val_acc: 0.9065\n",
      "Epoch 179/200\n",
      "390/390 [==============================] - 175s 450ms/step - loss: 0.3919 - acc: 0.8941 - val_loss: 0.3808 - val_acc: 0.9056\n",
      "Epoch 180/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.3892 - acc: 0.8936 - val_loss: 0.3816 - val_acc: 0.9055\n",
      "Epoch 181/200\n",
      "390/390 [==============================] - 176s 452ms/step - loss: 0.3855 - acc: 0.8959 - val_loss: 0.3810 - val_acc: 0.9069\n",
      "Epoch 182/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.3895 - acc: 0.8943 - val_loss: 0.3812 - val_acc: 0.9051\n",
      "Epoch 183/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.3922 - acc: 0.8943 - val_loss: 0.3776 - val_acc: 0.9068\n",
      "Epoch 184/200\n",
      "390/390 [==============================] - 177s 453ms/step - loss: 0.3883 - acc: 0.8951 - val_loss: 0.3796 - val_acc: 0.9074\n",
      "Epoch 185/200\n",
      "390/390 [==============================] - 180s 461ms/step - loss: 0.3942 - acc: 0.8928 - val_loss: 0.3794 - val_acc: 0.9056\n",
      "Epoch 186/200\n",
      "390/390 [==============================] - 179s 459ms/step - loss: 0.3870 - acc: 0.8960 - val_loss: 0.3777 - val_acc: 0.9072\n",
      "Epoch 187/200\n",
      "390/390 [==============================] - 177s 454ms/step - loss: 0.3899 - acc: 0.8941 - val_loss: 0.3780 - val_acc: 0.9066\n",
      "Epoch 188/200\n",
      "390/390 [==============================] - 178s 456ms/step - loss: 0.3853 - acc: 0.8971 - val_loss: 0.3778 - val_acc: 0.9066\n",
      "Epoch 189/200\n",
      "390/390 [==============================] - 178s 458ms/step - loss: 0.3878 - acc: 0.8958 - val_loss: 0.3792 - val_acc: 0.9076\n",
      "Epoch 190/200\n",
      "390/390 [==============================] - 177s 453ms/step - loss: 0.3873 - acc: 0.8956 - val_loss: 0.3781 - val_acc: 0.9085\n",
      "Epoch 191/200\n",
      "390/390 [==============================] - 176s 450ms/step - loss: 0.3876 - acc: 0.8948 - val_loss: 0.3769 - val_acc: 0.9078\n",
      "Epoch 192/200\n",
      "390/390 [==============================] - 175s 450ms/step - loss: 0.3837 - acc: 0.8973 - val_loss: 0.3768 - val_acc: 0.9075\n",
      "Epoch 193/200\n",
      "390/390 [==============================] - 175s 449ms/step - loss: 0.3876 - acc: 0.8946 - val_loss: 0.3755 - val_acc: 0.9085\n",
      "Epoch 194/200\n",
      "390/390 [==============================] - 175s 448ms/step - loss: 0.3926 - acc: 0.8941 - val_loss: 0.3756 - val_acc: 0.9087\n",
      "Epoch 195/200\n",
      "390/390 [==============================] - 175s 448ms/step - loss: 0.3839 - acc: 0.8957 - val_loss: 0.3782 - val_acc: 0.9072\n",
      "Epoch 196/200\n",
      "390/390 [==============================] - 175s 449ms/step - loss: 0.3867 - acc: 0.8934 - val_loss: 0.3760 - val_acc: 0.9079\n",
      "Epoch 197/200\n",
      "390/390 [==============================] - 175s 450ms/step - loss: 0.3884 - acc: 0.8948 - val_loss: 0.3788 - val_acc: 0.9064\n",
      "Epoch 198/200\n",
      "390/390 [==============================] - 175s 449ms/step - loss: 0.3857 - acc: 0.8943 - val_loss: 0.3773 - val_acc: 0.9082\n",
      "Epoch 199/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.3882 - acc: 0.8937 - val_loss: 0.3750 - val_acc: 0.9078\n",
      "Epoch 200/200\n",
      "390/390 [==============================] - 179s 458ms/step - loss: 0.3839 - acc: 0.8945 - val_loss: 0.3787 - val_acc: 0.9079\n",
      "{'loss': [2.0201412051543235, 1.4730862967280345, 1.2979482970713498, 1.2147280058112708, 1.139269012727147, 1.1029223366199796, 1.0239467595990437, 0.9956905530514073, 1.0515424083125946, 1.0200822336917739, 1.0034755292824533, 0.95621412508679693, 0.92128756541948809, 0.92403700344448558, 0.94516318151114798, 0.92964696276061687, 0.94772970659211919, 0.92375287334238987, 0.92339898792642561, 0.87195690629724742, 0.859061746097047, 0.82688941326820586, 0.8256743555463657, 0.81982975744994591, 0.80444548877340349, 0.81845859334680715, 0.80536214093730585, 0.84325594457633068, 0.79105372026557907, 0.78110432831339249, 0.77067945223035128, 0.76768239300947827, 0.78279785326669182, 0.77768564098035065, 0.76009057261907864, 0.75718800974534728, 0.75644770315300014, 0.77648020761135295, 0.75273463773130223, 0.74871308745426135, 0.75726116016889233, 0.74442003553466451, 0.73964915829742039, 0.72267108520789902, 0.7226381308381512, 0.72409669221652251, 0.72532766114682545, 0.72381119559972718, 0.71486470597098095, 0.70444184782645014, 0.71877365401818427, 0.72133107475259961, 0.70614163073202263, 0.70862732673249718, 0.7107884484987993, 0.70803516726120208, 0.69877851644233901, 0.69822317631458064, 0.70533661596431618, 0.69062833964614334, 0.68457893972256112, 0.70421360586559234, 0.6949754285628803, 0.68565615660722801, 0.67631265889156467, 0.68436918155023374, 0.66813396508777068, 0.6790378262953074, 0.67522547011349421, 0.67955225855255497, 0.67753855320441247, 0.67637158430727928, 0.67057134340970947, 0.67663838314795688, 0.66971177993676601, 0.62225125015126492, 0.60931481372709639, 0.59555977097221546, 0.59627075963269682, 0.58966724574565887, 0.58701672822753814, 0.57123861715967239, 0.57973551568519932, 0.57068959424462806, 0.56646132809556671, 0.56766898072909422, 0.55372014759364785, 0.566767399495406, 0.56289558686772556, 0.55642736568105189, 0.55349861041911341, 0.54233321058444484, 0.54742682563929357, 0.54975251402654512, 0.53632926970557748, 0.5433435326967484, 0.53580377313596128, 0.53897503449590234, 0.54353466469507949, 0.52961370136671282, 0.51228564876199301, 0.50358446919080502, 0.50495722298060697, 0.49455489364999738, 0.49852474362260452, 0.49637954658447719, 0.49158382002726786, 0.49286298759472674, 0.48583477858977969, 0.48968430696713222, 0.48323571493620288, 0.48210255549504205, 0.48791403813663492, 0.47857385481784542, 0.48428838068886149, 0.47997421168692034, 0.47857773732328246, 0.47253711353509853, 0.47528362542840202, 0.47406349281183147, 0.47121877701296283, 0.47093387618828247, 0.46223789463067688, 0.46835307028049078, 0.46146275175827473, 0.45219135473735983, 0.45157169251630119, 0.44313616960294205, 0.43602040043243995, 0.43912871536070824, 0.43975773578964444, 0.43873938171382437, 0.44233149824998319, 0.43719054341201491, 0.43305040742481293, 0.42767494873848955, 0.43417735627763043, 0.42849205241663779, 0.42967979190706784, 0.43050655899500673, 0.42658326962055304, 0.43048616991107402, 0.42554172776928689, 0.42844590824844364, 0.42353532736524008, 0.42230899435494013, 0.42144462462419119, 0.42079654560880775, 0.42279730167908547, 0.42176053559967519, 0.41150930931572738, 0.41490956218264829, 0.41129129154559896, 0.4111400498131107, 0.40955936027177448, 0.40845767976306824, 0.40462306337858339, 0.40404183234159763, 0.40509160658297971, 0.40194697009447294, 0.40556918224349725, 0.40244734728990322, 0.40287989163804139, 0.40252238456584111, 0.39835389512526825, 0.40276435301553987, 0.39822499628513719, 0.39888648662499826, 0.40480971340189237, 0.39509131220670846, 0.39762133907337716, 0.39969274554497158, 0.39883648013180467, 0.39916594200409378, 0.39281760699755119, 0.39283689818321132, 0.39197154339781776, 0.38883699598263377, 0.39187523818168885, 0.38914170988530933, 0.38552773503156806, 0.38947023925086721, 0.39224786360867331, 0.3883218056007498, 0.39438599584457884, 0.38695328732331596, 0.38983766656122532, 0.38517484679832498, 0.38784106591591516, 0.38731854684077777, 0.38764726293531071, 0.38371324646167265, 0.38736854988409297, 0.39255421047027295, 0.38406787209772653, 0.3866855384908045, 0.38842436121060298, 0.38571512767011856, 0.38810719323533066, 0.38372915907593669], 'val_acc': [0.55630000000000002, 0.63370000000000004, 0.66779999999999995, 0.68560000133514409, 0.63580000000000003, 0.75670000000000004, 0.76280000000000003, 0.72829999999999995, 0.72799999999999998, 0.78449999999999998, 0.78029999999999999, 0.78959999999999997, 0.80179999999999996, 0.78359999999999996, 0.8085, 0.8135, 0.81510000000000005, 0.80940000000000001, 0.81889999999999996, 0.82140000000000002, 0.81720000000000004, 0.83430000000000004, 0.82799999999999996, 0.81969999999999998, 0.82930000000000004, 0.83520000000000005, 0.82699999999999996, 0.83179999999999998, 0.83779999999999999, 0.84509999999999996, 0.82709999999999995, 0.84640000000000004, 0.83099999999999996, 0.84799999999999998, 0.84209999999999996, 0.83650000000000002, 0.84640000000000004, 0.83709999999999996, 0.85199999999999998, 0.84069999885559077, 0.84930000000000005, 0.85309999999999997, 0.85270000000000001, 0.85960000000000003, 0.85460000000000003, 0.84699999999999998, 0.85350000000000004, 0.8468, 0.85429999999999995, 0.85070000000000001, 0.86709999999999998, 0.8629, 0.86060000000000003, 0.86529999999999996, 0.85929999999999995, 0.85370000000000001, 0.86119999999999997, 0.8599, 0.86080000000000001, 0.85460000000000003, 0.86529999999999996, 0.85809999999999997, 0.86109999999999998, 0.86629999999999996, 0.86050000000000004, 0.86180000000000001, 0.87250000000000005, 0.85560000000000003, 0.86270000000000002, 0.86950000000000005, 0.87180000000000002, 0.86119999999999997, 0.86380000000000001, 0.87029999999999996, 0.872, 0.88280000000000003, 0.88470000000000004, 0.88290000000000002, 0.88260000000000005, 0.88580000000000003, 0.88859999999999995, 0.88200000000000001, 0.88429999999999997, 0.88849999999999996, 0.88419999999999999, 0.88580000000000003, 0.88470000000000004, 0.88780000000000003, 0.88139999999999996, 0.88619999999999999, 0.88470000000000004, 0.89019999999999999, 0.88580000000000003, 0.88770000000000004, 0.88690000000000002, 0.89149999999999996, 0.88660000000000005, 0.88649999999999995, 0.89129999999999998, 0.88370000000000004, 0.89470000000000005, 0.89270000000000005, 0.89810000000000001, 0.89149999999999996, 0.89780000000000004, 0.89580000000000004, 0.89839999999999998, 0.89570000000000005, 0.89739999999999998, 0.89710000000000001, 0.89739999999999998, 0.89529999999999998, 0.89700000000000002, 0.89700000000000002, 0.89670000000000005, 0.89190000000000003, 0.89480000000000004, 0.89359999999999995, 0.89219999999999999, 0.89059999999999995, 0.89359999999999995, 0.8962, 0.89449999999999996, 0.89580000000000004, 0.89970000000000006, 0.89800000000000002, 0.89970000000000006, 0.90000000000000002, 0.90069999999999995, 0.90110000000000001, 0.89949999999999997, 0.89629999999999999, 0.89859999999999995, 0.8962, 0.89900000000000002, 0.90139999999999998, 0.90059999999999996, 0.90129999999999999, 0.90149999999999997, 0.90090000000000003, 0.90159999999999996, 0.90190000000000003, 0.9002, 0.90110000000000001, 0.90280000000000005, 0.90100000000000002, 0.9002, 0.90239999999999998, 0.90029999999999999, 0.90039999999999998, 0.90290000000000004, 0.90369999999999995, 0.90129999999999999, 0.90349999999999997, 0.90239999999999998, 0.9042, 0.90459999999999996, 0.9032, 0.90459999999999996, 0.90559999999999996, 0.90620000000000001, 0.90439999999999998, 0.90439999999999998, 0.90610000000000002, 0.90620000000000001, 0.90559999999999996, 0.90700000000000003, 0.90349999999999997, 0.90339999999999998, 0.90449999999999997, 0.9052, 0.90439999999999998, 0.90490000000000004, 0.90400000000000003, 0.90390000000000004, 0.90559999999999996, 0.90569999999999995, 0.90649999999999997, 0.90559999999999996, 0.90549999999999997, 0.90690000000000004, 0.90510000000000002, 0.90680000000000005, 0.90739999999999998, 0.90559999999999996, 0.90720000000000001, 0.90659999999999996, 0.90659999999999996, 0.90759999999999996, 0.90849999999999997, 0.90780000000000005, 0.90749999999999997, 0.90849999999999997, 0.90869999999999995, 0.90720000000000001, 0.90790000000000004, 0.90639999999999998, 0.90820000000000001, 0.90780000000000005, 0.90790000000000004], 'val_loss': [1.3828066045761109, 1.1149481589317323, 1.0800856992721557, 0.97542974424362183, 1.5834138217926026, 0.79725623674392698, 0.78560814018249514, 1.2214744918823242, 1.6160156175613403, 0.79388591880798343, 0.74454604263305668, 0.73505972900390626, 0.68965172548294063, 0.73329002418518063, 0.67607630386352535, 0.65875459671020509, 0.70115664048194881, 0.69093471612930302, 0.67816493215560913, 0.65818354363441467, 0.66248381967544556, 0.60771105213165288, 0.62694776649475092, 0.64422553310394282, 0.62405767631530762, 0.60759727907180783, 0.65194696111679074, 0.63301668167114256, 0.61188423109054568, 0.59650948715209962, 0.63950466070175171, 0.58104936571121213, 0.63357513017654421, 0.57316147651672367, 0.59621555624008182, 0.61514090642929076, 0.58635173683166508, 0.60856553611755371, 0.57603694801330563, 0.61638281726837163, 0.60140842514038084, 0.59149319067001338, 0.58529000835418699, 0.5628053354263306, 0.57404150180816649, 0.58506468496322628, 0.57508620924949649, 0.59566203317642208, 0.57657867317199707, 0.58575306692123408, 0.54433997879028317, 0.55463515682220454, 0.56305325117111205, 0.53474248580932615, 0.5700468914031982, 0.5745738435745239, 0.55489535708427429, 0.54529797568321225, 0.57056295785903932, 0.57702060060501104, 0.55660605654716488, 0.56287061157226559, 0.5521870588302612, 0.54272465662956237, 0.56068901176452635, 0.55527155532836914, 0.53158151836395262, 0.56585892472267152, 0.55454040355682377, 0.54332448596954341, 0.52718350553512572, 0.55550083250999449, 0.57069375772476194, 0.54018706378936765, 0.51529249505996699, 0.49314017248153685, 0.48527694230079649, 0.49399032845497132, 0.49200497441291807, 0.48597445030212405, 0.47611082477569577, 0.48771644668579101, 0.49108706951141357, 0.47482822098731997, 0.48054147644042966, 0.46746555290222169, 0.47129365463256834, 0.46943403997421262, 0.48097795677185057, 0.46799174003601074, 0.47117161979675293, 0.4585959650993347, 0.46382991485595704, 0.45420955719947814, 0.45610471382141116, 0.44262643203735352, 0.45972531499862673, 0.45369298830032351, 0.44902858839035037, 0.45879654793739316, 0.43493123550415042, 0.43437450113296511, 0.42297578268051145, 0.43704067068099978, 0.42673912315368651, 0.42688748083114625, 0.42282746934890747, 0.42388535056114196, 0.42489575619697573, 0.42348796381950377, 0.4232664306640625, 0.43689974832534789, 0.42539454760551454, 0.41659560451507566, 0.42528611431121827, 0.43801623072624207, 0.42114722270965577, 0.42956444597244264, 0.42530335659980772, 0.43420361781120298, 0.43727777376174926, 0.41013834371566771, 0.4203367037296295, 0.41857446441650392, 0.40812465415000915, 0.40748909015655516, 0.40023262891769407, 0.40463097295761108, 0.39495950913429262, 0.3986885751247406, 0.3988555054664612, 0.40797302427291871, 0.40717832231521606, 0.40899492053985598, 0.40401787815093992, 0.39292701058387758, 0.40599501729011533, 0.39150876207351687, 0.40290233316421509, 0.40105933027267454, 0.38806440310478213, 0.39535146393775938, 0.39765832982063293, 0.40104306230545045, 0.38862742729187011, 0.39606499812602997, 0.39765700259208681, 0.39107732410430907, 0.39139668631553648, 0.39573748216629029, 0.3882148777484894, 0.39186592593193054, 0.39201340079307556, 0.38379741756916047, 0.39475980768203733, 0.39089255442619325, 0.38506788740158082, 0.38652037987709048, 0.38571582112312319, 0.38711544823646543, 0.38080652561187744, 0.3900695188522339, 0.38527872791290285, 0.38229644722938538, 0.38306267404556277, 0.38811155185699464, 0.38364315128326415, 0.38736110658645628, 0.38501691925525666, 0.3849272357940674, 0.38351216254234316, 0.3865489935398102, 0.37935949974060057, 0.38349194710254669, 0.38540293841362, 0.38042077803611757, 0.38166472558975217, 0.38014628505706788, 0.38078072862625123, 0.38159099376201627, 0.38099342994689939, 0.38124294857978819, 0.37758006939888, 0.37961545908451078, 0.37935590014457704, 0.3776896659374237, 0.37800100386142732, 0.37784726963043214, 0.37919219954013822, 0.37809970812797544, 0.37685366349220278, 0.37684274215698244, 0.37554384021759035, 0.37560194835662841, 0.37815707368850709, 0.37599482469558715, 0.37884682497978212, 0.37729874968528748, 0.37496673526763918, 0.37871543755531312], 'acc': [0.39059993583573949, 0.53091915301263892, 0.59221607316637936, 0.62987247357048737, 0.65096647413564024, 0.67065688157869452, 0.69281360284234994, 0.7040623997050981, 0.70522537694590803, 0.71272457493089658, 0.71711581647109246, 0.72589829964709651, 0.73664581332024681, 0.7409167468528699, 0.73947305104883199, 0.74629050370856742, 0.74821543148552949, 0.75348893170330744, 0.75489252488277336, 0.76050689761975965, 0.76417629132486509, 0.77249759387847583, 0.77458293226846031, 0.77684873279409983, 0.77899422519743189, 0.78071863969855482, 0.78230269487981885, 0.78083894774437257, 0.78785691373731448, 0.78901989088251223, 0.79022297084350634, 0.79317051650317461, 0.79230830930998053, 0.79485482837972554, 0.79924606991992153, 0.79898540264985707, 0.79806304138594808, 0.79843750000000002, 0.79740687215182748, 0.80303577153698769, 0.80102163461538467, 0.80521435448965661, 0.80544193132511877, 0.80742701319191823, 0.80867019568187215, 0.80977301894757925, 0.80810875840243679, 0.80929487179487181, 0.81103484262697345, 0.8129210779404541, 0.81308148859172136, 0.81235964069926359, 0.81252005136965333, 0.81947786333012507, 0.8151642628205128, 0.81480812454116447, 0.81877606676907577, 0.81929740136657192, 0.81717196023728089, 0.81985883860776243, 0.82236525503381308, 0.81945781196047185, 0.8193375039911438, 0.82124238045581988, 0.82623516199563529, 0.82298684632659613, 0.82801973054835076, 0.82392925890279112, 0.82358838628797071, 0.82525264673750098, 0.8265559833172923, 0.82661613729239503, 0.82624198717948716, 0.82719171487450527, 0.82650240384615381, 0.84092003853564545, 0.84474254092383849, 0.84853224250265979, 0.84738931661238071, 0.8492588141025641, 0.84696130378933843, 0.85232194413884843, 0.85009624635251546, 0.85210137956984577, 0.85141963422547018, 0.85204122553737571, 0.85488851459736925, 0.85194096886121118, 0.85256256013487175, 0.85382579401360137, 0.85456769325658988, 0.85757211538461542, 0.85657514450867056, 0.85352502409984254, 0.85859801094616917, 0.85428685897435896, 0.85683606297983594, 0.8565327237728585, 0.85354567307692308, 0.85739493098517505, 0.86496467560089474, 0.86627767081822116, 0.86707972409367984, 0.8680221366889973, 0.86760105872942084, 0.86876403595110829, 0.87034809107500499, 0.86925080128205123, 0.86921965319833161, 0.86996711577824537, 0.8696462945139557, 0.87171474358974355, 0.86982675647750896, 0.87151106829669256, 0.86986191391791756, 0.87108999037536095, 0.87066891245402933, 0.87291666666666667, 0.87117019566274967, 0.87201235165839242, 0.87407675014142427, 0.87277430217542207, 0.87604266925235952, 0.87293669871794877, 0.8746186576367343, 0.87756657045852082, 0.87816811038165055, 0.88165704204671302, 0.88389423076923079, 0.88110147723802479, 0.88127606669258607, 0.88253930060956043, 0.88000801282051277, 0.88077478348386562, 0.88227863333949608, 0.88429271030211642, 0.88249919796586607, 0.8839027911453321, 0.88328119983342657, 0.88388273977567877, 0.88489583333333333, 0.88375080282594731, 0.88687038815553121, 0.8837624318254732, 0.88414340714135531, 0.88486525505293556, 0.88519631410256405, 0.88575786761772046, 0.8851161858974359, 0.88606833489919501, 0.88708253050738595, 0.88769249274327577, 0.88882211538461542, 0.8881663455171469, 0.88989813921732286, 0.88925649661225392, 0.89029916590285829, 0.89156650641025637, 0.88963150292845516, 0.8913461538461539, 0.89125722545266683, 0.89230429900545394, 0.89100096244478522, 0.89108116782778612, 0.89276547962784725, 0.89037937115200216, 0.89132178378556448, 0.89246470963759872, 0.8894369586331744, 0.89372996794871795, 0.89201991004521219, 0.8900841346153846, 0.89127729615877282, 0.89152644230769229, 0.89234104044328688, 0.89383012820512819, 0.89312640363144347, 0.89525128454065661, 0.89409054487179485, 0.89360549132947975, 0.89591346153846152, 0.89434953480911128, 0.89426932944523285, 0.89509143403297742, 0.89268224145176323, 0.89601362179487176, 0.89410725750828213, 0.89719682384998245, 0.89575312796945483, 0.8955929487179487, 0.89474951832357241, 0.89725560897435896, 0.89458895307668296, 0.89405048076923077, 0.89571290942209225, 0.89348732759679472, 0.89475160256410258, 0.89432948347770291, 0.89376804621739003, 0.89452874109235114]}\n"
     ]
    }
   ],
   "source": [
    "# Create model and get ready - reproducible cell\n",
    "act, act_name = \"relu\", \"relu\"\n",
    "model = create(act)\n",
    "opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "\n",
    "# Training - \n",
    "batch_size = 128\n",
    "epochs=250\n",
    "# lr scheduler\n",
    "lr_1 = keras.callbacks.LearningRateScheduler(schedule)\n",
    "\n",
    "# Fit the model\n",
    "his = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,verbose=1,validation_data=(x_test,y_test), callbacks=[lr_1])\n",
    "# Save the model\n",
    "model.save('relu_at_2.h5')\n",
    "# Display logs\n",
    "print(his.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_test):\n",
    "    y_hat = np.argmax(y_pred, axis=1)\n",
    "    y = np.argmax(y_test, axis=1)\n",
    "\n",
    "    good = np.sum(np.equal(y, y_hat))\n",
    "    return float(good/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensembler = 0\n",
    "for i in range(3):\n",
    "    model = load_model('relu_at_+'str(i)'+.h5')\n",
    "    ensembler += model.predict_proba(x_test)\n",
    "    \n",
    "print(accuracy(ensembler, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
