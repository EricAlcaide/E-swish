{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Performing a large-scale experiment on cifar10 \"\"\"\n",
    "\n",
    "import keras\n",
    "import logging\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import cifar10\n",
    "import keras.regularizers as regularizers\n",
    "from keras.models import load_model\n",
    "\n",
    "# Record settings\n",
    "LOG_FORMAT = \"%(levelname)s %(asctime)s - %(message)s\"\n",
    "logging.basicConfig(filename=\"swish_first_exp_log.txt\",format = LOG_FORMAT, level = logging.DEBUG, filemode = \"a\")\n",
    "logs = logging.getLogger()\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#z-score\n",
    "mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "std = np.std(x_train,axis=(0,1,2,3))\n",
    "x_train = (x_train-mean)/(std+1e-7)\n",
    "x_test = (x_test-mean)/(std+1e-7)\n",
    "\n",
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train,num_classes)\n",
    "y_test = np_utils.to_categorical(y_test,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Swish activation function\n",
    "# x*sigmoid(x)\n",
    "def leaky(x):\n",
    "    return K.maximum(0.3*x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create(act):\n",
    "    baseMapNum = 32\n",
    "    weight_decay = 1e-4\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(2*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(2*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(4*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(4*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False\n",
    "    )\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def schedule(x):\n",
    "    if x < 75: \n",
    "        return 0.001\n",
    "    elif x < 100:\n",
    "        return 0.0005\n",
    "    elif x < 125:\n",
    "        return 0.0003\n",
    "    elif x<150:\n",
    "        return 0.00015\n",
    "    elif x<175:\n",
    "        return 0.000075\n",
    "    elif x<200: \n",
    "        return 0.000035\n",
    "    elif x<225:\n",
    "        return 0.000017\n",
    "    else:\n",
    "        return 0.000012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Epoch 1/250\n",
      "390/390 [==============================] - 160s 411ms/step - loss: 2.0497 - acc: 0.3803 - val_loss: 1.5434 - val_acc: 0.5072\n",
      "Epoch 2/250\n",
      "390/390 [==============================] - 156s 400ms/step - loss: 1.4977 - acc: 0.5185 - val_loss: 1.1258 - val_acc: 0.6378\n",
      "Epoch 3/250\n",
      "390/390 [==============================] - 159s 407ms/step - loss: 1.3323 - acc: 0.5770 - val_loss: 1.0530 - val_acc: 0.6606\n",
      "Epoch 4/250\n",
      "390/390 [==============================] - 157s 401ms/step - loss: 1.2426 - acc: 0.6093 - val_loss: 0.9503 - val_acc: 0.6990\n",
      "Epoch 5/250\n",
      "390/390 [==============================] - 158s 406ms/step - loss: 1.1912 - acc: 0.6337 - val_loss: 0.9501 - val_acc: 0.7043\n",
      "Epoch 6/250\n",
      "390/390 [==============================] - 158s 404ms/step - loss: 1.1404 - acc: 0.6505 - val_loss: 0.9167 - val_acc: 0.7123\n",
      "Epoch 7/250\n",
      "390/390 [==============================] - 156s 399ms/step - loss: 1.1165 - acc: 0.6575 - val_loss: 0.8119 - val_acc: 0.7536\n",
      "Epoch 8/250\n",
      "390/390 [==============================] - 156s 399ms/step - loss: 1.1051 - acc: 0.6693 - val_loss: 1.0560 - val_acc: 0.6871\n",
      "Epoch 9/250\n",
      "390/390 [==============================] - 156s 399ms/step - loss: 1.0505 - acc: 0.6839 - val_loss: 0.7961 - val_acc: 0.7609\n",
      "Epoch 10/250\n",
      "390/390 [==============================] - 156s 399ms/step - loss: 1.0288 - acc: 0.6873 - val_loss: 0.7886 - val_acc: 0.7653\n",
      "Epoch 11/250\n",
      "390/390 [==============================] - 156s 399ms/step - loss: 1.0052 - acc: 0.6981 - val_loss: 0.8151 - val_acc: 0.7677\n",
      "Epoch 12/250\n",
      "390/390 [==============================] - 155s 397ms/step - loss: 0.9966 - acc: 0.7026 - val_loss: 0.7218 - val_acc: 0.7849\n",
      "Epoch 13/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.9767 - acc: 0.7073 - val_loss: 0.7375 - val_acc: 0.7873\n",
      "Epoch 14/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.9539 - acc: 0.7103 - val_loss: 0.7404 - val_acc: 0.7859\n",
      "Epoch 15/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.9500 - acc: 0.7160 - val_loss: 0.7225 - val_acc: 0.7840\n",
      "Epoch 16/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.9283 - acc: 0.7214 - val_loss: 0.7956 - val_acc: 0.7753\n",
      "Epoch 17/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.9043 - acc: 0.7260 - val_loss: 0.7144 - val_acc: 0.8003\n",
      "Epoch 18/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.8811 - acc: 0.7322 - val_loss: 0.7141 - val_acc: 0.7914\n",
      "Epoch 19/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.8693 - acc: 0.7347 - val_loss: 0.6629 - val_acc: 0.8068\n",
      "Epoch 20/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.8460 - acc: 0.7404 - val_loss: 0.6518 - val_acc: 0.8093\n",
      "Epoch 21/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.8437 - acc: 0.7434 - val_loss: 0.6353 - val_acc: 0.8138\n",
      "Epoch 22/250\n",
      "390/390 [==============================] - 155s 396ms/step - loss: 0.8181 - acc: 0.7485 - val_loss: 0.6353 - val_acc: 0.8155\n",
      "Epoch 23/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.8200 - acc: 0.7481 - val_loss: 0.6167 - val_acc: 0.8229\n",
      "Epoch 24/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.8064 - acc: 0.7531 - val_loss: 0.6624 - val_acc: 0.8098\n",
      "Epoch 25/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.7993 - acc: 0.7546 - val_loss: 0.6195 - val_acc: 0.8232\n",
      "Epoch 26/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.7899 - acc: 0.7600 - val_loss: 0.6012 - val_acc: 0.8244\n",
      "Epoch 27/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.7837 - acc: 0.7631 - val_loss: 0.6281 - val_acc: 0.8170\n",
      "Epoch 28/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.7763 - acc: 0.7627 - val_loss: 0.6182 - val_acc: 0.8204\n",
      "Epoch 29/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.7733 - acc: 0.7653 - val_loss: 0.5992 - val_acc: 0.8281\n",
      "Epoch 30/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.7661 - acc: 0.7680 - val_loss: 0.6715 - val_acc: 0.8065\n",
      "Epoch 31/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.7620 - acc: 0.7694 - val_loss: 0.6001 - val_acc: 0.8287\n",
      "Epoch 32/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.7569 - acc: 0.7742 - val_loss: 0.5742 - val_acc: 0.8391\n",
      "Epoch 33/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7549 - acc: 0.7730 - val_loss: 0.6329 - val_acc: 0.8161\n",
      "Epoch 34/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7490 - acc: 0.7738 - val_loss: 0.6141 - val_acc: 0.8216\n",
      "Epoch 35/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7473 - acc: 0.7754 - val_loss: 0.5810 - val_acc: 0.8386\n",
      "Epoch 36/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7389 - acc: 0.7775 - val_loss: 0.5655 - val_acc: 0.8410\n",
      "Epoch 37/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7412 - acc: 0.7783 - val_loss: 0.5700 - val_acc: 0.8421\n",
      "Epoch 38/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7299 - acc: 0.7807 - val_loss: 0.5777 - val_acc: 0.8365\n",
      "Epoch 39/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7374 - acc: 0.7797 - val_loss: 0.5976 - val_acc: 0.8302\n",
      "Epoch 40/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7291 - acc: 0.7819 - val_loss: 0.5783 - val_acc: 0.8350\n",
      "Epoch 41/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7284 - acc: 0.7834 - val_loss: 0.5807 - val_acc: 0.8346\n",
      "Epoch 42/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7224 - acc: 0.7850 - val_loss: 0.5847 - val_acc: 0.8347\n",
      "Epoch 43/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7256 - acc: 0.7839 - val_loss: 0.5669 - val_acc: 0.8382\n",
      "Epoch 44/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7215 - acc: 0.7846 - val_loss: 0.5972 - val_acc: 0.8302\n",
      "Epoch 45/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7183 - acc: 0.7882 - val_loss: 0.5575 - val_acc: 0.8439\n",
      "Epoch 46/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7098 - acc: 0.7892 - val_loss: 0.6131 - val_acc: 0.8286\n",
      "Epoch 47/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7129 - acc: 0.7903 - val_loss: 0.5823 - val_acc: 0.8370\n",
      "Epoch 48/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7086 - acc: 0.7903 - val_loss: 0.5668 - val_acc: 0.8442\n",
      "Epoch 49/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7084 - acc: 0.7891 - val_loss: 0.6080 - val_acc: 0.8273\n",
      "Epoch 50/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7088 - acc: 0.7912 - val_loss: 0.5531 - val_acc: 0.8502\n",
      "Epoch 51/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7033 - acc: 0.7914 - val_loss: 0.5383 - val_acc: 0.8464\n",
      "Epoch 52/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7031 - acc: 0.7916 - val_loss: 0.5640 - val_acc: 0.8417\n",
      "Epoch 53/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6999 - acc: 0.7948 - val_loss: 0.5853 - val_acc: 0.8342\n",
      "Epoch 54/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6932 - acc: 0.7948 - val_loss: 0.5610 - val_acc: 0.8452\n",
      "Epoch 55/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6993 - acc: 0.7955 - val_loss: 0.5654 - val_acc: 0.8394\n",
      "Epoch 56/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6965 - acc: 0.7970 - val_loss: 0.5507 - val_acc: 0.8524\n",
      "Epoch 57/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6930 - acc: 0.7963 - val_loss: 0.5974 - val_acc: 0.8336\n",
      "Epoch 58/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6976 - acc: 0.7943 - val_loss: 0.5457 - val_acc: 0.8537\n",
      "Epoch 59/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6928 - acc: 0.7951 - val_loss: 0.5417 - val_acc: 0.8511\n",
      "Epoch 60/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6883 - acc: 0.8005 - val_loss: 0.5428 - val_acc: 0.8525\n",
      "Epoch 61/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6916 - acc: 0.7971 - val_loss: 0.6210 - val_acc: 0.8244\n",
      "Epoch 62/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6878 - acc: 0.7999 - val_loss: 0.5533 - val_acc: 0.8542\n",
      "Epoch 63/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6809 - acc: 0.7998 - val_loss: 0.5672 - val_acc: 0.8448\n",
      "Epoch 64/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6853 - acc: 0.7992 - val_loss: 0.5813 - val_acc: 0.8398\n",
      "Epoch 65/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6840 - acc: 0.7992 - val_loss: 0.5424 - val_acc: 0.8512\n",
      "Epoch 66/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6845 - acc: 0.8018 - val_loss: 0.5472 - val_acc: 0.8493\n",
      "Epoch 67/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6814 - acc: 0.7999 - val_loss: 0.5660 - val_acc: 0.8428\n",
      "Epoch 68/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6805 - acc: 0.8004 - val_loss: 0.5491 - val_acc: 0.8481\n",
      "Epoch 69/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.6799 - acc: 0.8017 - val_loss: 0.5508 - val_acc: 0.8496\n",
      "Epoch 70/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6801 - acc: 0.8011 - val_loss: 0.5613 - val_acc: 0.8442\n",
      "Epoch 71/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6752 - acc: 0.8051 - val_loss: 0.5512 - val_acc: 0.8497\n",
      "Epoch 72/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.6702 - acc: 0.8039 - val_loss: 0.5882 - val_acc: 0.8393\n",
      "Epoch 73/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6729 - acc: 0.8048 - val_loss: 0.5638 - val_acc: 0.8449\n",
      "Epoch 74/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6745 - acc: 0.8022 - val_loss: 0.5602 - val_acc: 0.8477\n",
      "Epoch 75/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6733 - acc: 0.8029 - val_loss: 0.5571 - val_acc: 0.8484\n",
      "Epoch 76/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6218 - acc: 0.8219 - val_loss: 0.4882 - val_acc: 0.8699\n",
      "Epoch 77/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6121 - acc: 0.8259 - val_loss: 0.4837 - val_acc: 0.8694\n",
      "Epoch 78/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6038 - acc: 0.8279 - val_loss: 0.4912 - val_acc: 0.8667\n",
      "Epoch 79/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.6038 - acc: 0.8259 - val_loss: 0.4843 - val_acc: 0.8693\n",
      "Epoch 80/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.5962 - acc: 0.8274 - val_loss: 0.4969 - val_acc: 0.8651\n",
      "Epoch 81/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5985 - acc: 0.8256 - val_loss: 0.4701 - val_acc: 0.8724\n",
      "Epoch 82/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5913 - acc: 0.8292 - val_loss: 0.4709 - val_acc: 0.8696\n",
      "Epoch 83/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5895 - acc: 0.8286 - val_loss: 0.4852 - val_acc: 0.8689\n",
      "Epoch 84/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5889 - acc: 0.8296 - val_loss: 0.4597 - val_acc: 0.8748\n",
      "Epoch 85/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5828 - acc: 0.8312 - val_loss: 0.4769 - val_acc: 0.8712\n",
      "Epoch 86/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5860 - acc: 0.8296 - val_loss: 0.4577 - val_acc: 0.8788\n",
      "Epoch 87/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5836 - acc: 0.8305 - val_loss: 0.4806 - val_acc: 0.8697\n",
      "Epoch 88/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.5811 - acc: 0.8310 - val_loss: 0.4644 - val_acc: 0.8717\n",
      "Epoch 89/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5802 - acc: 0.8324 - val_loss: 0.4677 - val_acc: 0.8730\n",
      "Epoch 90/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5731 - acc: 0.8338 - val_loss: 0.4575 - val_acc: 0.8725\n",
      "Epoch 91/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5746 - acc: 0.8327 - val_loss: 0.4578 - val_acc: 0.8738\n",
      "Epoch 92/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5747 - acc: 0.8312 - val_loss: 0.4679 - val_acc: 0.8708\n",
      "Epoch 93/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.5703 - acc: 0.8350 - val_loss: 0.4547 - val_acc: 0.8726\n",
      "Epoch 94/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5770 - acc: 0.8316 - val_loss: 0.4657 - val_acc: 0.8716\n",
      "Epoch 95/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5697 - acc: 0.8338 - val_loss: 0.4771 - val_acc: 0.8665\n",
      "Epoch 96/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5691 - acc: 0.8330 - val_loss: 0.4679 - val_acc: 0.8702\n",
      "Epoch 97/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5677 - acc: 0.8340 - val_loss: 0.4673 - val_acc: 0.8727\n",
      "Epoch 98/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5696 - acc: 0.8352 - val_loss: 0.4712 - val_acc: 0.8705\n",
      "Epoch 99/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5665 - acc: 0.8351 - val_loss: 0.4713 - val_acc: 0.8710\n",
      "Epoch 100/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5718 - acc: 0.8312 - val_loss: 0.4595 - val_acc: 0.8722\n",
      "Epoch 101/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5464 - acc: 0.8428 - val_loss: 0.4349 - val_acc: 0.8823\n",
      "Epoch 102/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5345 - acc: 0.8466 - val_loss: 0.4318 - val_acc: 0.8816\n",
      "Epoch 103/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.5333 - acc: 0.8465 - val_loss: 0.4305 - val_acc: 0.8823\n",
      "Epoch 104/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5373 - acc: 0.8433 - val_loss: 0.4250 - val_acc: 0.8850\n",
      "Epoch 105/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5330 - acc: 0.8454 - val_loss: 0.4216 - val_acc: 0.8864\n",
      "Epoch 106/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5291 - acc: 0.8457 - val_loss: 0.4333 - val_acc: 0.8831\n",
      "Epoch 107/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5250 - acc: 0.8482 - val_loss: 0.4312 - val_acc: 0.8807\n",
      "Epoch 108/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5216 - acc: 0.8506 - val_loss: 0.4475 - val_acc: 0.8761\n",
      "Epoch 109/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.5268 - acc: 0.8458 - val_loss: 0.4136 - val_acc: 0.8825\n",
      "Epoch 110/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5212 - acc: 0.8499 - val_loss: 0.4281 - val_acc: 0.8834\n",
      "Epoch 111/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5211 - acc: 0.8485 - val_loss: 0.4310 - val_acc: 0.8790\n",
      "Epoch 112/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.5206 - acc: 0.8482 - val_loss: 0.4194 - val_acc: 0.8857\n",
      "Epoch 113/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5214 - acc: 0.8498 - val_loss: 0.4262 - val_acc: 0.8829\n",
      "Epoch 114/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5144 - acc: 0.8496 - val_loss: 0.4336 - val_acc: 0.8832\n",
      "Epoch 115/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5250 - acc: 0.8467 - val_loss: 0.4232 - val_acc: 0.8832\n",
      "Epoch 116/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.5100 - acc: 0.8510 - val_loss: 0.4285 - val_acc: 0.8799\n",
      "Epoch 117/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5151 - acc: 0.8500 - val_loss: 0.4093 - val_acc: 0.8880\n",
      "Epoch 118/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5130 - acc: 0.8497 - val_loss: 0.4226 - val_acc: 0.8857\n",
      "Epoch 119/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5168 - acc: 0.8482 - val_loss: 0.4183 - val_acc: 0.8842\n",
      "Epoch 120/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5160 - acc: 0.8490 - val_loss: 0.4122 - val_acc: 0.8845\n",
      "Epoch 121/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5101 - acc: 0.8500 - val_loss: 0.4230 - val_acc: 0.8843\n",
      "Epoch 122/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5193 - acc: 0.8478 - val_loss: 0.4145 - val_acc: 0.8849\n",
      "Epoch 123/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5075 - acc: 0.8522 - val_loss: 0.4093 - val_acc: 0.8886\n",
      "Epoch 124/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5112 - acc: 0.8506 - val_loss: 0.4175 - val_acc: 0.8877\n",
      "Epoch 125/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5120 - acc: 0.8476 - val_loss: 0.4181 - val_acc: 0.8829\n",
      "Epoch 126/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4906 - acc: 0.8580 - val_loss: 0.3994 - val_acc: 0.8906\n",
      "Epoch 127/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4855 - acc: 0.8599 - val_loss: 0.3992 - val_acc: 0.8919\n",
      "Epoch 128/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4839 - acc: 0.8589 - val_loss: 0.4033 - val_acc: 0.8892\n",
      "Epoch 129/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4827 - acc: 0.8599 - val_loss: 0.3958 - val_acc: 0.8916\n",
      "Epoch 130/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4837 - acc: 0.8607 - val_loss: 0.4038 - val_acc: 0.8915\n",
      "Epoch 131/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4751 - acc: 0.8630 - val_loss: 0.3974 - val_acc: 0.8910\n",
      "Epoch 132/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4779 - acc: 0.8604 - val_loss: 0.4060 - val_acc: 0.8883\n",
      "Epoch 133/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4732 - acc: 0.8625 - val_loss: 0.4154 - val_acc: 0.8871\n",
      "Epoch 134/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4809 - acc: 0.8604 - val_loss: 0.3960 - val_acc: 0.8917\n",
      "Epoch 135/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4771 - acc: 0.8602 - val_loss: 0.3959 - val_acc: 0.8920\n",
      "Epoch 136/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4709 - acc: 0.8624 - val_loss: 0.4099 - val_acc: 0.8864\n",
      "Epoch 137/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4776 - acc: 0.8623 - val_loss: 0.3937 - val_acc: 0.8893\n",
      "Epoch 138/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4697 - acc: 0.8619 - val_loss: 0.4021 - val_acc: 0.8924\n",
      "Epoch 139/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4737 - acc: 0.8611 - val_loss: 0.3951 - val_acc: 0.8919\n",
      "Epoch 140/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4688 - acc: 0.8622 - val_loss: 0.3951 - val_acc: 0.8895\n",
      "Epoch 141/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4694 - acc: 0.8630 - val_loss: 0.3947 - val_acc: 0.8940\n",
      "Epoch 142/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4720 - acc: 0.8620 - val_loss: 0.3951 - val_acc: 0.8918\n",
      "Epoch 143/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4738 - acc: 0.8619 - val_loss: 0.3950 - val_acc: 0.8915\n",
      "Epoch 144/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4642 - acc: 0.8656 - val_loss: 0.3973 - val_acc: 0.8906\n",
      "Epoch 145/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4668 - acc: 0.8630 - val_loss: 0.3923 - val_acc: 0.8917\n",
      "Epoch 146/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4668 - acc: 0.8636 - val_loss: 0.3864 - val_acc: 0.8955\n",
      "Epoch 147/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4589 - acc: 0.8669 - val_loss: 0.3910 - val_acc: 0.8947\n",
      "Epoch 148/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4653 - acc: 0.8638 - val_loss: 0.3927 - val_acc: 0.8927\n",
      "Epoch 149/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4644 - acc: 0.8644 - val_loss: 0.3921 - val_acc: 0.8943\n",
      "Epoch 150/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4658 - acc: 0.8634 - val_loss: 0.3867 - val_acc: 0.8939\n",
      "Epoch 151/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4586 - acc: 0.8655 - val_loss: 0.3817 - val_acc: 0.8969\n",
      "Epoch 152/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4505 - acc: 0.8687 - val_loss: 0.3820 - val_acc: 0.8969\n",
      "Epoch 153/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4513 - acc: 0.8684 - val_loss: 0.3825 - val_acc: 0.8967\n",
      "Epoch 154/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4518 - acc: 0.8685 - val_loss: 0.3760 - val_acc: 0.8980\n",
      "Epoch 155/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4490 - acc: 0.8696 - val_loss: 0.3800 - val_acc: 0.8960\n",
      "Epoch 156/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4472 - acc: 0.8694 - val_loss: 0.3762 - val_acc: 0.8971\n",
      "Epoch 157/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4478 - acc: 0.8706 - val_loss: 0.3755 - val_acc: 0.8989\n",
      "Epoch 158/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4446 - acc: 0.8705 - val_loss: 0.3769 - val_acc: 0.8982\n",
      "Epoch 159/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4482 - acc: 0.8706 - val_loss: 0.3737 - val_acc: 0.8981\n",
      "Epoch 160/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4476 - acc: 0.8693 - val_loss: 0.3771 - val_acc: 0.8990\n",
      "Epoch 161/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4427 - acc: 0.8704 - val_loss: 0.3721 - val_acc: 0.8991\n",
      "Epoch 162/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4460 - acc: 0.8706 - val_loss: 0.3780 - val_acc: 0.8976\n",
      "Epoch 163/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4436 - acc: 0.8711 - val_loss: 0.3802 - val_acc: 0.8969\n",
      "Epoch 164/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4457 - acc: 0.8692 - val_loss: 0.3800 - val_acc: 0.8952\n",
      "Epoch 165/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4442 - acc: 0.8696 - val_loss: 0.3760 - val_acc: 0.8965\n",
      "Epoch 166/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4437 - acc: 0.8700 - val_loss: 0.3773 - val_acc: 0.8981\n",
      "Epoch 167/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4396 - acc: 0.8721 - val_loss: 0.3725 - val_acc: 0.8988\n",
      "Epoch 168/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4454 - acc: 0.8691 - val_loss: 0.3736 - val_acc: 0.8988\n",
      "Epoch 169/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4381 - acc: 0.8728 - val_loss: 0.3793 - val_acc: 0.8970\n",
      "Epoch 170/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4415 - acc: 0.8716 - val_loss: 0.3809 - val_acc: 0.8959\n",
      "Epoch 171/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4403 - acc: 0.8696 - val_loss: 0.3698 - val_acc: 0.9000\n",
      "Epoch 172/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4377 - acc: 0.8718 - val_loss: 0.3771 - val_acc: 0.8992\n",
      "Epoch 173/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4438 - acc: 0.8710 - val_loss: 0.3753 - val_acc: 0.8989\n",
      "Epoch 174/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4383 - acc: 0.8712 - val_loss: 0.3773 - val_acc: 0.8986\n",
      "Epoch 175/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4431 - acc: 0.8702 - val_loss: 0.3864 - val_acc: 0.8962\n",
      "Epoch 176/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4406 - acc: 0.8711 - val_loss: 0.3724 - val_acc: 0.8980\n",
      "Epoch 177/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4233 - acc: 0.8781 - val_loss: 0.3705 - val_acc: 0.9000\n",
      "Epoch 178/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4340 - acc: 0.8719 - val_loss: 0.3716 - val_acc: 0.9001\n",
      "Epoch 179/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4338 - acc: 0.8731 - val_loss: 0.3736 - val_acc: 0.8995\n",
      "Epoch 180/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4312 - acc: 0.8739 - val_loss: 0.3705 - val_acc: 0.8992\n",
      "Epoch 181/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4314 - acc: 0.8749 - val_loss: 0.3699 - val_acc: 0.9005\n",
      "Epoch 182/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4300 - acc: 0.8752 - val_loss: 0.3691 - val_acc: 0.9000\n",
      "Epoch 183/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4260 - acc: 0.8765 - val_loss: 0.3721 - val_acc: 0.8990\n",
      "Epoch 184/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4291 - acc: 0.8751 - val_loss: 0.3723 - val_acc: 0.8997\n",
      "Epoch 185/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4358 - acc: 0.8740 - val_loss: 0.3705 - val_acc: 0.8990\n",
      "Epoch 186/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4223 - acc: 0.8768 - val_loss: 0.3721 - val_acc: 0.8995\n",
      "Epoch 187/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4346 - acc: 0.8747 - val_loss: 0.3759 - val_acc: 0.8986\n",
      "Epoch 188/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4237 - acc: 0.8776 - val_loss: 0.3725 - val_acc: 0.8996\n",
      "Epoch 189/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4359 - acc: 0.8728 - val_loss: 0.3679 - val_acc: 0.9017\n",
      "Epoch 190/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4271 - acc: 0.8758 - val_loss: 0.3704 - val_acc: 0.9005\n",
      "Epoch 191/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4301 - acc: 0.8737 - val_loss: 0.3710 - val_acc: 0.8982\n",
      "Epoch 192/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4285 - acc: 0.8739 - val_loss: 0.3682 - val_acc: 0.9001\n",
      "Epoch 193/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4274 - acc: 0.8754 - val_loss: 0.3689 - val_acc: 0.9019\n",
      "Epoch 194/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4269 - acc: 0.8753 - val_loss: 0.3692 - val_acc: 0.8993\n",
      "Epoch 195/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4304 - acc: 0.8742 - val_loss: 0.3709 - val_acc: 0.8997\n",
      "Epoch 196/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4294 - acc: 0.8746 - val_loss: 0.3681 - val_acc: 0.9011\n",
      "Epoch 197/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4211 - acc: 0.8762 - val_loss: 0.3708 - val_acc: 0.9003\n",
      "Epoch 198/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4310 - acc: 0.8728 - val_loss: 0.3678 - val_acc: 0.9014\n",
      "Epoch 199/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4204 - acc: 0.8765 - val_loss: 0.3702 - val_acc: 0.9012\n",
      "Epoch 200/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4273 - acc: 0.8751 - val_loss: 0.3668 - val_acc: 0.9025\n",
      "Epoch 201/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4240 - acc: 0.8749 - val_loss: 0.3681 - val_acc: 0.9005\n",
      "Epoch 202/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4231 - acc: 0.8781 - val_loss: 0.3656 - val_acc: 0.9013\n",
      "Epoch 203/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4227 - acc: 0.8775 - val_loss: 0.3687 - val_acc: 0.9015\n",
      "Epoch 204/250\n",
      "390/390 [==============================] - 155s 397ms/step - loss: 0.4158 - acc: 0.8796 - val_loss: 0.3684 - val_acc: 0.9012\n",
      "Epoch 205/250\n",
      "390/390 [==============================] - 157s 401ms/step - loss: 0.4217 - acc: 0.8764 - val_loss: 0.3680 - val_acc: 0.9023\n",
      "Epoch 206/250\n",
      "390/390 [==============================] - 156s 400ms/step - loss: 0.4236 - acc: 0.8761 - val_loss: 0.3667 - val_acc: 0.9018\n",
      "Epoch 207/250\n",
      "390/390 [==============================] - 156s 400ms/step - loss: 0.4199 - acc: 0.8769 - val_loss: 0.3662 - val_acc: 0.9010\n",
      "Epoch 208/250\n",
      "390/390 [==============================] - 156s 399ms/step - loss: 0.4253 - acc: 0.8764 - val_loss: 0.3664 - val_acc: 0.9013\n",
      "Epoch 209/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4247 - acc: 0.8758 - val_loss: 0.3660 - val_acc: 0.9031\n",
      "Epoch 210/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4194 - acc: 0.8785 - val_loss: 0.3655 - val_acc: 0.9033\n",
      "Epoch 211/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4193 - acc: 0.8779 - val_loss: 0.3629 - val_acc: 0.9019\n",
      "Epoch 212/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4196 - acc: 0.8778 - val_loss: 0.3645 - val_acc: 0.9022\n",
      "Epoch 213/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4246 - acc: 0.8755 - val_loss: 0.3642 - val_acc: 0.9027\n",
      "Epoch 214/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4190 - acc: 0.8764 - val_loss: 0.3657 - val_acc: 0.9009\n",
      "Epoch 215/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4227 - acc: 0.8781 - val_loss: 0.3676 - val_acc: 0.9025\n",
      "Epoch 216/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4229 - acc: 0.8768 - val_loss: 0.3646 - val_acc: 0.9020\n",
      "Epoch 217/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4231 - acc: 0.8765 - val_loss: 0.3667 - val_acc: 0.9016\n",
      "Epoch 218/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4218 - acc: 0.8792 - val_loss: 0.3687 - val_acc: 0.9016\n",
      "Epoch 219/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4169 - acc: 0.8784 - val_loss: 0.3659 - val_acc: 0.9037\n",
      "Epoch 220/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4213 - acc: 0.8785 - val_loss: 0.3651 - val_acc: 0.9020\n",
      "Epoch 221/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4195 - acc: 0.8782 - val_loss: 0.3643 - val_acc: 0.9025\n",
      "Epoch 222/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4208 - acc: 0.8762 - val_loss: 0.3650 - val_acc: 0.9012\n",
      "Epoch 223/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4188 - acc: 0.8785 - val_loss: 0.3650 - val_acc: 0.9010\n",
      "Epoch 224/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4206 - acc: 0.8769 - val_loss: 0.3642 - val_acc: 0.9022\n",
      "Epoch 225/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4205 - acc: 0.8748 - val_loss: 0.3668 - val_acc: 0.9015\n",
      "Epoch 226/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4227 - acc: 0.8771 - val_loss: 0.3664 - val_acc: 0.9016\n",
      "Epoch 227/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4175 - acc: 0.8768 - val_loss: 0.3649 - val_acc: 0.9018\n",
      "Epoch 228/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4146 - acc: 0.8783 - val_loss: 0.3676 - val_acc: 0.9019\n",
      "Epoch 229/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4170 - acc: 0.8766 - val_loss: 0.3669 - val_acc: 0.9019\n",
      "Epoch 230/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4195 - acc: 0.8782 - val_loss: 0.3663 - val_acc: 0.9020\n",
      "Epoch 231/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4146 - acc: 0.8785 - val_loss: 0.3658 - val_acc: 0.9014\n",
      "Epoch 232/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4188 - acc: 0.8797 - val_loss: 0.3658 - val_acc: 0.9026\n",
      "Epoch 233/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4184 - acc: 0.8780 - val_loss: 0.3656 - val_acc: 0.9027\n",
      "Epoch 234/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4215 - acc: 0.8773 - val_loss: 0.3656 - val_acc: 0.9011\n",
      "Epoch 235/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4186 - acc: 0.8783 - val_loss: 0.3663 - val_acc: 0.9013\n",
      "Epoch 236/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4128 - acc: 0.8797 - val_loss: 0.3658 - val_acc: 0.9013\n",
      "Epoch 237/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4186 - acc: 0.8783 - val_loss: 0.3660 - val_acc: 0.9012\n",
      "Epoch 238/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4155 - acc: 0.8792 - val_loss: 0.3656 - val_acc: 0.9015\n",
      "Epoch 239/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4203 - acc: 0.8767 - val_loss: 0.3668 - val_acc: 0.9011\n",
      "Epoch 240/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4194 - acc: 0.8783 - val_loss: 0.3657 - val_acc: 0.9011\n",
      "Epoch 241/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4211 - acc: 0.8770 - val_loss: 0.3655 - val_acc: 0.9018\n",
      "Epoch 242/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4116 - acc: 0.8804 - val_loss: 0.3649 - val_acc: 0.9025\n",
      "Epoch 243/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4201 - acc: 0.8775 - val_loss: 0.3655 - val_acc: 0.9025\n",
      "Epoch 244/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4179 - acc: 0.8777 - val_loss: 0.3649 - val_acc: 0.9017\n",
      "Epoch 245/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4219 - acc: 0.8759 - val_loss: 0.3649 - val_acc: 0.9017\n",
      "Epoch 246/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4117 - acc: 0.8801 - val_loss: 0.3654 - val_acc: 0.9023\n",
      "Epoch 247/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4194 - acc: 0.8772 - val_loss: 0.3641 - val_acc: 0.9034\n",
      "Epoch 248/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4160 - acc: 0.8783 - val_loss: 0.3630 - val_acc: 0.9038\n",
      "Epoch 249/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4231 - acc: 0.8750 - val_loss: 0.3637 - val_acc: 0.9022\n",
      "Epoch 250/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4183 - acc: 0.8786 - val_loss: 0.3645 - val_acc: 0.9013\n",
      "{'val_loss': [1.5434356340408326, 1.1258149061203002, 1.0529549388885497, 0.95034396286010747, 0.95013308086395265, 0.91671153006553652, 0.81185307016372676, 1.0560010885238647, 0.79613626575469976, 0.78856168498992918, 0.81513289489746099, 0.72178837394714357, 0.73753624334335333, 0.74038853707313534, 0.72249449129104615, 0.79557975053787233, 0.71443974943161015, 0.71406953649520877, 0.66288874888420102, 0.6518380347251892, 0.63528442444801325, 0.63530824193954472, 0.61670734057426457, 0.66243103685379023, 0.61950780920982362, 0.60118625221252442, 0.62814018049240117, 0.61821136865615844, 0.59923357710838321, 0.67147289924621578, 0.60014952917098996, 0.57421350073814392, 0.63293058290481563, 0.61413907151222225, 0.58099061465263369, 0.56548011341094973, 0.57004739937782289, 0.57771571121215826, 0.59757643775939939, 0.57826159958839418, 0.58072964010238648, 0.58469004297256466, 0.56693359718322756, 0.5971707654476166, 0.55753450245857239, 0.61312366456985479, 0.5822628577709198, 0.56680996880531309, 0.60802466936111454, 0.55313376278877258, 0.53825575404167181, 0.56397207117080683, 0.58533357362747196, 0.56104719576835638, 0.56540825471878053, 0.5507257898807526, 0.59740658969879146, 0.54567935528755185, 0.54171613988876344, 0.54284796638488775, 0.62097120099067693, 0.55332985563278203, 0.56718552703857417, 0.58130892634391784, 0.5424428527355194, 0.54723837709426881, 0.56600854806900025, 0.54906285524368281, 0.55083533143997188, 0.56134757313728334, 0.55124365262985231, 0.58820301122665408, 0.56379883742332459, 0.56019655103683474, 0.55710947256088261, 0.48821637854576111, 0.48372525029182434, 0.49116680674552915, 0.48430523519515989, 0.49689690732955932, 0.47010862517356872, 0.47089478068351748, 0.48515863189697267, 0.45965447082519534, 0.47688335685729982, 0.45771792426109315, 0.48058999819755555, 0.46444438052177428, 0.46771104135513308, 0.45745631504058837, 0.45781955022811888, 0.46787357616424563, 0.45470729112625125, 0.46569477994441988, 0.47707840352058412, 0.46785336985588072, 0.46732154793739317, 0.47121453804969787, 0.47130050177574156, 0.45948193016052247, 0.43492195653915405, 0.43177326722145082, 0.43052429351806643, 0.42496807048320773, 0.42155438885688784, 0.43333560509681701, 0.43123701438903811, 0.44745009016990661, 0.41361058254241945, 0.42806454515457154, 0.43101483073234559, 0.41940549087524415, 0.4262335775375366, 0.43364839987754822, 0.42317193126678465, 0.42853009438514711, 0.40932817258834842, 0.42264183459281923, 0.41830561146736145, 0.41220114674568176, 0.42301781759262086, 0.41445427422523501, 0.40926827106475833, 0.41750700540542601, 0.41814418091773986, 0.39939708652496336, 0.39918131394386291, 0.40326075754165647, 0.3958213119506836, 0.40379271006584166, 0.39737285799980165, 0.40599585227966306, 0.41540740904808043, 0.39604961824417112, 0.39588488044738768, 0.40990783562660216, 0.39373813185691836, 0.40209175009727477, 0.39511250252723695, 0.39512256369590759, 0.39472868418693541, 0.39505214676856992, 0.39499689645767211, 0.39727873454093932, 0.39226789908409121, 0.38638600215911867, 0.39102224769592286, 0.39266233901977537, 0.39208719882965087, 0.38669717636108397, 0.3817301601409912, 0.38197511968612669, 0.3824538633823395, 0.37597452206611631, 0.38002386608123778, 0.3761859836578369, 0.37547753796577454, 0.37693296079635619, 0.37372981405258177, 0.37706913981437684, 0.37207537288665771, 0.37802881202697752, 0.38015209035873415, 0.38003891978263854, 0.37601032342910767, 0.37731604347229003, 0.37252567048072815, 0.37357555580139162, 0.3792808657169342, 0.38088427481651305, 0.36979018898010252, 0.37712745752334592, 0.3752868043899536, 0.37729648194313048, 0.38639519715309145, 0.37235339965820313, 0.37054552950859071, 0.37163046483993528, 0.37358409795761111, 0.37054867038726808, 0.36985203876495359, 0.36906854739189149, 0.37213267288208007, 0.37227040519714355, 0.3705091510772705, 0.37213657822608948, 0.37588057050704954, 0.37253608312606812, 0.3679158114910126, 0.37040586853027346, 0.3710382598400116, 0.36819866833686826, 0.36891567130088804, 0.36917122254371643, 0.37089855623245238, 0.36808927783966067, 0.37079372105598452, 0.36775009245872498, 0.37021622991561892, 0.36676858968734743, 0.3681019660949707, 0.36559600906372069, 0.36871289930343626, 0.3684041875362396, 0.36803856663703921, 0.36668211512565613, 0.36622182841300965, 0.36641469836235047, 0.36602188510894773, 0.36547687816619873, 0.36289431390762328, 0.3644535096168518, 0.36420499386787414, 0.36572673015594481, 0.36755600810050965, 0.36459574480056761, 0.36671193099021909, 0.36865636801719665, 0.36589593501091006, 0.36508620319366453, 0.36431653122901919, 0.36504897871017455, 0.36498558797836306, 0.36418798904418948, 0.3667602075099945, 0.36637795553207397, 0.3649035864830017, 0.36762639164924621, 0.3668711437702179, 0.36631480946540834, 0.36584018430709841, 0.36575808463096621, 0.36560468645095823, 0.36555675363540652, 0.36631099228858949, 0.36577963161468507, 0.36602733898162843, 0.36563904824256899, 0.36684866418838502, 0.36574229001998904, 0.36548278121948241, 0.36489976468086244, 0.36546578493118287, 0.36486298713684084, 0.36488206334114076, 0.36539640545845031, 0.36414739561080933, 0.36296739940643308, 0.36374494419097902, 0.36450429015159608], 'acc': [0.38023339748803264, 0.51848732759679472, 0.57701716393968561, 0.60923965349387077, 0.63376243186371806, 0.65034488288110215, 0.65752325950618884, 0.6693334937822295, 0.68399101700352904, 0.68729948666679352, 0.69806705169047456, 0.70259865258877419, 0.70741097205030179, 0.71033846651241872, 0.715972890561691, 0.72140680137336888, 0.72597850499185268, 0.73215431508476392, 0.73476098812961177, 0.7403954122936186, 0.74348331725402328, 0.74861645815194244, 0.74817532888007998, 0.75316811040077292, 0.75465190888675004, 0.75992540900891581, 0.76297321142123842, 0.76273259548258243, 0.76537937122849187, 0.76788578757805281, 0.76942974013474497, 0.77416185430888373, 0.77319939045864761, 0.77382098171318581, 0.77542508818761335, 0.77749037536092391, 0.77831247994866859, 0.78079884508155573, 0.77977622712235972, 0.78196182226499844, 0.78338546679499521, 0.78496952199538161, 0.78396695540583894, 0.78458854664125466, 0.78819778637125737, 0.78922040425396367, 0.79024302215579234, 0.79032322748142592, 0.78910009622726829, 0.7912255373756818, 0.79140599932011246, 0.79162660256410255, 0.79487797047514597, 0.7947345204103976, 0.79549647096567211, 0.79698026945164924, 0.79637872952851951, 0.79427333971151448, 0.79513221153846159, 0.80052935513005952, 0.79712588310854204, 0.79996791785062416, 0.79980750723760174, 0.7991786858974359, 0.79917308923596364, 0.80181264037844235, 0.79986766121270447, 0.80050930383689589, 0.80172275641025637, 0.80095937704534659, 0.80518126399768708, 0.80394631410256412, 0.80484039142111152, 0.8021234746115592, 0.8029355149181906, 0.82174366377927499, 0.82587423804940652, 0.82791947387218623, 0.82593439204363173, 0.82742387820512819, 0.8256061335902376, 0.82922676282051277, 0.82867694278136994, 0.8295837343408391, 0.83124799482861433, 0.8295837343408391, 0.83046599296105383, 0.83102964743589747, 0.83243102338171016, 0.8337949581956281, 0.83277189609214286, 0.83122794349720586, 0.83501602564102562, 0.83158718691702138, 0.83383461665678837, 0.83295235803657364, 0.83399439102564099, 0.83519810717343745, 0.8351196210661529, 0.83118784091087883, 0.84277751040756987, 0.84656721202463614, 0.84645432692307687, 0.8431880218177249, 0.84540423486031591, 0.84568495350003359, 0.84819136992608424, 0.85059752971421532, 0.84575320512820518, 0.84985147719974308, 0.84845203725351592, 0.84823717948717947, 0.84987154781014473, 0.8495548604618558, 0.84664741740763705, 0.85096153846153844, 0.8500321130570343, 0.84973532244453154, 0.84823147260802356, 0.84891321781854201, 0.85001604106512674, 0.84787054862354982, 0.85216153352582613, 0.85057747838280695, 0.84758983000295451, 0.85799278846153848, 0.85988680150314412, 0.8588541666666667, 0.85994701346833502, 0.86074350338774608, 0.86300930377952867, 0.86046278472890603, 0.86250802055168585, 0.86048283604119191, 0.86028232276535277, 0.86246791783150167, 0.86230750721847926, 0.8618663779466168, 0.86107772435897434, 0.86215478486178698, 0.86298076923076927, 0.86196663458453637, 0.86186637792749443, 0.86558686580578337, 0.86302083333333335, 0.86359987154784845, 0.86681905678537052, 0.86377125437304803, 0.86435274306038157, 0.86343038181559495, 0.86551572026294665, 0.86866987179487176, 0.86837668587706685, 0.86846955128205128, 0.86962106617199897, 0.86937099358974357, 0.87058870705190594, 0.87050417469492614, 0.8705686557778648, 0.86942572984934086, 0.87034809115149481, 0.87061298076923077, 0.87104988767429925, 0.86917951182440145, 0.86956608915007727, 0.87001201923076921, 0.87204961460526353, 0.86914501126699051, 0.87281650641025643, 0.87165142767391868, 0.8696009955233156, 0.8718149038461539, 0.87096579959550269, 0.87121394230769234, 0.87018768046198269, 0.87112636484239214, 0.87806785366724116, 0.87191209494398314, 0.87313522619814077, 0.87385817307692304, 0.87491979467436642, 0.87512030802669527, 0.8765454399103374, 0.87510025665704205, 0.87401748479923302, 0.87680461980763402, 0.87473958333333335, 0.8776091843096967, 0.87275425088225855, 0.87578124999999996, 0.8736766121079228, 0.87391618493281653, 0.87542067307692306, 0.87546162488143564, 0.87423804940648064, 0.87457892207866839, 0.8762419871794872, 0.87279222868311157, 0.87656400388810052, 0.87514022435897432, 0.87491979469348879, 0.87812800769971122, 0.87742854843326468, 0.87958733974358971, 0.87636480416789142, 0.87606272060289037, 0.87690304487179482, 0.87644508664777976, 0.87574189922386614, 0.87848893168418496, 0.87784728904087117, 0.87778445512820513, 0.87542107799782143, 0.87642501607565981, 0.87810496794871795, 0.87682467111992002, 0.87648522804085072, 0.87920673076923073, 0.87839193964662965, 0.87852903436612428, 0.87818816169393643, 0.87618302853397345, 0.87850898299647096, 0.87690304487179482, 0.87479929347488461, 0.8771254411484134, 0.87680288461538458, 0.87832852103291781, 0.8766843118191836, 0.87825144508670516, 0.87848557692307694, 0.87969201151132204, 0.87801059728336395, 0.87726362179487183, 0.878308469644142, 0.8797521655437921, 0.878351798330122, 0.87916666666666665, 0.87670600509980434, 0.87834857238344866, 0.87702323717948716, 0.88033365417375831, 0.87746631374411144, 0.87766682709644039, 0.87590230993250073, 0.88015815671162489, 0.87724358974358974, 0.8782915863840719, 0.87495993589743593, 0.87859264607604071], 'loss': [2.0500930321304547, 1.4976031004636763, 1.3322521097047382, 1.2427266239928709, 1.1910975696874575, 1.1408146837601343, 1.1161276076441971, 1.1049077060691508, 1.0504133961007196, 1.0287767725951555, 1.0053653004347685, 0.99668085292091646, 0.97646549316335263, 0.95398846627199618, 0.94987367336896122, 0.92847240066773085, 0.90440205996871037, 0.88115673597554267, 0.86922510312006651, 0.84589120951225105, 0.84352001820308187, 0.81794718291310498, 0.81988012924537013, 0.80634027992948565, 0.79932583908182631, 0.78999661376241459, 0.78393393577120418, 0.77628096478238562, 0.77329277789550677, 0.76627794987580922, 0.76185683564264728, 0.75699695930444244, 0.75475061033488777, 0.74871661458827765, 0.74730557077084747, 0.73904934931841648, 0.7412303638909542, 0.72970465998791867, 0.73729597097564514, 0.72907545348945468, 0.72828077042297867, 0.72238196669230681, 0.7256111024395181, 0.72155473633458078, 0.71815430582583162, 0.70980651907558279, 0.71294381463990442, 0.70862160495431292, 0.70829771817170317, 0.70872124076310283, 0.70328714599355424, 0.70311937446777639, 0.69989626055048615, 0.69330423728066604, 0.69937906890180124, 0.69659313193337447, 0.69297554032482089, 0.69757942199936496, 0.69283582033255164, 0.68824225687085727, 0.69141288519900967, 0.68769579519627377, 0.68080361144910606, 0.68532629074194495, 0.68404378162489998, 0.68443303544971856, 0.6813727039001215, 0.68019050336779019, 0.67986287138400936, 0.68030209403889408, 0.6751017679302288, 0.67023973747705801, 0.67278697946188659, 0.67474586131408876, 0.67316866125630614, 0.6220359365872018, 0.61215748693608762, 0.60362235305398793, 0.6038627377258573, 0.59620493681002884, 0.59853249330817693, 0.59127274812796182, 0.58941676561925838, 0.58900892838744889, 0.58282337226583136, 0.58615248242287366, 0.58359637494113226, 0.58113414897368509, 0.58016810290584742, 0.57298868923288315, 0.57459643983213726, 0.57453052795656145, 0.5703123781161431, 0.5770923207605505, 0.56965662492633362, 0.56920835793611135, 0.56773776381443708, 0.56966768518295818, 0.56616567920321592, 0.5718542139978412, 0.54645646564043371, 0.53441386705489735, 0.53333085087629462, 0.53740659626960452, 0.53286298117960418, 0.52918233761888378, 0.5250945068587467, 0.52163992036716045, 0.5268422603607178, 0.52131586635273541, 0.52113329876605086, 0.5205736680672719, 0.52133197015343991, 0.51448902828134246, 0.52507809600961641, 0.51000975462106557, 0.51514127158590362, 0.51304713465251872, 0.51675886302029506, 0.51614419229144581, 0.51000205580148705, 0.51928128601841561, 0.50756029726834029, 0.51119951913859318, 0.51197630941466254, 0.49055510705862287, 0.48563049917391038, 0.48391140898068746, 0.48268456871913806, 0.4837105608651911, 0.47504157707558864, 0.47784057516756445, 0.47318207340856988, 0.48089303398958416, 0.47702228242784195, 0.47088232270257002, 0.47761299457540868, 0.46969537511061582, 0.47374226427995242, 0.4689312421536706, 0.46944305308354206, 0.47210813093575521, 0.47383986978068887, 0.46408716858620053, 0.46675689571943041, 0.46650452617084515, 0.45908706286491247, 0.46540707114571211, 0.46451400494323086, 0.46582692645473439, 0.45860423250461491, 0.45047901272773744, 0.45130484936401183, 0.45179440891131378, 0.44896797860709997, 0.44715718328952792, 0.44778288711061986, 0.44434762929369415, 0.44829397257758058, 0.44741440760159973, 0.44275556509985081, 0.44601459923462988, 0.44362206905745605, 0.44592669930646417, 0.44419520801751322, 0.44374510806340439, 0.43976887889259048, 0.44536717197319081, 0.4381453255812327, 0.44151981068338231, 0.44032839331094009, 0.43767339694194307, 0.4438495826246498, 0.43834728063681189, 0.4431184863179855, 0.44052599463312764, 0.42328659952509123, 0.4340174231519292, 0.43375465859003776, 0.43120741225205933, 0.4313154271159908, 0.43003183751485652, 0.42587280815123901, 0.4289917024351444, 0.43586203781962585, 0.42237602524700785, 0.43458874026934308, 0.42364402087590752, 0.43593454765401513, 0.42712327425296492, 0.43009577067685428, 0.42862314620344166, 0.42737260842934632, 0.42668627815975468, 0.43036035353417651, 0.42938593242234974, 0.42110521457134148, 0.43099280008920587, 0.42036736703737448, 0.42729021937419209, 0.42401468375190388, 0.42323386391883561, 0.42265328386883438, 0.41580226371685663, 0.42178688885006449, 0.42364652945821701, 0.41991139191847582, 0.42520803632304388, 0.4248107680055932, 0.41931660497727818, 0.41928270360694547, 0.41957767957296127, 0.42471181577621914, 0.41871649166599023, 0.42273363066025271, 0.42277970874695814, 0.42316708876945763, 0.42177404455649548, 0.4168052863408605, 0.42130149548582285, 0.41931043085778108, 0.42093560513602568, 0.41874255832281104, 0.42064506262540818, 0.42054364175159503, 0.42241424285795659, 0.41753028531869252, 0.41470869756066192, 0.41687775237613051, 0.41957811716472637, 0.41464181420130608, 0.41869077269067967, 0.41838675898164224, 0.42153709821211988, 0.41861941265034913, 0.41274301978805183, 0.41855051813021482, 0.41548118736499395, 0.42032517581776158, 0.41934951123496755, 0.42105607233750514, 0.4116980476585152, 0.42009779321370999, 0.41788316102872342, 0.4220052810483228, 0.41171902098484198, 0.41943459384716475, 0.41597282764960652, 0.42307444910208386, 0.41839389100087016], 'val_acc': [0.50719999999999998, 0.63780000000000003, 0.66059999999999997, 0.69899999999999995, 0.70430000000000004, 0.71230000000000004, 0.75360000000000005, 0.68710000000000004, 0.76090000000000002, 0.76529999999999998, 0.76770000000000005, 0.78490000000000004, 0.7873, 0.78590000000000004, 0.78400000000000003, 0.77529999999999999, 0.80030000000000001, 0.79139999999999999, 0.80679999999999996, 0.80930000000000002, 0.81379999999999997, 0.8155, 0.82289999999999996, 0.80979999999999996, 0.82320000000000004, 0.82440000000000002, 0.81699999999999995, 0.82040000000000002, 0.82809999999999995, 0.80649999999999999, 0.82869999999999999, 0.83909999999999996, 0.81610000000000005, 0.8216, 0.83860000000000001, 0.84099999999999997, 0.84209999999999996, 0.83650000000000002, 0.83020000000000005, 0.83499999999999996, 0.83460000000000001, 0.8347, 0.83819999999999995, 0.83020000000000005, 0.84389999999999998, 0.8286, 0.83699999999999997, 0.84419999999999995, 0.82730000000000004, 0.85019999999999996, 0.84640000000000004, 0.8417, 0.83420000000000005, 0.84519999999999995, 0.83940000000000003, 0.85240000000000005, 0.83360000000000001, 0.85370000000000001, 0.85109999999999997, 0.85250000000000004, 0.82440000000000002, 0.85419999999999996, 0.8448, 0.83979999999999999, 0.85119999999999996, 0.84930000000000005, 0.84279999999999999, 0.84809999999999997, 0.84960000000000002, 0.84419999999999995, 0.84970000000000001, 0.83930000000000005, 0.84489999999999998, 0.84770000000000001, 0.84840000000000004, 0.86990000000000001, 0.86939999999999995, 0.86670000000000003, 0.86929999999999996, 0.86509999999999998, 0.87239999999999995, 0.86960000000000004, 0.86890000000000001, 0.87480000000000002, 0.87119999999999997, 0.87880000000000003, 0.86970000000000003, 0.87170000000000003, 0.873, 0.87250000000000005, 0.87380000000000002, 0.87079999637603756, 0.87260000000000004, 0.87160000000000004, 0.86650000000000005, 0.87019999999999997, 0.87270000000000003, 0.87050000000000005, 0.871, 0.87219999999999998, 0.88229999999999997, 0.88160000000000005, 0.88229999999999997, 0.88500000000000001, 0.88639999999999997, 0.8831, 0.88070000000000004, 0.87609999999999999, 0.88249999999999995, 0.88339999999999996, 0.879, 0.88570000000000004, 0.88290000000000002, 0.88319999999999999, 0.88319999999999999, 0.87990000000000002, 0.88800000000000001, 0.88570000000000004, 0.88419999999999999, 0.88449999999999995, 0.88429999999999997, 0.88490000000000002, 0.88859999999999995, 0.88770000000000004, 0.88290000000000002, 0.89059999999999995, 0.89190000000000003, 0.88919999999999999, 0.89159999999999995, 0.89149999999999996, 0.89100000000000001, 0.88829999999999998, 0.8871, 0.89170000000000005, 0.89200000000000002, 0.88639999999999997, 0.88929999999999998, 0.89239999999999997, 0.89190000000000003, 0.88949999999999996, 0.89400000000000002, 0.89180000000000004, 0.89149999999999996, 0.89059999999999995, 0.89170000000000005, 0.89549999999999996, 0.89470000000000005, 0.89270000000000005, 0.89429999999999998, 0.89390000000000003, 0.89690000000000003, 0.89690000000000003, 0.89670000000000005, 0.89800000000000002, 0.89600000000000002, 0.89710000000000001, 0.89890000000000003, 0.8982, 0.89810000000000001, 0.89900000000000002, 0.89910000000000001, 0.89759999999999995, 0.89690000000000003, 0.8952, 0.89649999999999996, 0.89810000000000001, 0.89880000000000004, 0.89880000000000004, 0.89700000000000002, 0.89590000000000003, 0.90000000000000002, 0.8992, 0.89890000000000003, 0.89859999999999995, 0.8962, 0.89800000000000002, 0.90000000000000002, 0.90010000000000001, 0.89949999999999997, 0.8992, 0.90049999999999997, 0.90000000000000002, 0.89900000000000002, 0.89970000000000006, 0.89900000000000002, 0.89949999999999997, 0.89859999999999995, 0.89959999999999996, 0.90169999999999995, 0.90049999999999997, 0.8982, 0.90010000000000001, 0.90190000000000003, 0.89929999999999999, 0.89970000000000006, 0.90110000000000001, 0.90029999999999999, 0.90139999999999998, 0.9012, 0.90249999999999997, 0.90049999999999997, 0.90129999999999999, 0.90149999999999997, 0.9012, 0.90229999999999999, 0.90180000000000005, 0.90100000000000002, 0.90129999999999999, 0.90310000000000001, 0.90329999999999999, 0.90190000000000003, 0.9022, 0.90269999999999995, 0.90090000000000003, 0.90249999999999997, 0.90200000000000002, 0.90159999999999996, 0.90159999999999996, 0.90369999999999995, 0.90200000000000002, 0.90249999999999997, 0.9012, 0.90100000000000002, 0.9022, 0.90149999999999997, 0.90159999999999996, 0.90180000000000005, 0.90190000000000003, 0.90190000000000003, 0.90200000000000002, 0.90139999999999998, 0.90259999999999996, 0.90269999999999995, 0.90110000000000001, 0.90129999999999999, 0.90129999999999999, 0.9012, 0.90149999999999997, 0.90110000000000001, 0.90110000000000001, 0.90180000000000005, 0.90249999999999997, 0.90249999999999997, 0.90169999999999995, 0.90169999999999995, 0.90229999999999999, 0.90339999999999998, 0.90380000000000005, 0.9022, 0.90129999999999999]}\n"
     ]
    }
   ],
   "source": [
    "# Create model and get ready - reproducible cell\n",
    "act, act_name = leaky, \"leaky\"\n",
    "model = create(act)\n",
    "opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "\n",
    "# Training - only for 125 eppochs\n",
    "batch_size = 128\n",
    "epochs=250\n",
    "# lr scheduler\n",
    "lr_1 = keras.callbacks.LearningRateScheduler(schedule)\n",
    "\n",
    "# Fit the model\n",
    "his = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,verbose=1,validation_data=(x_test,y_test), callbacks=[lr_1])\n",
    "# Save the model\n",
    "model.save('models/leaky_relu/leaky_at_2.h5')\n",
    "# Display logs\n",
    "print(his.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Epoch 1/250\n",
      "390/390 [==============================] - 158s 405ms/step - loss: 2.0187 - acc: 0.3877 - val_loss: 1.6984 - val_acc: 0.4242\n",
      "Epoch 2/250\n",
      "390/390 [==============================] - 155s 398ms/step - loss: 1.4728 - acc: 0.5273 - val_loss: 1.1026 - val_acc: 0.6552\n",
      "Epoch 3/250\n",
      "390/390 [==============================] - 155s 398ms/step - loss: 1.3274 - acc: 0.5788 - val_loss: 1.0162 - val_acc: 0.6786\n",
      "Epoch 4/250\n",
      "390/390 [==============================] - 156s 399ms/step - loss: 1.2513 - acc: 0.6113 - val_loss: 0.9912 - val_acc: 0.6954\n",
      "Epoch 5/250\n",
      "390/390 [==============================] - 155s 397ms/step - loss: 1.1921 - acc: 0.6330 - val_loss: 0.9877 - val_acc: 0.7016\n",
      "Epoch 6/250\n",
      "390/390 [==============================] - 155s 397ms/step - loss: 1.1351 - acc: 0.6514 - val_loss: 0.9224 - val_acc: 0.7261\n",
      "Epoch 7/250\n",
      "390/390 [==============================] - 155s 397ms/step - loss: 1.1040 - acc: 0.6649 - val_loss: 0.8577 - val_acc: 0.7413\n",
      "Epoch 8/250\n",
      "390/390 [==============================] - 155s 397ms/step - loss: 1.0846 - acc: 0.6734 - val_loss: 0.8276 - val_acc: 0.7465\n",
      "Epoch 9/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 1.0717 - acc: 0.6793 - val_loss: 0.7919 - val_acc: 0.7667\n",
      "Epoch 10/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 1.0318 - acc: 0.6882 - val_loss: 0.8241 - val_acc: 0.7489\n",
      "Epoch 11/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 1.0091 - acc: 0.6976 - val_loss: 0.7637 - val_acc: 0.7735\n",
      "Epoch 12/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 1.0045 - acc: 0.7013 - val_loss: 0.7834 - val_acc: 0.7789\n",
      "Epoch 13/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.9775 - acc: 0.7078 - val_loss: 0.7633 - val_acc: 0.7755\n",
      "Epoch 14/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.9667 - acc: 0.7118 - val_loss: 0.7498 - val_acc: 0.7761\n",
      "Epoch 15/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.9531 - acc: 0.7171 - val_loss: 0.7334 - val_acc: 0.7829\n",
      "Epoch 16/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.9309 - acc: 0.7235 - val_loss: 0.7671 - val_acc: 0.7907\n",
      "Epoch 17/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.9220 - acc: 0.7249 - val_loss: 0.7211 - val_acc: 0.7929\n",
      "Epoch 18/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.9041 - acc: 0.7303 - val_loss: 0.7120 - val_acc: 0.7940\n",
      "Epoch 19/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.9014 - acc: 0.7297 - val_loss: 0.7206 - val_acc: 0.7921\n",
      "Epoch 20/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.8709 - acc: 0.7373 - val_loss: 0.6885 - val_acc: 0.8060\n",
      "Epoch 21/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.8559 - acc: 0.7386 - val_loss: 0.6510 - val_acc: 0.8108\n",
      "Epoch 22/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.8403 - acc: 0.7442 - val_loss: 0.6884 - val_acc: 0.8009\n",
      "Epoch 23/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.8309 - acc: 0.7474 - val_loss: 0.6261 - val_acc: 0.8188\n",
      "Epoch 24/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.8249 - acc: 0.7464 - val_loss: 0.6359 - val_acc: 0.8186\n",
      "Epoch 25/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.8077 - acc: 0.7553 - val_loss: 0.6508 - val_acc: 0.8112\n",
      "Epoch 26/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.7998 - acc: 0.7557 - val_loss: 0.5940 - val_acc: 0.8323\n",
      "Epoch 27/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.7990 - acc: 0.7580 - val_loss: 0.6385 - val_acc: 0.8179\n",
      "Epoch 28/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.7833 - acc: 0.7616 - val_loss: 0.6023 - val_acc: 0.8304\n",
      "Epoch 29/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.7806 - acc: 0.7629 - val_loss: 0.6287 - val_acc: 0.8172\n",
      "Epoch 30/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.7735 - acc: 0.7659 - val_loss: 0.6216 - val_acc: 0.8211\n",
      "Epoch 31/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.7695 - acc: 0.7668 - val_loss: 0.6119 - val_acc: 0.8238\n",
      "Epoch 32/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.7598 - acc: 0.7697 - val_loss: 0.5801 - val_acc: 0.8383\n",
      "Epoch 33/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 154s 394ms/step - loss: 0.7559 - acc: 0.7708 - val_loss: 0.6157 - val_acc: 0.8208\n",
      "Epoch 34/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.7564 - acc: 0.7719 - val_loss: 0.5777 - val_acc: 0.8366\n",
      "Epoch 35/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.7415 - acc: 0.7770 - val_loss: 0.5707 - val_acc: 0.8409\n",
      "Epoch 36/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.7410 - acc: 0.7778 - val_loss: 0.5970 - val_acc: 0.8287\n",
      "Epoch 37/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.7365 - acc: 0.7777 - val_loss: 0.6265 - val_acc: 0.8236\n",
      "Epoch 38/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.7386 - acc: 0.7800 - val_loss: 0.6287 - val_acc: 0.8206\n",
      "Epoch 39/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.7345 - acc: 0.7794 - val_loss: 0.6056 - val_acc: 0.8251\n",
      "Epoch 40/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.7334 - acc: 0.7794 - val_loss: 0.5592 - val_acc: 0.8403\n",
      "Epoch 41/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.7323 - acc: 0.7794 - val_loss: 0.6101 - val_acc: 0.8220\n",
      "Epoch 42/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.7238 - acc: 0.7826 - val_loss: 0.5687 - val_acc: 0.8396\n",
      "Epoch 43/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.7235 - acc: 0.7849 - val_loss: 0.5998 - val_acc: 0.8275\n",
      "Epoch 44/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.7207 - acc: 0.7847 - val_loss: 0.5663 - val_acc: 0.8386\n",
      "Epoch 45/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.7185 - acc: 0.7866 - val_loss: 0.5845 - val_acc: 0.8363\n",
      "Epoch 46/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.7163 - acc: 0.7861 - val_loss: 0.5689 - val_acc: 0.8395\n",
      "Epoch 47/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7154 - acc: 0.7882 - val_loss: 0.5389 - val_acc: 0.8545\n",
      "Epoch 48/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.7077 - acc: 0.7898 - val_loss: 0.5888 - val_acc: 0.8328\n",
      "Epoch 49/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.7107 - acc: 0.7876 - val_loss: 0.5653 - val_acc: 0.8431\n",
      "Epoch 50/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.7114 - acc: 0.7893 - val_loss: 0.5476 - val_acc: 0.8460\n",
      "Epoch 51/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.7027 - acc: 0.7908 - val_loss: 0.5696 - val_acc: 0.8381\n",
      "Epoch 52/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.6998 - acc: 0.7930 - val_loss: 0.5449 - val_acc: 0.8520\n",
      "Epoch 53/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.7005 - acc: 0.7929 - val_loss: 0.5493 - val_acc: 0.8526\n",
      "Epoch 54/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.7001 - acc: 0.7927 - val_loss: 0.5469 - val_acc: 0.8471\n",
      "Epoch 55/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.6977 - acc: 0.7932 - val_loss: 0.5678 - val_acc: 0.8433\n",
      "Epoch 56/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.6920 - acc: 0.7953 - val_loss: 0.5427 - val_acc: 0.8455\n",
      "Epoch 57/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.6936 - acc: 0.7966 - val_loss: 0.5562 - val_acc: 0.8439\n",
      "Epoch 58/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.6897 - acc: 0.7977 - val_loss: 0.5381 - val_acc: 0.8518\n",
      "Epoch 59/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.6862 - acc: 0.7978 - val_loss: 0.5477 - val_acc: 0.8508\n",
      "Epoch 60/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.6902 - acc: 0.7985 - val_loss: 0.5409 - val_acc: 0.8484\n",
      "Epoch 61/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.6845 - acc: 0.8002 - val_loss: 0.5676 - val_acc: 0.8432\n",
      "Epoch 62/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.6876 - acc: 0.7961 - val_loss: 0.5776 - val_acc: 0.8355\n",
      "Epoch 63/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.6801 - acc: 0.8000 - val_loss: 0.5549 - val_acc: 0.8488\n",
      "Epoch 64/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.6936 - acc: 0.7983 - val_loss: 0.5386 - val_acc: 0.8529\n",
      "Epoch 65/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.6842 - acc: 0.8000 - val_loss: 0.5483 - val_acc: 0.8482\n",
      "Epoch 66/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.6718 - acc: 0.8014 - val_loss: 0.5532 - val_acc: 0.8464\n",
      "Epoch 67/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.6821 - acc: 0.7986 - val_loss: 0.5445 - val_acc: 0.8502\n",
      "Epoch 68/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.6805 - acc: 0.8019 - val_loss: 0.5609 - val_acc: 0.8470\n",
      "Epoch 69/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.6719 - acc: 0.8045 - val_loss: 0.5368 - val_acc: 0.8538\n",
      "Epoch 70/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.6795 - acc: 0.8033 - val_loss: 0.5334 - val_acc: 0.8550\n",
      "Epoch 71/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.6776 - acc: 0.8007 - val_loss: 0.5289 - val_acc: 0.8551\n",
      "Epoch 72/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.6675 - acc: 0.8048 - val_loss: 0.5592 - val_acc: 0.8461\n",
      "Epoch 73/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.6729 - acc: 0.8035 - val_loss: 0.5627 - val_acc: 0.8422\n",
      "Epoch 74/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.6755 - acc: 0.8016 - val_loss: 0.5352 - val_acc: 0.8570\n",
      "Epoch 75/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.6740 - acc: 0.8038 - val_loss: 0.5265 - val_acc: 0.8564\n",
      "Epoch 76/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.6197 - acc: 0.8231 - val_loss: 0.4882 - val_acc: 0.8689\n",
      "Epoch 77/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.6079 - acc: 0.8228 - val_loss: 0.4784 - val_acc: 0.8698\n",
      "Epoch 78/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.6054 - acc: 0.8267 - val_loss: 0.4693 - val_acc: 0.8750\n",
      "Epoch 79/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5995 - acc: 0.8264 - val_loss: 0.4876 - val_acc: 0.8692\n",
      "Epoch 80/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5933 - acc: 0.8294 - val_loss: 0.4814 - val_acc: 0.8722\n",
      "Epoch 81/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5943 - acc: 0.8275 - val_loss: 0.4764 - val_acc: 0.8696\n",
      "Epoch 82/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5929 - acc: 0.8291 - val_loss: 0.4643 - val_acc: 0.8743\n",
      "Epoch 83/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5861 - acc: 0.8319 - val_loss: 0.4605 - val_acc: 0.8746\n",
      "Epoch 84/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5865 - acc: 0.8313 - val_loss: 0.4609 - val_acc: 0.8769\n",
      "Epoch 85/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5835 - acc: 0.8312 - val_loss: 0.4680 - val_acc: 0.8727\n",
      "Epoch 86/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5839 - acc: 0.8322 - val_loss: 0.4677 - val_acc: 0.8727\n",
      "Epoch 87/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5843 - acc: 0.8296 - val_loss: 0.4785 - val_acc: 0.8696\n",
      "Epoch 88/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5783 - acc: 0.8317 - val_loss: 0.4733 - val_acc: 0.8688\n",
      "Epoch 89/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5788 - acc: 0.8312 - val_loss: 0.4757 - val_acc: 0.8677\n",
      "Epoch 90/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5758 - acc: 0.8330 - val_loss: 0.4666 - val_acc: 0.8700\n",
      "Epoch 91/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5709 - acc: 0.8330 - val_loss: 0.4630 - val_acc: 0.8725\n",
      "Epoch 92/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5724 - acc: 0.8347 - val_loss: 0.4670 - val_acc: 0.8711\n",
      "Epoch 93/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5658 - acc: 0.8353 - val_loss: 0.4676 - val_acc: 0.8720\n",
      "Epoch 94/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5726 - acc: 0.8330 - val_loss: 0.4584 - val_acc: 0.8747\n",
      "Epoch 95/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5703 - acc: 0.8352 - val_loss: 0.4690 - val_acc: 0.8732\n",
      "Epoch 96/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5722 - acc: 0.8324 - val_loss: 0.4423 - val_acc: 0.8789\n",
      "Epoch 97/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5648 - acc: 0.8350 - val_loss: 0.4614 - val_acc: 0.8756\n",
      "Epoch 98/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5668 - acc: 0.8348 - val_loss: 0.4466 - val_acc: 0.8803\n",
      "Epoch 99/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5646 - acc: 0.8371 - val_loss: 0.4613 - val_acc: 0.8737\n",
      "Epoch 100/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5654 - acc: 0.8343 - val_loss: 0.4508 - val_acc: 0.8783\n",
      "Epoch 101/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5393 - acc: 0.8422 - val_loss: 0.4364 - val_acc: 0.8800\n",
      "Epoch 102/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5359 - acc: 0.8433 - val_loss: 0.4305 - val_acc: 0.8818\n",
      "Epoch 103/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5389 - acc: 0.8433 - val_loss: 0.4264 - val_acc: 0.8839\n",
      "Epoch 104/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5304 - acc: 0.8461 - val_loss: 0.4202 - val_acc: 0.8860\n",
      "Epoch 105/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5285 - acc: 0.8460 - val_loss: 0.4330 - val_acc: 0.8814\n",
      "Epoch 106/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5276 - acc: 0.8463 - val_loss: 0.4243 - val_acc: 0.8848\n",
      "Epoch 107/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5255 - acc: 0.8464 - val_loss: 0.4263 - val_acc: 0.8844\n",
      "Epoch 108/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5248 - acc: 0.8484 - val_loss: 0.4317 - val_acc: 0.8828\n",
      "Epoch 109/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5234 - acc: 0.8473 - val_loss: 0.4238 - val_acc: 0.8876\n",
      "Epoch 110/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5181 - acc: 0.8485 - val_loss: 0.4450 - val_acc: 0.8761\n",
      "Epoch 111/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5255 - acc: 0.8450 - val_loss: 0.4161 - val_acc: 0.8862\n",
      "Epoch 112/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5167 - acc: 0.8493 - val_loss: 0.4255 - val_acc: 0.8826\n",
      "Epoch 113/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5150 - acc: 0.8503 - val_loss: 0.4202 - val_acc: 0.8846\n",
      "Epoch 114/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5141 - acc: 0.8497 - val_loss: 0.4259 - val_acc: 0.8843\n",
      "Epoch 115/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5159 - acc: 0.8497 - val_loss: 0.4166 - val_acc: 0.8876\n",
      "Epoch 116/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5159 - acc: 0.8474 - val_loss: 0.4232 - val_acc: 0.8818\n",
      "Epoch 117/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5132 - acc: 0.8532 - val_loss: 0.4119 - val_acc: 0.8887\n",
      "Epoch 118/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5142 - acc: 0.8506 - val_loss: 0.4162 - val_acc: 0.8863\n",
      "Epoch 119/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5102 - acc: 0.8494 - val_loss: 0.4042 - val_acc: 0.8898\n",
      "Epoch 120/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.5110 - acc: 0.8502 - val_loss: 0.4237 - val_acc: 0.8858\n",
      "Epoch 121/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5162 - acc: 0.8496 - val_loss: 0.4116 - val_acc: 0.8878\n",
      "Epoch 122/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5116 - acc: 0.8504 - val_loss: 0.4070 - val_acc: 0.8901\n",
      "Epoch 123/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5030 - acc: 0.8534 - val_loss: 0.4209 - val_acc: 0.8841\n",
      "Epoch 124/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5045 - acc: 0.8531 - val_loss: 0.4214 - val_acc: 0.8846\n",
      "Epoch 125/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.5063 - acc: 0.8523 - val_loss: 0.4031 - val_acc: 0.8935\n",
      "Epoch 126/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4880 - acc: 0.8573 - val_loss: 0.3977 - val_acc: 0.8943\n",
      "Epoch 127/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4836 - acc: 0.8597 - val_loss: 0.4005 - val_acc: 0.8914\n",
      "Epoch 128/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4837 - acc: 0.8601 - val_loss: 0.3955 - val_acc: 0.8928\n",
      "Epoch 129/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4776 - acc: 0.8602 - val_loss: 0.3921 - val_acc: 0.8965\n",
      "Epoch 130/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4767 - acc: 0.8614 - val_loss: 0.3920 - val_acc: 0.8925\n",
      "Epoch 131/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4787 - acc: 0.8594 - val_loss: 0.3978 - val_acc: 0.8923\n",
      "Epoch 132/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4780 - acc: 0.8615 - val_loss: 0.3866 - val_acc: 0.8959\n",
      "Epoch 133/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4786 - acc: 0.8605 - val_loss: 0.3959 - val_acc: 0.8936\n",
      "Epoch 134/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4749 - acc: 0.8615 - val_loss: 0.3889 - val_acc: 0.8945\n",
      "Epoch 135/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4738 - acc: 0.8615 - val_loss: 0.3883 - val_acc: 0.8928\n",
      "Epoch 136/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4742 - acc: 0.8639 - val_loss: 0.3853 - val_acc: 0.8958\n",
      "Epoch 137/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4680 - acc: 0.8633 - val_loss: 0.3842 - val_acc: 0.8980\n",
      "Epoch 138/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4750 - acc: 0.8615 - val_loss: 0.3883 - val_acc: 0.8945\n",
      "Epoch 139/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4691 - acc: 0.8638 - val_loss: 0.3870 - val_acc: 0.8955\n",
      "Epoch 140/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4710 - acc: 0.8618 - val_loss: 0.4007 - val_acc: 0.8918\n",
      "Epoch 141/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4682 - acc: 0.8624 - val_loss: 0.3821 - val_acc: 0.8986\n",
      "Epoch 142/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4710 - acc: 0.8626 - val_loss: 0.3911 - val_acc: 0.8939\n",
      "Epoch 143/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4713 - acc: 0.8622 - val_loss: 0.3865 - val_acc: 0.8936\n",
      "Epoch 144/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4646 - acc: 0.8637 - val_loss: 0.3898 - val_acc: 0.8928\n",
      "Epoch 145/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4728 - acc: 0.8617 - val_loss: 0.3886 - val_acc: 0.8928\n",
      "Epoch 146/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4638 - acc: 0.8641 - val_loss: 0.3861 - val_acc: 0.8945\n",
      "Epoch 147/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4692 - acc: 0.8643 - val_loss: 0.3844 - val_acc: 0.8956\n",
      "Epoch 148/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4643 - acc: 0.8643 - val_loss: 0.3810 - val_acc: 0.8962\n",
      "Epoch 149/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4635 - acc: 0.8640 - val_loss: 0.3778 - val_acc: 0.8945\n",
      "Epoch 150/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4656 - acc: 0.8645 - val_loss: 0.3775 - val_acc: 0.8970\n",
      "Epoch 151/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4508 - acc: 0.8674 - val_loss: 0.3747 - val_acc: 0.8983\n",
      "Epoch 152/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4501 - acc: 0.8684 - val_loss: 0.3777 - val_acc: 0.8972\n",
      "Epoch 153/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4488 - acc: 0.8699 - val_loss: 0.3742 - val_acc: 0.9002\n",
      "Epoch 154/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4495 - acc: 0.8696 - val_loss: 0.3709 - val_acc: 0.8970\n",
      "Epoch 155/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4441 - acc: 0.8702 - val_loss: 0.3732 - val_acc: 0.8999\n",
      "Epoch 156/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4450 - acc: 0.8695 - val_loss: 0.3719 - val_acc: 0.8985\n",
      "Epoch 157/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4452 - acc: 0.8690 - val_loss: 0.3732 - val_acc: 0.8981\n",
      "Epoch 158/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4479 - acc: 0.8677 - val_loss: 0.3722 - val_acc: 0.8996\n",
      "Epoch 159/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4489 - acc: 0.8685 - val_loss: 0.3735 - val_acc: 0.8997\n",
      "Epoch 160/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4447 - acc: 0.8712 - val_loss: 0.3697 - val_acc: 0.8993\n",
      "Epoch 161/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4408 - acc: 0.8713 - val_loss: 0.3734 - val_acc: 0.8972\n",
      "Epoch 162/250\n",
      "390/390 [==============================] - 156s 399ms/step - loss: 0.4461 - acc: 0.8675 - val_loss: 0.3731 - val_acc: 0.8983\n",
      "Epoch 163/250\n",
      "390/390 [==============================] - 155s 398ms/step - loss: 0.4498 - acc: 0.8689 - val_loss: 0.3729 - val_acc: 0.8977\n",
      "Epoch 164/250\n",
      "390/390 [==============================] - 155s 399ms/step - loss: 0.4383 - acc: 0.8708 - val_loss: 0.3690 - val_acc: 0.9009\n",
      "Epoch 165/250\n",
      "390/390 [==============================] - 155s 398ms/step - loss: 0.4403 - acc: 0.8718 - val_loss: 0.3741 - val_acc: 0.8974\n",
      "Epoch 166/250\n",
      "390/390 [==============================] - 155s 397ms/step - loss: 0.4407 - acc: 0.8719 - val_loss: 0.3686 - val_acc: 0.8986\n",
      "Epoch 167/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4429 - acc: 0.8689 - val_loss: 0.3727 - val_acc: 0.8999\n",
      "Epoch 168/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4434 - acc: 0.8727 - val_loss: 0.3714 - val_acc: 0.9001\n",
      "Epoch 169/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4446 - acc: 0.8696 - val_loss: 0.3705 - val_acc: 0.8993\n",
      "Epoch 170/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4371 - acc: 0.8723 - val_loss: 0.3727 - val_acc: 0.8996\n",
      "Epoch 171/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4399 - acc: 0.8705 - val_loss: 0.3696 - val_acc: 0.8983\n",
      "Epoch 172/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4431 - acc: 0.8718 - val_loss: 0.3706 - val_acc: 0.8995\n",
      "Epoch 173/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4406 - acc: 0.8699 - val_loss: 0.3718 - val_acc: 0.9002\n",
      "Epoch 174/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4363 - acc: 0.8748 - val_loss: 0.3708 - val_acc: 0.8987\n",
      "Epoch 175/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4431 - acc: 0.8698 - val_loss: 0.3684 - val_acc: 0.9018\n",
      "Epoch 176/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4340 - acc: 0.8736 - val_loss: 0.3629 - val_acc: 0.9028\n",
      "Epoch 177/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4333 - acc: 0.8743 - val_loss: 0.3659 - val_acc: 0.9018\n",
      "Epoch 178/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4313 - acc: 0.8731 - val_loss: 0.3638 - val_acc: 0.9013\n",
      "Epoch 179/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4285 - acc: 0.8745 - val_loss: 0.3638 - val_acc: 0.9016\n",
      "Epoch 180/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4359 - acc: 0.8729 - val_loss: 0.3668 - val_acc: 0.8998\n",
      "Epoch 181/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4301 - acc: 0.8775 - val_loss: 0.3630 - val_acc: 0.9008\n",
      "Epoch 182/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4323 - acc: 0.8723 - val_loss: 0.3651 - val_acc: 0.9016\n",
      "Epoch 183/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4235 - acc: 0.8757 - val_loss: 0.3654 - val_acc: 0.9017\n",
      "Epoch 184/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4366 - acc: 0.8735 - val_loss: 0.3641 - val_acc: 0.9015\n",
      "Epoch 185/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4233 - acc: 0.8770 - val_loss: 0.3638 - val_acc: 0.9016\n",
      "Epoch 186/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4293 - acc: 0.8755 - val_loss: 0.3637 - val_acc: 0.9006\n",
      "Epoch 187/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4299 - acc: 0.8730 - val_loss: 0.3661 - val_acc: 0.9003\n",
      "Epoch 188/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4300 - acc: 0.8730 - val_loss: 0.3629 - val_acc: 0.9020\n",
      "Epoch 189/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4273 - acc: 0.8756 - val_loss: 0.3606 - val_acc: 0.9022\n",
      "Epoch 190/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4270 - acc: 0.8752 - val_loss: 0.3617 - val_acc: 0.9025\n",
      "Epoch 191/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4269 - acc: 0.8765 - val_loss: 0.3629 - val_acc: 0.9008\n",
      "Epoch 192/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4243 - acc: 0.8764 - val_loss: 0.3652 - val_acc: 0.9006\n",
      "Epoch 193/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4301 - acc: 0.8764 - val_loss: 0.3647 - val_acc: 0.9016\n",
      "Epoch 194/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4267 - acc: 0.8763 - val_loss: 0.3645 - val_acc: 0.9022\n",
      "Epoch 195/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4189 - acc: 0.8788 - val_loss: 0.3646 - val_acc: 0.9010\n",
      "Epoch 196/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4214 - acc: 0.8784 - val_loss: 0.3616 - val_acc: 0.9032\n",
      "Epoch 197/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4298 - acc: 0.8745 - val_loss: 0.3649 - val_acc: 0.9007\n",
      "Epoch 198/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4275 - acc: 0.8737 - val_loss: 0.3647 - val_acc: 0.9019\n",
      "Epoch 199/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4266 - acc: 0.8759 - val_loss: 0.3628 - val_acc: 0.9020\n",
      "Epoch 200/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4250 - acc: 0.8770 - val_loss: 0.3624 - val_acc: 0.9003\n",
      "Epoch 201/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4267 - acc: 0.8755 - val_loss: 0.3625 - val_acc: 0.9021\n",
      "Epoch 202/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4228 - acc: 0.8771 - val_loss: 0.3623 - val_acc: 0.9031\n",
      "Epoch 203/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4228 - acc: 0.8753 - val_loss: 0.3599 - val_acc: 0.9021\n",
      "Epoch 204/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4209 - acc: 0.8764 - val_loss: 0.3619 - val_acc: 0.9029\n",
      "Epoch 205/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4260 - acc: 0.8760 - val_loss: 0.3597 - val_acc: 0.9030\n",
      "Epoch 206/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4255 - acc: 0.8763 - val_loss: 0.3605 - val_acc: 0.9030\n",
      "Epoch 207/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4246 - acc: 0.8737 - val_loss: 0.3609 - val_acc: 0.9034\n",
      "Epoch 208/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4227 - acc: 0.8758 - val_loss: 0.3623 - val_acc: 0.9029\n",
      "Epoch 209/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4202 - acc: 0.8783 - val_loss: 0.3602 - val_acc: 0.9028\n",
      "Epoch 210/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4192 - acc: 0.8769 - val_loss: 0.3605 - val_acc: 0.9026\n",
      "Epoch 211/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4259 - acc: 0.8754 - val_loss: 0.3601 - val_acc: 0.9043\n",
      "Epoch 212/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4221 - acc: 0.8769 - val_loss: 0.3610 - val_acc: 0.9035\n",
      "Epoch 213/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4216 - acc: 0.8767 - val_loss: 0.3593 - val_acc: 0.9041\n",
      "Epoch 214/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4192 - acc: 0.8782 - val_loss: 0.3621 - val_acc: 0.9037\n",
      "Epoch 215/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4226 - acc: 0.8766 - val_loss: 0.3615 - val_acc: 0.9028\n",
      "Epoch 216/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4167 - acc: 0.8776 - val_loss: 0.3610 - val_acc: 0.9020\n",
      "Epoch 217/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4214 - acc: 0.8763 - val_loss: 0.3619 - val_acc: 0.9017\n",
      "Epoch 218/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4186 - acc: 0.8772 - val_loss: 0.3593 - val_acc: 0.9037\n",
      "Epoch 219/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4206 - acc: 0.8757 - val_loss: 0.3622 - val_acc: 0.9026\n",
      "Epoch 220/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4214 - acc: 0.8755 - val_loss: 0.3602 - val_acc: 0.9019\n",
      "Epoch 221/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4148 - acc: 0.8795 - val_loss: 0.3605 - val_acc: 0.9033\n",
      "Epoch 222/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4223 - acc: 0.8751 - val_loss: 0.3587 - val_acc: 0.9031\n",
      "Epoch 223/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4173 - acc: 0.8779 - val_loss: 0.3598 - val_acc: 0.9030\n",
      "Epoch 224/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4185 - acc: 0.8767 - val_loss: 0.3573 - val_acc: 0.9047\n",
      "Epoch 225/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4214 - acc: 0.8768 - val_loss: 0.3585 - val_acc: 0.9035\n",
      "Epoch 226/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4181 - acc: 0.8769 - val_loss: 0.3583 - val_acc: 0.9034\n",
      "Epoch 227/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4141 - acc: 0.8804 - val_loss: 0.3583 - val_acc: 0.9021\n",
      "Epoch 228/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4136 - acc: 0.8809 - val_loss: 0.3588 - val_acc: 0.9035\n",
      "Epoch 229/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4133 - acc: 0.8782 - val_loss: 0.3595 - val_acc: 0.9032\n",
      "Epoch 230/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4223 - acc: 0.8776 - val_loss: 0.3595 - val_acc: 0.9036\n",
      "Epoch 231/250\n",
      "390/390 [==============================] - 153s 394ms/step - loss: 0.4159 - acc: 0.8796 - val_loss: 0.3593 - val_acc: 0.9020\n",
      "Epoch 232/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4213 - acc: 0.8769 - val_loss: 0.3590 - val_acc: 0.9032\n",
      "Epoch 233/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4140 - acc: 0.8785 - val_loss: 0.3591 - val_acc: 0.9030\n",
      "Epoch 234/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4160 - acc: 0.8787 - val_loss: 0.3588 - val_acc: 0.9033\n",
      "Epoch 235/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4162 - acc: 0.8797 - val_loss: 0.3582 - val_acc: 0.9028\n",
      "Epoch 236/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4176 - acc: 0.8759 - val_loss: 0.3595 - val_acc: 0.9023\n",
      "Epoch 237/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4183 - acc: 0.8766 - val_loss: 0.3578 - val_acc: 0.9032\n",
      "Epoch 238/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4218 - acc: 0.8763 - val_loss: 0.3598 - val_acc: 0.9023\n",
      "Epoch 239/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4122 - acc: 0.8801 - val_loss: 0.3581 - val_acc: 0.9029\n",
      "Epoch 240/250\n",
      "390/390 [==============================] - 155s 397ms/step - loss: 0.4158 - acc: 0.8780 - val_loss: 0.3574 - val_acc: 0.9036\n",
      "Epoch 241/250\n",
      "390/390 [==============================] - 155s 397ms/step - loss: 0.4185 - acc: 0.8769 - val_loss: 0.3578 - val_acc: 0.9017\n",
      "Epoch 242/250\n",
      "390/390 [==============================] - 155s 397ms/step - loss: 0.4185 - acc: 0.8777 - val_loss: 0.3583 - val_acc: 0.9038\n",
      "Epoch 243/250\n",
      "390/390 [==============================] - 155s 396ms/step - loss: 0.4181 - acc: 0.8785 - val_loss: 0.3591 - val_acc: 0.9035\n",
      "Epoch 244/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4131 - acc: 0.8802 - val_loss: 0.3593 - val_acc: 0.9032\n",
      "Epoch 245/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4169 - acc: 0.8781 - val_loss: 0.3590 - val_acc: 0.9036\n",
      "Epoch 246/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4229 - acc: 0.8756 - val_loss: 0.3575 - val_acc: 0.9037\n",
      "Epoch 247/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4151 - acc: 0.8804 - val_loss: 0.3577 - val_acc: 0.9033\n",
      "Epoch 248/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4190 - acc: 0.8774 - val_loss: 0.3573 - val_acc: 0.9033\n",
      "Epoch 249/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4117 - acc: 0.8800 - val_loss: 0.3579 - val_acc: 0.9040\n",
      "Epoch 250/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4163 - acc: 0.8794 - val_loss: 0.3575 - val_acc: 0.9041\n",
      "{'val_loss': [1.69844476146698, 1.10255758934021, 1.0162035938262939, 0.99121091995239263, 0.98769437923431391, 0.92243268432617187, 0.85773613691329953, 0.82757522525787353, 0.7918540649414062, 0.82412618999481202, 0.76367973775863651, 0.78343602600097662, 0.76326329460144038, 0.74984951562881474, 0.73339763879775999, 0.76710075254440313, 0.72111176176071168, 0.71199607219696048, 0.72063539810180666, 0.68846529235839848, 0.65099856433868408, 0.68842286310195921, 0.62607128496170039, 0.63591160545349124, 0.65079018630981444, 0.59402961940765386, 0.63850093641281125, 0.60233068556785585, 0.62873395338058469, 0.62157561225891111, 0.61189703245162963, 0.58006687221527098, 0.61572996826171877, 0.57769016299247744, 0.57066855144500728, 0.59703390197753903, 0.62647314252853392, 0.62873158750534053, 0.60559868993759158, 0.55915756092071534, 0.61012804961204525, 0.56873265495300296, 0.59981377668380742, 0.56632644696235657, 0.58450867495536807, 0.56888947129249567, 0.53891247615814208, 0.58882334079742427, 0.56526924972534176, 0.54758558001518254, 0.56955878534317017, 0.54491399784088135, 0.54929418334960933, 0.54694432783126834, 0.56776758995056154, 0.5427351209640503, 0.55622307615280153, 0.53806647281646725, 0.54773270750045777, 0.54089007625579832, 0.56763276519775385, 0.57755228500366207, 0.55494134793281558, 0.53859292178153995, 0.54829431252479555, 0.55324373488426204, 0.54453772883415219, 0.56086219110488889, 0.53677828936576844, 0.53338422908782956, 0.52888446969985958, 0.55920011038780215, 0.56269969186782842, 0.53518657360076904, 0.52654424161911006, 0.4881937255382538, 0.47843701744079592, 0.46929176034927367, 0.48763456616401674, 0.48144071636199953, 0.47639603404998782, 0.46426149969100949, 0.46054615998268128, 0.46088529849052429, 0.46803397707939148, 0.46773505878448485, 0.47848197002410886, 0.47326397280693056, 0.47566709160804749, 0.46659362068176269, 0.46299011578559873, 0.46699074654579165, 0.46761678261756895, 0.45837152781486512, 0.46902015352249143, 0.44232066183090207, 0.46136799964904784, 0.44657934355735779, 0.46134043264389041, 0.4508156590461731, 0.43644471373558047, 0.43051472363471988, 0.42643908019065857, 0.42016448245048521, 0.43298595933914186, 0.42432867054939272, 0.42632327842712403, 0.43174253277778624, 0.42376273126602171, 0.44503649044036864, 0.41606077661514285, 0.42553251624107363, 0.42022109155654908, 0.42588692588806154, 0.4165972906112671, 0.4232101818084717, 0.41189677658081053, 0.41617748517990111, 0.40418667476177217, 0.42374472823143006, 0.41155695397853853, 0.4070392175912857, 0.4208857940673828, 0.42136451444625855, 0.40306153678894041, 0.39774053382873537, 0.40050448074340822, 0.39554477937221527, 0.39212130529880523, 0.39204368515014648, 0.39779311509132387, 0.38655101714134216, 0.39594157586097717, 0.38887541799545289, 0.38831424665451048, 0.38531288533210756, 0.38424253740310671, 0.38829840807914734, 0.38703325028419494, 0.40068966255187988, 0.38208568549156191, 0.39109010787010196, 0.38651892471313476, 0.3897839092731476, 0.38860496916770937, 0.38606491293907164, 0.38440433561801912, 0.38102407839298247, 0.37779700300693514, 0.37751899487972257, 0.3747282587528229, 0.37767341747283933, 0.37418129825592039, 0.37091925506591794, 0.37315180923938751, 0.3719334022760391, 0.37319155437946322, 0.37221368193626403, 0.37345832915306093, 0.36968439795970914, 0.37341992316246031, 0.37311853110790255, 0.37287141385078432, 0.36904160733222963, 0.37406146187782285, 0.36855335335731504, 0.37270171070098879, 0.3713803517341614, 0.37045706081390378, 0.37274345159530642, 0.36957687082290647, 0.37064996833801267, 0.3717508162498474, 0.37081976046562193, 0.36843535776138303, 0.36288731615543368, 0.36588049583435056, 0.36378996896743776, 0.36382812058925629, 0.36678982412815092, 0.36296364469528197, 0.36510817921161653, 0.36539304552078244, 0.36412720127105713, 0.36377244889736177, 0.36374738216400149, 0.36611279706954958, 0.36291139986515047, 0.36060317635536193, 0.36169262943267821, 0.36290266883373262, 0.36518313457965851, 0.36468770132064821, 0.36446358466148376, 0.36456937868595124, 0.36155074987411501, 0.36485870306491852, 0.36474181427955626, 0.3627959972381592, 0.36242569904327393, 0.36250797901153564, 0.36226287913322447, 0.35985801517963412, 0.36194683382511139, 0.35974684779644012, 0.36053136396408081, 0.36085866508483888, 0.36227998089790342, 0.36023172502517697, 0.36054555590152743, 0.36011090030670168, 0.36102799899578092, 0.35934447116851809, 0.36208614535331723, 0.3614671002149582, 0.36102723615169524, 0.3619061961174011, 0.35929867107868196, 0.36220310752391816, 0.3601665175437927, 0.36048668420314789, 0.35873206570148469, 0.35975789957046511, 0.35729349224567414, 0.35853289160728452, 0.35832786824703217, 0.35825048413276672, 0.35876732463836669, 0.35945387461185457, 0.35950751628875732, 0.3593241233110428, 0.35902800953388214, 0.35911840028762815, 0.35883012256622315, 0.35818891458511354, 0.35951996755599974, 0.35775730798244476, 0.35975675122737882, 0.35813167717456817, 0.35737687849998473, 0.35780744795799257, 0.35826483614444732, 0.35907680552005766, 0.35931504664421082, 0.35895031719207765, 0.3575418622493744, 0.35773699712753299, 0.35728992719650271, 0.35786051893234255, 0.35750342681407926], 'loss': [2.0166485940641343, 1.4725236877402244, 1.327193694616458, 1.2513442674737196, 1.192255206700124, 1.1350885025002384, 1.1043468435720101, 1.0846757566007161, 1.0715064499138791, 1.0319501901422512, 1.0088959429024655, 1.0045496434174097, 0.97737904473838033, 0.96713641227572855, 0.95298925140632973, 0.93075760729541601, 0.92205577910921688, 0.90385013308171702, 0.90121120881835426, 0.87094317654367059, 0.85606550833794182, 0.84045499090892906, 0.83095227757974965, 0.82470648155634629, 0.80784359860198407, 0.79993048567476666, 0.79878297248328767, 0.78318300855668421, 0.78076921927840959, 0.77359192410378186, 0.76943788340660024, 0.75996362987067934, 0.75597689749913832, 0.75634604866473309, 0.74140703431451627, 0.74102380237863885, 0.73651695737174816, 0.73881262018774652, 0.73445661511176674, 0.73324825295516904, 0.73228614621717725, 0.72384165258487143, 0.72327002811248309, 0.72079009655235593, 0.71845638046365612, 0.71613021884776862, 0.71537730716741998, 0.70756269659303894, 0.71053527496853131, 0.71141997476418817, 0.70285525732340537, 0.6999202646962871, 0.70047574264880941, 0.70018568752589883, 0.69786797531843947, 0.69196631977191336, 0.69364267404866509, 0.68960870895624393, 0.68625769716139817, 0.69018760415223923, 0.68474322065529181, 0.68768537256784223, 0.67980022079714375, 0.69341608209796624, 0.68413452577009615, 0.67162615219522526, 0.68228038916956513, 0.6804027720296234, 0.67156060666735062, 0.67953159813319486, 0.67758786609539623, 0.66720711095790952, 0.67259439178717073, 0.67563135301910004, 0.67398017109968722, 0.61968332693679407, 0.6078427105635299, 0.60535186085945525, 0.5995611656083103, 0.59325337966972236, 0.59434201073332937, 0.59287842489566256, 0.58597496967496188, 0.58650730313398902, 0.58335404112389555, 0.58390628331587946, 0.58428679489347468, 0.57826011726632121, 0.57897383839915029, 0.57591347663609849, 0.57084631638760974, 0.57257805526122707, 0.56586572314670236, 0.57272294699132231, 0.57026470464737633, 0.57218248171684072, 0.56462705567928939, 0.56681000949480598, 0.56477609176282018, 0.56529941995211963, 0.53918747971522185, 0.53590738544097316, 0.53878150685765935, 0.53032570713670424, 0.52846657082481496, 0.52771355970417566, 0.52534188436781093, 0.52494716845072731, 0.52339863002701836, 0.51810834086858304, 0.52589434940384683, 0.51669386252019889, 0.51503143294866471, 0.51391879141923513, 0.51597362712747519, 0.51590963807058599, 0.51337228965445669, 0.51422561848052695, 0.51025394162651361, 0.51095877519020672, 0.51636703676073381, 0.51158077105497701, 0.50306804380550085, 0.50450867062959914, 0.50647027823085888, 0.48801234731307397, 0.48370071283769517, 0.48369157023918935, 0.47767504664974175, 0.47666907365940914, 0.47867227410658814, 0.4778297549262428, 0.47859780333116342, 0.47471903087161926, 0.47381768725336765, 0.47416479036063769, 0.46802562626417277, 0.47514193996734483, 0.46900551803684937, 0.47110886332083868, 0.4682437591457887, 0.47102067814423487, 0.47143828271289723, 0.46462757430860607, 0.47276027310839208, 0.4638275539263701, 0.46919249752067321, 0.46426906919509997, 0.46353153599964564, 0.46556875270146592, 0.45047294125008752, 0.45006749729315437, 0.44878187766041444, 0.44936261838316954, 0.44414560596148173, 0.44486967037991526, 0.44522298246689013, 0.44785339871698132, 0.44881628516896382, 0.4448233141534253, 0.44077981870908006, 0.44606251937669172, 0.44980939660955871, 0.43830215716973331, 0.44040596263103937, 0.44058088216790797, 0.44288532076737819, 0.44344289278157978, 0.44465119498526395, 0.43703530886099051, 0.44000051731893325, 0.44303362911467298, 0.44053381713950407, 0.4362686179387264, 0.44306109727557208, 0.43400088147850624, 0.43332508993454466, 0.43139939677994488, 0.42824550289965457, 0.43595687013332401, 0.42995532843253531, 0.43234666150349838, 0.42342169486182424, 0.43666583225059324, 0.42344306466248238, 0.42927067417306014, 0.42994173516829809, 0.43002706759429238, 0.42729344929640112, 0.42689080771093457, 0.42684478128916337, 0.42437831222934452, 0.43013988718008384, 0.42685189063460316, 0.41896513425695769, 0.42134149541333199, 0.42986626916384957, 0.42756350127634118, 0.42655745576589538, 0.42503515027408462, 0.42679809281744863, 0.42283876787633284, 0.42280076451790638, 0.42094432129705417, 0.42610666064983538, 0.42547921870829741, 0.42464306690503972, 0.42264161273994849, 0.42019502711601747, 0.41947018130704539, 0.42585618794442448, 0.42212272137403489, 0.4214269043680331, 0.41916012213780329, 0.42265424997637507, 0.41653563114248254, 0.42146089615674975, 0.41856856098686002, 0.4205328905781493, 0.42137755473187077, 0.41479811446788983, 0.4223180018561577, 0.41720250000317277, 0.4185157396451456, 0.42136899232481934, 0.41812291340186047, 0.41403245239407732, 0.41371826631332254, 0.41312235555550897, 0.42227507206874015, 0.4156576637932331, 0.42134839021242582, 0.41391789115771754, 0.41585120867566111, 0.41646358858872928, 0.41778584667934143, 0.41830000594640387, 0.42187735018609313, 0.41230434390608989, 0.4156001029123203, 0.41852222927533783, 0.41855531624008302, 0.41811572782573386, 0.41296054672881277, 0.41692123726311503, 0.42286444702781772, 0.41517051391812676, 0.4191458753125194, 0.41148430057343721, 0.41629224755825139], 'val_acc': [0.42420000000000002, 0.6552, 0.67859999999999998, 0.69540000000000002, 0.7016, 0.72609999999999997, 0.74129999999999996, 0.74650000000000005, 0.76670000000000005, 0.74890000000000001, 0.77349999999999997, 0.77890000000000004, 0.77549999999999997, 0.77610000000000001, 0.78290000000000004, 0.79069999999999996, 0.79290000000000005, 0.79400000000000004, 0.79210000000000003, 0.80600000000000005, 0.81079999999999997, 0.80089999999999995, 0.81879999999999997, 0.81859999999999999, 0.81120000000000003, 0.83230000000000004, 0.81789999999999996, 0.83040000000000003, 0.81720000000000004, 0.82110000000000005, 0.82379999999999998, 0.83830000000000005, 0.82079999999999997, 0.83660000000000001, 0.84089999999999998, 0.82869999999999999, 0.8236, 0.8206, 0.82509999999999994, 0.84030000000000005, 0.82199999999999995, 0.83960000000000001, 0.82750000000000001, 0.83860000000000001, 0.83630000000000004, 0.83950000000000002, 0.85450000000000004, 0.83279999999999998, 0.84309999999999996, 0.84599999999999997, 0.83809999999999996, 0.85199999999999998, 0.85260000000000002, 0.84709999999999996, 0.84330000000000005, 0.84550000000000003, 0.84389999999999998, 0.8518, 0.8508, 0.84840000000000004, 0.84319999999999995, 0.83550000000000002, 0.8488, 0.85289999999999999, 0.84819999999999995, 0.84640000000000004, 0.85019999999999996, 0.84699999999999998, 0.8538, 0.85499999999999998, 0.85509999999999997, 0.84609999999999996, 0.84219999999999995, 0.85699999999999998, 0.85640000000000005, 0.86890000000000001, 0.86980000000000002, 0.875, 0.86919999999999997, 0.87219999999999998, 0.86960000000000004, 0.87429999999999997, 0.87460000000000004, 0.87690000000000001, 0.87270000000000003, 0.87270000000000003, 0.86960000000000004, 0.86880000000000002, 0.86770000000000003, 0.87, 0.87250000000000005, 0.87109999999999999, 0.872, 0.87470000000000003, 0.87319999999999998, 0.87890000000000001, 0.87560000000000004, 0.88029999999999997, 0.87370000000000003, 0.87829999999999997, 0.88, 0.88180000000000003, 0.88390000000000002, 0.88600000000000001, 0.88139999999999996, 0.88480000000000003, 0.88439999999999996, 0.88280000000000003, 0.88759999999999994, 0.87609999999999999, 0.88619999999999999, 0.88260000000000005, 0.88460000000000005, 0.88429999999999997, 0.88759999999999994, 0.88180000000000003, 0.88870000000000005, 0.88629999999999998, 0.88980000000000004, 0.88580000000000003, 0.88780000000000003, 0.8901, 0.8841, 0.88460000000000005, 0.89349999999999996, 0.89429999999999998, 0.89139999999999997, 0.89280000000000004, 0.89649999999999996, 0.89249999999999996, 0.89229999999999998, 0.89590000000000003, 0.89359999999999995, 0.89449999999999996, 0.89280000000000004, 0.89580000000000004, 0.89800000000000002, 0.89449999999999996, 0.89549999999999996, 0.89180000000000004, 0.89859999999999995, 0.89390000000000003, 0.89359999999999995, 0.89280000000000004, 0.89280000000000004, 0.89449999999999996, 0.89559999999999995, 0.8962, 0.89449999999999996, 0.89700000000000002, 0.89829999999999999, 0.8972, 0.9002, 0.89700000000000002, 0.89990000000000003, 0.89849999999999997, 0.89810000000000001, 0.89959999999999996, 0.89970000000000006, 0.89929999999999999, 0.8972, 0.89829999999999999, 0.89770000000000005, 0.90090000000000003, 0.89739999999999998, 0.89859999999999995, 0.89990000000000003, 0.90010000000000001, 0.89929999999999999, 0.89959999999999996, 0.89829999999999999, 0.89949999999999997, 0.9002, 0.89870000000000005, 0.90180000000000005, 0.90280000000000005, 0.90180000000000005, 0.90129999999999999, 0.90159999999999996, 0.89980000000000004, 0.90080000000000005, 0.90159999999999996, 0.90169999999999995, 0.90149999999999997, 0.90159999999999996, 0.90059999999999996, 0.90029999999999999, 0.90200000000000002, 0.9022, 0.90249999999999997, 0.90080000000000005, 0.90059999999999996, 0.90159999999999996, 0.9022, 0.90100000000000002, 0.9032, 0.90069999999999995, 0.90190000000000003, 0.90200000000000002, 0.90029999999999999, 0.90210000000000001, 0.90310000000000001, 0.90210000000000001, 0.90290000000000004, 0.90300000000000002, 0.90300000000000002, 0.90339999999999998, 0.90290000000000004, 0.90280000000000005, 0.90259999999999996, 0.90429999999999999, 0.90349999999999997, 0.90410000000000001, 0.90369999999999995, 0.90280000000000005, 0.90200000000000002, 0.90169999999999995, 0.90369999999999995, 0.90259999999999996, 0.90190000000000003, 0.90329999999999999, 0.90310000000000001, 0.90300000000000002, 0.90469999999999995, 0.90349999999999997, 0.90339999999999998, 0.90210000000000001, 0.90349999999999997, 0.9032, 0.90359999999999996, 0.90200000000000002, 0.9032, 0.90300000000000002, 0.90329999999999999, 0.90280000000000005, 0.90229999999999999, 0.9032, 0.90229999999999999, 0.90290000000000004, 0.90359999999999996, 0.90169999999999995, 0.90380000000000005, 0.90349999999999997, 0.9032, 0.90359999999999996, 0.90369999999999995, 0.90329999999999999, 0.90329999999999999, 0.90400000000000003, 0.90410000000000001], 'acc': [0.38793310875364095, 0.52730991334000343, 0.57892204042348416, 0.61124478663471138, 0.63296037855001452, 0.65148780875225876, 0.66478184151427655, 0.67338386270760497, 0.67931905674712567, 0.68822184794982499, 0.69766602504318409, 0.70141562401655588, 0.70777189605389801, 0.71164180299659785, 0.71715591915303178, 0.72351219120949628, 0.72493583577773801, 0.73026948987500651, 0.72970805263381611, 0.73726740459390738, 0.73859079244773973, 0.74410490858505118, 0.74737327562374378, 0.74653111966634589, 0.75531360286147231, 0.75569457808174234, 0.75798042991966785, 0.76162977221687522, 0.76287295480244122, 0.76594080846968238, 0.76688322108412232, 0.76967035614989066, 0.77083333329508841, 0.77197625918536761, 0.77700914340712224, 0.7778512993071528, 0.77767083734359965, 0.77989653516817747, 0.77944711538461542, 0.77938342971072871, 0.77935514920102811, 0.78254331089496465, 0.78498957332679009, 0.78474895729252192, 0.786613731132626, 0.78611244790478318, 0.78816105769230771, 0.78986030826602294, 0.78771655434096588, 0.78932291666666665, 0.79078355812459855, 0.79294995187680462, 0.79290865384615383, 0.79264918186743361, 0.79313182403969318, 0.79525240384615381, 0.79650369299935775, 0.7976820661274332, 0.79784247675957798, 0.798457532051282, 0.80015655109801198, 0.7961180622202102, 0.80008822587731943, 0.79838386268848249, 0.80002807182572688, 0.80145171639396851, 0.79848411930727969, 0.80189284572319841, 0.80465992938106856, 0.803276387533011, 0.80072115384615383, 0.80497350678201973, 0.80355710621097354, 0.80149181905678535, 0.80380608974358969, 0.82303709057790775, 0.8227662816428587, 0.82670272435897441, 0.82641562395918855, 0.82947976880526808, 0.82749839587436491, 0.82914260506897663, 0.83198989409072532, 0.83129006410256412, 0.8313262684649968, 0.83225056141815701, 0.82960378569136994, 0.83164902149502729, 0.83106753290330593, 0.83299246073763533, 0.83309271735643242, 0.83459656717381803, 0.83523820985537678, 0.83297240940622697, 0.83527831249907114, 0.83235176282051282, 0.8349991972314752, 0.83475560897435896, 0.83700626208067097, 0.83431584853410035, 0.84221607316637936, 0.84326923076923077, 0.84331889633647439, 0.84613840723160216, 0.84602582609573163, 0.84628649346140816, 0.84644690411267542, 0.8483317291885758, 0.84730911134411446, 0.84847756410256414, 0.84491409762363523, 0.84931424450407744, 0.85031681103625278, 0.84975537379506239, 0.84965511709977537, 0.84746952199538161, 0.85312399747167444, 0.85061758098825646, 0.84933429575899611, 0.85022035256410255, 0.84957048811817593, 0.85042067307692304, 0.8533839113871563, 0.853145032051282, 0.85228002570957118, 0.85731169871794877, 0.85960581249814039, 0.86011618589743588, 0.86018786127167635, 0.86140519732422349, 0.859375, 0.86154555658671506, 0.8604627847480284, 0.86152550529355154, 0.86152550529355154, 0.86393166506256014, 0.86335902374450713, 0.86146535129932622, 0.86379130572357887, 0.86174606990079905, 0.86242781524517464, 0.86264022435897436, 0.86216714791774296, 0.86372029542080775, 0.86176612123220753, 0.86414262820512822, 0.86427253773474788, 0.86426220297354039, 0.86395171641309099, 0.8645232371794872, 0.86753371876635155, 0.86840945512820511, 0.86984184326923419, 0.86966634580711921, 0.87021233974358969, 0.86958614055797534, 0.86892444654500822, 0.86772136673699363, 0.86849710980744876, 0.87108999037536095, 0.8712740384615385, 0.86754090473519563, 0.86883831085420682, 0.87083333333333335, 0.87176862560025992, 0.8719722489764532, 0.86885016025641026, 0.87269409686891097, 0.86960619185113897, 0.87225296755880355, 0.8704640333975594, 0.87185194098800278, 0.86990696182226501, 0.87475961538461533, 0.86992701319191823, 0.8735749839626219, 0.8742788461538461, 0.8729948668591595, 0.87453881941585154, 0.87285244055259814, 0.8775264677765815, 0.8723357371794872, 0.87570247270391777, 0.87347609883208366, 0.87686477385922657, 0.875501283323455, 0.87295673076923075, 0.87299293509338172, 0.87556089743589749, 0.8752807186205952, 0.87650384981738549, 0.87638487470172965, 0.87636217948717954, 0.87628328521013799, 0.87876965028565779, 0.87836862367661217, 0.87451830441245837, 0.8737167148089845, 0.87588141025641031, 0.87698508179030976, 0.87550176621708409, 0.87706528717331067, 0.87530048076923073, 0.87640359323683326, 0.87596246386935861, 0.87628452153487624, 0.87369666345845365, 0.87588225854372492, 0.87832532051282053, 0.87678628777110124, 0.87550128328521015, 0.87692307692307692, 0.87674614639717108, 0.8781850961538461, 0.87658405514301918, 0.87768687838960391, 0.87632338787295472, 0.87716554379210776, 0.87568240209351611, 0.87552133459749615, 0.87954727564102564, 0.87504014135478936, 0.87796759708668892, 0.87664420915636676, 0.87680461980763402, 0.87694310897435901, 0.88041385946114703, 0.88088070010250785, 0.87826836700044764, 0.87762419871794872, 0.87965639053282252, 0.87694310897435901, 0.87852903430875695, 0.87870949629143258, 0.87955603727026477, 0.87578200188668298, 0.8766025641025641, 0.87628328524838284, 0.88009303821597984, 0.87810795638742534, 0.87686477380185923, 0.87768687838960391, 0.87848893170330744, 0.88029355147269661, 0.87806785368636364, 0.87558148862996621, 0.88033365411639097, 0.8773259544624975, 0.88009794476557479, 0.87940705128205132]}\n"
     ]
    }
   ],
   "source": [
    "# Create model and get ready - reproducible cell\n",
    "act, act_name = leaky, \"leaky\"\n",
    "model = create(act)\n",
    "opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "\n",
    "# Training - only for 125 eppochs\n",
    "batch_size = 128\n",
    "epochs=250\n",
    "# lr scheduler\n",
    "lr_1 = keras.callbacks.LearningRateScheduler(schedule)\n",
    "\n",
    "# Fit the model\n",
    "his = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,verbose=1,validation_data=(x_test,y_test), callbacks=[lr_1])\n",
    "# Save the model\n",
    "model.save('models/leaky_relu/leaky_at_3.h5')\n",
    "# Display logs\n",
    "print(his.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_test):\n",
    "    y_hat = np.argmax(y_pred, axis=1)\n",
    "    y = np.argmax(y_test, axis=1)\n",
    "\n",
    "    good = np.sum(np.equal(y, y_hat))\n",
    "    return float(good/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_25 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "0.9138\n"
     ]
    }
   ],
   "source": [
    "import keras.regularizers as regularizers\n",
    "ensembler = 0\n",
    "for i in range(3):\n",
    "    if i == 0:\n",
    "        model = create(leaky)\n",
    "        model.load_weights('models/leaky_relu/leaky_at_'+str(i+1)+'.h5')\n",
    "        model.save('models/leaky_relu/leaky_at_'+str(i+1)+'.h5')\n",
    "    else:\n",
    "        model = load_model('models/leaky_relu/leaky_at_'+str(i+1)+'.h5', custom_objects={\"leaky\": leaky})\n",
    "    ensembler += model.predict_proba(x_test)\n",
    "    \n",
    "print(accuracy(ensembler, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
