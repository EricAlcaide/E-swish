{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Performing a large-scale experiment on cifar10 \"\"\"\n",
    "\n",
    "import keras\n",
    "import logging\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import cifar10\n",
    "import keras.regularizers as regularizers\n",
    "from keras.models import load_model\n",
    "\n",
    "# Record settings\n",
    "LOG_FORMAT = \"%(levelname)s %(asctime)s - %(message)s\"\n",
    "logging.basicConfig(filename=\"swish_first_exp_log.txt\",format = LOG_FORMAT, level = logging.DEBUG, filemode = \"a\")\n",
    "logs = logging.getLogger()\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#z-score\n",
    "mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "std = np.std(x_train,axis=(0,1,2,3))\n",
    "x_train = (x_train-mean)/(std+1e-7)\n",
    "x_test = (x_test-mean)/(std+1e-7)\n",
    "\n",
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train,num_classes)\n",
    "y_test = np_utils.to_categorical(y_test,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Swish activation function\n",
    "# x*sigmoid(x)\n",
    "def swish(x):\n",
    "    return x*K.sigmoid(x)\n",
    "\n",
    "# Custom activation function 1\n",
    "# mix between relu and positive part of swish mirrored across x=1\n",
    "def e_swish_1(x):\n",
    "    return K.maximum(0.0, x*(2-K.sigmoid(x)))\n",
    "\n",
    "# Custom activation function 2\n",
    "# positive part of swish mirrored across x=1\n",
    "def e_swish_2(x):\n",
    "    return K.maximum(x*K.sigmoid(x), x*(2-K.sigmoid(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create(act):\n",
    "    baseMapNum = 32\n",
    "    weight_decay = 1e-4\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(2*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(2*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(4*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(4*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False\n",
    "    )\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def schedule(x):\n",
    "    if x < 75: \n",
    "        return 0.001\n",
    "    elif x < 100:\n",
    "        return 0.0005\n",
    "    elif x < 125:\n",
    "        return 0.0003\n",
    "    elif x<150:\n",
    "        return 0.00015\n",
    "    elif x<175:\n",
    "        return 0.000075\n",
    "    elif x<200: \n",
    "        return 0.000035\n",
    "    elif x<225:\n",
    "        return 0.000017\n",
    "    else:\n",
    "        return 0.000012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Epoch 1/250\n",
      "390/390 [==============================] - 159s 407ms/step - loss: 1.9607 - acc: 0.4037 - val_loss: 1.5353 - val_acc: 0.4661\n",
      "Epoch 2/250\n",
      "390/390 [==============================] - 152s 389ms/step - loss: 1.4286 - acc: 0.5464 - val_loss: 1.2511 - val_acc: 0.6299\n",
      "Epoch 3/250\n",
      "390/390 [==============================] - 152s 389ms/step - loss: 1.2857 - acc: 0.5990 - val_loss: 1.0288 - val_acc: 0.6810\n",
      "Epoch 4/250\n",
      "390/390 [==============================] - 151s 388ms/step - loss: 1.2402 - acc: 0.6301 - val_loss: 0.9638 - val_acc: 0.7194\n",
      "Epoch 5/250\n",
      "390/390 [==============================] - 152s 389ms/step - loss: 1.2152 - acc: 0.6521 - val_loss: 0.9194 - val_acc: 0.7308\n",
      "Epoch 6/250\n",
      "390/390 [==============================] - 151s 386ms/step - loss: 1.0922 - acc: 0.6738 - val_loss: 0.7698 - val_acc: 0.7634\n",
      "Epoch 7/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 1.0202 - acc: 0.6946 - val_loss: 0.8365 - val_acc: 0.7496\n",
      "Epoch 8/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.9961 - acc: 0.7088 - val_loss: 0.7827 - val_acc: 0.7660\n",
      "Epoch 9/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.9665 - acc: 0.7173 - val_loss: 0.7273 - val_acc: 0.7776\n",
      "Epoch 10/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.9594 - acc: 0.7254 - val_loss: 0.7126 - val_acc: 0.7974\n",
      "Epoch 11/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.9199 - acc: 0.7341 - val_loss: 0.7007 - val_acc: 0.7997\n",
      "Epoch 12/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.9265 - acc: 0.7388 - val_loss: 0.6689 - val_acc: 0.8065\n",
      "Epoch 13/250\n",
      "390/390 [==============================] - 151s 386ms/step - loss: 0.8972 - acc: 0.7449 - val_loss: 0.6445 - val_acc: 0.8118\n",
      "Epoch 14/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.9353 - acc: 0.7466 - val_loss: 0.7344 - val_acc: 0.8036\n",
      "Epoch 15/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.9597 - acc: 0.7479 - val_loss: 0.6855 - val_acc: 0.8020\n",
      "Epoch 16/250\n",
      "390/390 [==============================] - 151s 386ms/step - loss: 0.8871 - acc: 0.7552 - val_loss: 0.7053 - val_acc: 0.8134\n",
      "Epoch 17/250\n",
      "390/390 [==============================] - 151s 386ms/step - loss: 0.8633 - acc: 0.7630 - val_loss: 0.6666 - val_acc: 0.8233\n",
      "Epoch 18/250\n",
      "390/390 [==============================] - 151s 386ms/step - loss: 0.8575 - acc: 0.7643 - val_loss: 0.6169 - val_acc: 0.8243\n",
      "Epoch 19/250\n",
      "390/390 [==============================] - 151s 386ms/step - loss: 0.8306 - acc: 0.7709 - val_loss: 0.6180 - val_acc: 0.8285\n",
      "Epoch 20/250\n",
      "390/390 [==============================] - 151s 386ms/step - loss: 0.8410 - acc: 0.7733 - val_loss: 0.6410 - val_acc: 0.8217\n",
      "Epoch 21/250\n",
      "390/390 [==============================] - 151s 386ms/step - loss: 0.8234 - acc: 0.7738 - val_loss: 0.6827 - val_acc: 0.8170\n",
      "Epoch 22/250\n",
      "390/390 [==============================] - 151s 386ms/step - loss: 0.8191 - acc: 0.7768 - val_loss: 0.6031 - val_acc: 0.8374\n",
      "Epoch 23/250\n",
      "390/390 [==============================] - 151s 386ms/step - loss: 0.7964 - acc: 0.7798 - val_loss: 0.5974 - val_acc: 0.8337\n",
      "Epoch 24/250\n",
      "390/390 [==============================] - 151s 386ms/step - loss: 0.7923 - acc: 0.7820 - val_loss: 0.6062 - val_acc: 0.8327\n",
      "Epoch 25/250\n",
      "390/390 [==============================] - 151s 386ms/step - loss: 0.8048 - acc: 0.7842 - val_loss: 0.5988 - val_acc: 0.8347\n",
      "Epoch 26/250\n",
      "390/390 [==============================] - 151s 386ms/step - loss: 0.7899 - acc: 0.7871 - val_loss: 0.5919 - val_acc: 0.8415\n",
      "Epoch 27/250\n",
      "390/390 [==============================] - 151s 386ms/step - loss: 0.7810 - acc: 0.7915 - val_loss: 0.6094 - val_acc: 0.8382\n",
      "Epoch 28/250\n",
      "390/390 [==============================] - 151s 386ms/step - loss: 0.7917 - acc: 0.7895 - val_loss: 0.5819 - val_acc: 0.8405\n",
      "Epoch 29/250\n",
      "390/390 [==============================] - 151s 386ms/step - loss: 0.8231 - acc: 0.7864 - val_loss: 0.6292 - val_acc: 0.8307\n",
      "Epoch 30/250\n",
      "390/390 [==============================] - 151s 386ms/step - loss: 0.7670 - acc: 0.7948 - val_loss: 0.6131 - val_acc: 0.8364\n",
      "Epoch 31/250\n",
      "390/390 [==============================] - 151s 386ms/step - loss: 0.7697 - acc: 0.7936 - val_loss: 0.5832 - val_acc: 0.8498\n",
      "Epoch 32/250\n",
      "390/390 [==============================] - 151s 386ms/step - loss: 0.7845 - acc: 0.7940 - val_loss: 0.6014 - val_acc: 0.8391\n",
      "Epoch 33/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7679 - acc: 0.7970 - val_loss: 0.6067 - val_acc: 0.8449\n",
      "Epoch 34/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7566 - acc: 0.7991 - val_loss: 0.5702 - val_acc: 0.8479\n",
      "Epoch 35/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7485 - acc: 0.8010 - val_loss: 0.5778 - val_acc: 0.8500\n",
      "Epoch 36/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7523 - acc: 0.8006 - val_loss: 0.5981 - val_acc: 0.8408\n",
      "Epoch 37/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7618 - acc: 0.7996 - val_loss: 0.5886 - val_acc: 0.8446\n",
      "Epoch 38/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7589 - acc: 0.8017 - val_loss: 0.5585 - val_acc: 0.8570\n",
      "Epoch 39/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7456 - acc: 0.8043 - val_loss: 0.5568 - val_acc: 0.8569\n",
      "Epoch 40/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7448 - acc: 0.8025 - val_loss: 0.5420 - val_acc: 0.8596\n",
      "Epoch 41/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7456 - acc: 0.8055 - val_loss: 0.5875 - val_acc: 0.8479\n",
      "Epoch 42/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7488 - acc: 0.8046 - val_loss: 0.5746 - val_acc: 0.8532\n",
      "Epoch 43/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7332 - acc: 0.8081 - val_loss: 0.5869 - val_acc: 0.8570\n",
      "Epoch 44/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7271 - acc: 0.8080 - val_loss: 0.5485 - val_acc: 0.8590\n",
      "Epoch 45/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7278 - acc: 0.8078 - val_loss: 0.5750 - val_acc: 0.8556\n",
      "Epoch 46/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7360 - acc: 0.8110 - val_loss: 0.5665 - val_acc: 0.8594\n",
      "Epoch 47/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7253 - acc: 0.8104 - val_loss: 0.6102 - val_acc: 0.8375\n",
      "Epoch 48/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7254 - acc: 0.8111 - val_loss: 0.5536 - val_acc: 0.8617\n",
      "Epoch 49/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7207 - acc: 0.8137 - val_loss: 0.5481 - val_acc: 0.8643\n",
      "Epoch 50/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7241 - acc: 0.8126 - val_loss: 0.5328 - val_acc: 0.8686\n",
      "Epoch 51/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7254 - acc: 0.8126 - val_loss: 0.5562 - val_acc: 0.8618\n",
      "Epoch 52/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7249 - acc: 0.8146 - val_loss: 0.5849 - val_acc: 0.8508\n",
      "Epoch 53/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7328 - acc: 0.8156 - val_loss: 0.5818 - val_acc: 0.8559\n",
      "Epoch 54/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7237 - acc: 0.8150 - val_loss: 0.5505 - val_acc: 0.8599\n",
      "Epoch 55/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7107 - acc: 0.8140 - val_loss: 0.5561 - val_acc: 0.8610\n",
      "Epoch 56/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7144 - acc: 0.8167 - val_loss: 0.5647 - val_acc: 0.8596\n",
      "Epoch 57/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7080 - acc: 0.8160 - val_loss: 0.5530 - val_acc: 0.8616\n",
      "Epoch 58/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.7028 - acc: 0.8171 - val_loss: 0.5376 - val_acc: 0.8632\n",
      "Epoch 59/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7076 - acc: 0.8183 - val_loss: 0.5867 - val_acc: 0.8532\n",
      "Epoch 60/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7047 - acc: 0.8188 - val_loss: 0.5577 - val_acc: 0.8658\n",
      "Epoch 61/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.6937 - acc: 0.8200 - val_loss: 0.5476 - val_acc: 0.8625\n",
      "Epoch 62/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.6877 - acc: 0.8191 - val_loss: 0.5789 - val_acc: 0.8560\n",
      "Epoch 63/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7072 - acc: 0.8188 - val_loss: 0.5463 - val_acc: 0.8636\n",
      "Epoch 64/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.6968 - acc: 0.8208 - val_loss: 0.5454 - val_acc: 0.8621\n",
      "Epoch 65/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.6923 - acc: 0.8237 - val_loss: 0.5574 - val_acc: 0.8653\n",
      "Epoch 66/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.6869 - acc: 0.8205 - val_loss: 0.5408 - val_acc: 0.8688\n",
      "Epoch 67/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.6811 - acc: 0.8240 - val_loss: 0.5454 - val_acc: 0.8651\n",
      "Epoch 68/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.6895 - acc: 0.8254 - val_loss: 0.5717 - val_acc: 0.8680\n",
      "Epoch 69/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.6890 - acc: 0.8249 - val_loss: 0.5490 - val_acc: 0.8592\n",
      "Epoch 70/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.6904 - acc: 0.8218 - val_loss: 0.5270 - val_acc: 0.8726\n",
      "Epoch 71/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.6811 - acc: 0.8251 - val_loss: 0.5497 - val_acc: 0.8599\n",
      "Epoch 72/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.6903 - acc: 0.8247 - val_loss: 0.5429 - val_acc: 0.8668\n",
      "Epoch 73/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.6797 - acc: 0.8262 - val_loss: 0.5302 - val_acc: 0.8659\n",
      "Epoch 74/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.6852 - acc: 0.8243 - val_loss: 0.5682 - val_acc: 0.8639\n",
      "Epoch 75/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.6758 - acc: 0.8283 - val_loss: 0.5234 - val_acc: 0.8661\n",
      "Epoch 76/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.6297 - acc: 0.8402 - val_loss: 0.4903 - val_acc: 0.8821\n",
      "Epoch 77/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.6190 - acc: 0.8444 - val_loss: 0.4822 - val_acc: 0.8836\n",
      "Epoch 78/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.6179 - acc: 0.8450 - val_loss: 0.4820 - val_acc: 0.8801\n",
      "Epoch 79/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.6061 - acc: 0.8459 - val_loss: 0.4963 - val_acc: 0.8827\n",
      "Epoch 80/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.6175 - acc: 0.8454 - val_loss: 0.4798 - val_acc: 0.8854\n",
      "Epoch 81/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5960 - acc: 0.8480 - val_loss: 0.4919 - val_acc: 0.8817\n",
      "Epoch 82/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.5970 - acc: 0.8487 - val_loss: 0.4873 - val_acc: 0.8844\n",
      "Epoch 83/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5971 - acc: 0.8504 - val_loss: 0.4662 - val_acc: 0.8886\n",
      "Epoch 84/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5972 - acc: 0.8472 - val_loss: 0.4676 - val_acc: 0.8880\n",
      "Epoch 85/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.5922 - acc: 0.8494 - val_loss: 0.4811 - val_acc: 0.8821\n",
      "Epoch 86/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5934 - acc: 0.8510 - val_loss: 0.4633 - val_acc: 0.8883\n",
      "Epoch 87/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5857 - acc: 0.8497 - val_loss: 0.4836 - val_acc: 0.8835\n",
      "Epoch 88/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5764 - acc: 0.8518 - val_loss: 0.4701 - val_acc: 0.8838\n",
      "Epoch 89/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5773 - acc: 0.8523 - val_loss: 0.4629 - val_acc: 0.8896\n",
      "Epoch 90/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5782 - acc: 0.8524 - val_loss: 0.4949 - val_acc: 0.8811\n",
      "Epoch 91/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5654 - acc: 0.8528 - val_loss: 0.4598 - val_acc: 0.8912\n",
      "Epoch 92/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5776 - acc: 0.8518 - val_loss: 0.4753 - val_acc: 0.8864\n",
      "Epoch 93/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.5795 - acc: 0.8518 - val_loss: 0.4751 - val_acc: 0.8819\n",
      "Epoch 94/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5717 - acc: 0.8523 - val_loss: 0.4918 - val_acc: 0.8786\n",
      "Epoch 95/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5834 - acc: 0.8516 - val_loss: 0.4560 - val_acc: 0.8905\n",
      "Epoch 96/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5774 - acc: 0.8516 - val_loss: 0.4567 - val_acc: 0.8879\n",
      "Epoch 97/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5705 - acc: 0.8517 - val_loss: 0.4828 - val_acc: 0.8789\n",
      "Epoch 98/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5698 - acc: 0.8552 - val_loss: 0.4745 - val_acc: 0.8835\n",
      "Epoch 99/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5774 - acc: 0.8511 - val_loss: 0.4811 - val_acc: 0.8794\n",
      "Epoch 100/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5666 - acc: 0.8533 - val_loss: 0.4663 - val_acc: 0.8847\n",
      "Epoch 101/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.5290 - acc: 0.8651 - val_loss: 0.4497 - val_acc: 0.8922\n",
      "Epoch 102/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5405 - acc: 0.8629 - val_loss: 0.4439 - val_acc: 0.8902\n",
      "Epoch 103/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5401 - acc: 0.8603 - val_loss: 0.4558 - val_acc: 0.8895\n",
      "Epoch 104/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.5221 - acc: 0.8660 - val_loss: 0.4442 - val_acc: 0.8907\n",
      "Epoch 105/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5362 - acc: 0.8621 - val_loss: 0.4456 - val_acc: 0.8923\n",
      "Epoch 106/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.5147 - acc: 0.8680 - val_loss: 0.4432 - val_acc: 0.8908\n",
      "Epoch 107/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5202 - acc: 0.8640 - val_loss: 0.4417 - val_acc: 0.8925\n",
      "Epoch 108/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5253 - acc: 0.8668 - val_loss: 0.4458 - val_acc: 0.8887\n",
      "Epoch 109/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5198 - acc: 0.8655 - val_loss: 0.4422 - val_acc: 0.8916\n",
      "Epoch 110/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5193 - acc: 0.8652 - val_loss: 0.4454 - val_acc: 0.8916\n",
      "Epoch 111/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.5175 - acc: 0.8681 - val_loss: 0.4413 - val_acc: 0.8950\n",
      "Epoch 112/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5132 - acc: 0.8660 - val_loss: 0.4370 - val_acc: 0.8941\n",
      "Epoch 113/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5137 - acc: 0.8674 - val_loss: 0.4386 - val_acc: 0.8906\n",
      "Epoch 114/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.5087 - acc: 0.8701 - val_loss: 0.4414 - val_acc: 0.8945\n",
      "Epoch 115/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5140 - acc: 0.8671 - val_loss: 0.4354 - val_acc: 0.8931\n",
      "Epoch 116/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4996 - acc: 0.8704 - val_loss: 0.4404 - val_acc: 0.8914\n",
      "Epoch 117/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5068 - acc: 0.8687 - val_loss: 0.4207 - val_acc: 0.8965\n",
      "Epoch 118/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5097 - acc: 0.8673 - val_loss: 0.4304 - val_acc: 0.8949\n",
      "Epoch 119/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5007 - acc: 0.8673 - val_loss: 0.4279 - val_acc: 0.8951\n",
      "Epoch 120/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5048 - acc: 0.8679 - val_loss: 0.4372 - val_acc: 0.8927\n",
      "Epoch 121/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4933 - acc: 0.8713 - val_loss: 0.4266 - val_acc: 0.8937\n",
      "Epoch 122/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5012 - acc: 0.8699 - val_loss: 0.4199 - val_acc: 0.8949\n",
      "Epoch 123/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4957 - acc: 0.8701 - val_loss: 0.4270 - val_acc: 0.8952\n",
      "Epoch 124/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5054 - acc: 0.8707 - val_loss: 0.4258 - val_acc: 0.8947\n",
      "Epoch 125/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4958 - acc: 0.8708 - val_loss: 0.4236 - val_acc: 0.8952\n",
      "Epoch 126/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4919 - acc: 0.8712 - val_loss: 0.4180 - val_acc: 0.9004\n",
      "Epoch 127/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4887 - acc: 0.8752 - val_loss: 0.4157 - val_acc: 0.8971\n",
      "Epoch 128/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4743 - acc: 0.8754 - val_loss: 0.4138 - val_acc: 0.8989\n",
      "Epoch 129/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4728 - acc: 0.8772 - val_loss: 0.4148 - val_acc: 0.8999\n",
      "Epoch 130/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4729 - acc: 0.8771 - val_loss: 0.4100 - val_acc: 0.8996\n",
      "Epoch 131/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4806 - acc: 0.8756 - val_loss: 0.4102 - val_acc: 0.9013\n",
      "Epoch 132/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4702 - acc: 0.8756 - val_loss: 0.4066 - val_acc: 0.9009\n",
      "Epoch 133/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4665 - acc: 0.8785 - val_loss: 0.4116 - val_acc: 0.9013\n",
      "Epoch 134/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4669 - acc: 0.8794 - val_loss: 0.4107 - val_acc: 0.9021\n",
      "Epoch 135/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4740 - acc: 0.8768 - val_loss: 0.4120 - val_acc: 0.8973\n",
      "Epoch 136/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4732 - acc: 0.8782 - val_loss: 0.4165 - val_acc: 0.8975\n",
      "Epoch 137/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4641 - acc: 0.8805 - val_loss: 0.4108 - val_acc: 0.9007\n",
      "Epoch 138/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4726 - acc: 0.8775 - val_loss: 0.4095 - val_acc: 0.8992\n",
      "Epoch 139/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4730 - acc: 0.8768 - val_loss: 0.4038 - val_acc: 0.9009\n",
      "Epoch 140/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4615 - acc: 0.8786 - val_loss: 0.4090 - val_acc: 0.8993\n",
      "Epoch 141/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4596 - acc: 0.8809 - val_loss: 0.4067 - val_acc: 0.9002\n",
      "Epoch 142/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4594 - acc: 0.8804 - val_loss: 0.4079 - val_acc: 0.8994\n",
      "Epoch 143/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4672 - acc: 0.8786 - val_loss: 0.4062 - val_acc: 0.9019\n",
      "Epoch 144/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4569 - acc: 0.8807 - val_loss: 0.4140 - val_acc: 0.9011\n",
      "Epoch 145/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4558 - acc: 0.8806 - val_loss: 0.4125 - val_acc: 0.9000\n",
      "Epoch 146/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4571 - acc: 0.8800 - val_loss: 0.4090 - val_acc: 0.8989\n",
      "Epoch 147/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4587 - acc: 0.8794 - val_loss: 0.4107 - val_acc: 0.9012\n",
      "Epoch 148/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4529 - acc: 0.8810 - val_loss: 0.4158 - val_acc: 0.9004\n",
      "Epoch 149/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4613 - acc: 0.8802 - val_loss: 0.4089 - val_acc: 0.9009\n",
      "Epoch 150/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4523 - acc: 0.8833 - val_loss: 0.4183 - val_acc: 0.9005\n",
      "Epoch 151/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4558 - acc: 0.8821 - val_loss: 0.4013 - val_acc: 0.9032\n",
      "Epoch 152/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4455 - acc: 0.8815 - val_loss: 0.4038 - val_acc: 0.9020\n",
      "Epoch 153/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4510 - acc: 0.8824 - val_loss: 0.4017 - val_acc: 0.9032\n",
      "Epoch 154/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4404 - acc: 0.8848 - val_loss: 0.4056 - val_acc: 0.9021\n",
      "Epoch 155/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4498 - acc: 0.8828 - val_loss: 0.4026 - val_acc: 0.9015\n",
      "Epoch 156/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4371 - acc: 0.8846 - val_loss: 0.4042 - val_acc: 0.9017\n",
      "Epoch 157/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4392 - acc: 0.8833 - val_loss: 0.4030 - val_acc: 0.9032\n",
      "Epoch 158/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4372 - acc: 0.8873 - val_loss: 0.3990 - val_acc: 0.9021\n",
      "Epoch 159/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4281 - acc: 0.8863 - val_loss: 0.3996 - val_acc: 0.9034\n",
      "Epoch 160/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4400 - acc: 0.8828 - val_loss: 0.4030 - val_acc: 0.9022\n",
      "Epoch 161/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4468 - acc: 0.8848 - val_loss: 0.3962 - val_acc: 0.9044\n",
      "Epoch 162/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4409 - acc: 0.8843 - val_loss: 0.3986 - val_acc: 0.9038\n",
      "Epoch 163/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4314 - acc: 0.8875 - val_loss: 0.4032 - val_acc: 0.9022\n",
      "Epoch 164/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4289 - acc: 0.8863 - val_loss: 0.4030 - val_acc: 0.9028\n",
      "Epoch 165/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4336 - acc: 0.8859 - val_loss: 0.3985 - val_acc: 0.9046\n",
      "Epoch 166/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4401 - acc: 0.8848 - val_loss: 0.3937 - val_acc: 0.9040\n",
      "Epoch 167/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4365 - acc: 0.8860 - val_loss: 0.3976 - val_acc: 0.9039\n",
      "Epoch 168/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4307 - acc: 0.8868 - val_loss: 0.3939 - val_acc: 0.9063\n",
      "Epoch 169/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4343 - acc: 0.8838 - val_loss: 0.3913 - val_acc: 0.9066\n",
      "Epoch 170/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4370 - acc: 0.8870 - val_loss: 0.4016 - val_acc: 0.9034\n",
      "Epoch 171/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4343 - acc: 0.8866 - val_loss: 0.3982 - val_acc: 0.9031\n",
      "Epoch 172/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4341 - acc: 0.8883 - val_loss: 0.3960 - val_acc: 0.9031\n",
      "Epoch 173/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4298 - acc: 0.8879 - val_loss: 0.3942 - val_acc: 0.9046\n",
      "Epoch 174/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4281 - acc: 0.8878 - val_loss: 0.3977 - val_acc: 0.9043\n",
      "Epoch 175/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4319 - acc: 0.8869 - val_loss: 0.3896 - val_acc: 0.9036\n",
      "Epoch 176/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4295 - acc: 0.8861 - val_loss: 0.3899 - val_acc: 0.9058\n",
      "Epoch 177/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4249 - acc: 0.8886 - val_loss: 0.3915 - val_acc: 0.9049\n",
      "Epoch 178/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4228 - acc: 0.8904 - val_loss: 0.3927 - val_acc: 0.9047\n",
      "Epoch 179/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4314 - acc: 0.8859 - val_loss: 0.3882 - val_acc: 0.9059\n",
      "Epoch 180/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4262 - acc: 0.8881 - val_loss: 0.3908 - val_acc: 0.9055\n",
      "Epoch 181/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4155 - acc: 0.8924 - val_loss: 0.3944 - val_acc: 0.9046\n",
      "Epoch 182/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4219 - acc: 0.8912 - val_loss: 0.3904 - val_acc: 0.9056\n",
      "Epoch 183/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4236 - acc: 0.8887 - val_loss: 0.3917 - val_acc: 0.9049\n",
      "Epoch 184/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4249 - acc: 0.8869 - val_loss: 0.3892 - val_acc: 0.9054\n",
      "Epoch 185/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4277 - acc: 0.8900 - val_loss: 0.3896 - val_acc: 0.9061\n",
      "Epoch 186/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4183 - acc: 0.8894 - val_loss: 0.3899 - val_acc: 0.9052\n",
      "Epoch 187/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4275 - acc: 0.8881 - val_loss: 0.3913 - val_acc: 0.9051\n",
      "Epoch 188/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4256 - acc: 0.8872 - val_loss: 0.3913 - val_acc: 0.9052\n",
      "Epoch 189/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4203 - acc: 0.8910 - val_loss: 0.3971 - val_acc: 0.9049\n",
      "Epoch 190/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4239 - acc: 0.8902 - val_loss: 0.3947 - val_acc: 0.9044\n",
      "Epoch 191/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4304 - acc: 0.8884 - val_loss: 0.3913 - val_acc: 0.9062\n",
      "Epoch 192/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4125 - acc: 0.8910 - val_loss: 0.3908 - val_acc: 0.9062\n",
      "Epoch 193/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4260 - acc: 0.8880 - val_loss: 0.3904 - val_acc: 0.9035\n",
      "Epoch 194/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4234 - acc: 0.8903 - val_loss: 0.3897 - val_acc: 0.9055\n",
      "Epoch 195/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4208 - acc: 0.8907 - val_loss: 0.3863 - val_acc: 0.9071\n",
      "Epoch 196/250\n",
      "390/390 [==============================] - 151s 387ms/step - loss: 0.4245 - acc: 0.8871 - val_loss: 0.3886 - val_acc: 0.9054\n",
      "Epoch 197/250\n",
      "390/390 [==============================] - 152s 390ms/step - loss: 0.4171 - acc: 0.8908 - val_loss: 0.3893 - val_acc: 0.9042\n",
      "Epoch 198/250\n",
      "390/390 [==============================] - 152s 389ms/step - loss: 0.4272 - acc: 0.8884 - val_loss: 0.3888 - val_acc: 0.9050\n",
      "Epoch 199/250\n",
      "390/390 [==============================] - 152s 390ms/step - loss: 0.4164 - acc: 0.8895 - val_loss: 0.3919 - val_acc: 0.9048\n",
      "Epoch 200/250\n",
      "390/390 [==============================] - 152s 390ms/step - loss: 0.4216 - acc: 0.8897 - val_loss: 0.3838 - val_acc: 0.9076\n",
      "Epoch 201/250\n",
      "390/390 [==============================] - 151s 386ms/step - loss: 0.4196 - acc: 0.8889 - val_loss: 0.3863 - val_acc: 0.9055\n",
      "Epoch 202/250\n",
      "390/390 [==============================] - 151s 386ms/step - loss: 0.4119 - acc: 0.8917 - val_loss: 0.3888 - val_acc: 0.9069\n",
      "Epoch 203/250\n",
      "390/390 [==============================] - 151s 386ms/step - loss: 0.4101 - acc: 0.8901 - val_loss: 0.3889 - val_acc: 0.9062\n",
      "Epoch 204/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4184 - acc: 0.8902 - val_loss: 0.3859 - val_acc: 0.9061\n",
      "Epoch 205/250\n",
      "390/390 [==============================] - 151s 387ms/step - loss: 0.4185 - acc: 0.8907 - val_loss: 0.3884 - val_acc: 0.9062\n",
      "Epoch 206/250\n",
      "390/390 [==============================] - 151s 386ms/step - loss: 0.4217 - acc: 0.8918 - val_loss: 0.3884 - val_acc: 0.9067\n",
      "Epoch 207/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4095 - acc: 0.8930 - val_loss: 0.3885 - val_acc: 0.9067\n",
      "Epoch 208/250\n",
      "390/390 [==============================] - 151s 386ms/step - loss: 0.4205 - acc: 0.8919 - val_loss: 0.3890 - val_acc: 0.9060\n",
      "Epoch 209/250\n",
      "390/390 [==============================] - 151s 386ms/step - loss: 0.4157 - acc: 0.8920 - val_loss: 0.3872 - val_acc: 0.9060\n",
      "Epoch 210/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4151 - acc: 0.8919 - val_loss: 0.3867 - val_acc: 0.9061\n",
      "Epoch 211/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4108 - acc: 0.8911 - val_loss: 0.3858 - val_acc: 0.9067\n",
      "Epoch 212/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4157 - acc: 0.8901 - val_loss: 0.3851 - val_acc: 0.9062\n",
      "Epoch 213/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4156 - acc: 0.8897 - val_loss: 0.3867 - val_acc: 0.9059\n",
      "Epoch 214/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4125 - acc: 0.8933 - val_loss: 0.3861 - val_acc: 0.9056\n",
      "Epoch 215/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4115 - acc: 0.8920 - val_loss: 0.3853 - val_acc: 0.9065\n",
      "Epoch 216/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4139 - acc: 0.8915 - val_loss: 0.3849 - val_acc: 0.9062\n",
      "Epoch 217/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4056 - acc: 0.8913 - val_loss: 0.3864 - val_acc: 0.9068\n",
      "Epoch 218/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4063 - acc: 0.8922 - val_loss: 0.3835 - val_acc: 0.9058\n",
      "Epoch 219/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4164 - acc: 0.8909 - val_loss: 0.3859 - val_acc: 0.9067\n",
      "Epoch 220/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4095 - acc: 0.8928 - val_loss: 0.3866 - val_acc: 0.9063\n",
      "Epoch 221/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4103 - acc: 0.8922 - val_loss: 0.3841 - val_acc: 0.9071\n",
      "Epoch 222/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4123 - acc: 0.8927 - val_loss: 0.3858 - val_acc: 0.9056\n",
      "Epoch 223/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4169 - acc: 0.8909 - val_loss: 0.3826 - val_acc: 0.9066\n",
      "Epoch 224/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4040 - acc: 0.8927 - val_loss: 0.3835 - val_acc: 0.9059\n",
      "Epoch 225/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4157 - acc: 0.8910 - val_loss: 0.3838 - val_acc: 0.9061\n",
      "Epoch 226/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4099 - acc: 0.8905 - val_loss: 0.3833 - val_acc: 0.9069\n",
      "Epoch 227/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4170 - acc: 0.8902 - val_loss: 0.3850 - val_acc: 0.9073\n",
      "Epoch 228/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4120 - acc: 0.8931 - val_loss: 0.3851 - val_acc: 0.9061\n",
      "Epoch 229/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4103 - acc: 0.8920 - val_loss: 0.3846 - val_acc: 0.9076\n",
      "Epoch 230/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4180 - acc: 0.8912 - val_loss: 0.3862 - val_acc: 0.9060\n",
      "Epoch 231/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4036 - acc: 0.8934 - val_loss: 0.3844 - val_acc: 0.9063\n",
      "Epoch 232/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4111 - acc: 0.8932 - val_loss: 0.3829 - val_acc: 0.9067\n",
      "Epoch 233/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4167 - acc: 0.8905 - val_loss: 0.3864 - val_acc: 0.9072\n",
      "Epoch 234/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4123 - acc: 0.8926 - val_loss: 0.3833 - val_acc: 0.9065\n",
      "Epoch 235/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4074 - acc: 0.8920 - val_loss: 0.3856 - val_acc: 0.9066\n",
      "Epoch 236/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4161 - acc: 0.8911 - val_loss: 0.3874 - val_acc: 0.9062\n",
      "Epoch 237/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4083 - acc: 0.8933 - val_loss: 0.3882 - val_acc: 0.9066\n",
      "Epoch 238/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4122 - acc: 0.8926 - val_loss: 0.3864 - val_acc: 0.9056\n",
      "Epoch 239/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4134 - acc: 0.8913 - val_loss: 0.3865 - val_acc: 0.9072\n",
      "Epoch 240/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4080 - acc: 0.8921 - val_loss: 0.3838 - val_acc: 0.9061\n",
      "Epoch 241/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4032 - acc: 0.8934 - val_loss: 0.3836 - val_acc: 0.9077\n",
      "Epoch 242/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4087 - acc: 0.8922 - val_loss: 0.3833 - val_acc: 0.9073\n",
      "Epoch 243/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4023 - acc: 0.8933 - val_loss: 0.3835 - val_acc: 0.9076\n",
      "Epoch 244/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4170 - acc: 0.8902 - val_loss: 0.3845 - val_acc: 0.9077\n",
      "Epoch 245/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4165 - acc: 0.8920 - val_loss: 0.3871 - val_acc: 0.9067\n",
      "Epoch 246/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4125 - acc: 0.8923 - val_loss: 0.3866 - val_acc: 0.9072\n",
      "Epoch 247/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4044 - acc: 0.8944 - val_loss: 0.3838 - val_acc: 0.9070\n",
      "Epoch 248/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4118 - acc: 0.8921 - val_loss: 0.3868 - val_acc: 0.9072\n",
      "Epoch 249/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4136 - acc: 0.8919 - val_loss: 0.3869 - val_acc: 0.9066\n",
      "Epoch 250/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4125 - acc: 0.8923 - val_loss: 0.3830 - val_acc: 0.9080\n",
      "{'loss': [1.9606263613066002, 1.4285279590042501, 1.2851647286985715, 1.2395645600693097, 1.2154007523267132, 1.092348838487343, 1.0201720210807539, 0.99553388982466251, 0.96668620590678689, 0.95963329839752309, 0.91996326580772725, 0.9265274648525641, 0.8971790177033504, 0.93530390235449901, 0.95997798421805891, 0.88684825240169762, 0.8634051599294511, 0.85662877154266137, 0.83068536015936756, 0.8407915790837508, 0.82336696763431794, 0.81910734844773792, 0.79648888698173403, 0.79235424475674454, 0.80469731007931822, 0.78987562235708908, 0.78108935026086823, 0.79173751037243967, 0.82323106110841748, 0.76724145385291209, 0.76981521301021705, 0.78413070804766061, 0.76799681405054288, 0.75656035382815723, 0.74857977880741644, 0.75228735073124475, 0.76166106165315928, 0.75878710155116846, 0.74535414711343662, 0.7447460931120754, 0.74540607197605191, 0.74901985740638677, 0.73326936965574618, 0.72710004426899877, 0.72776343285521139, 0.73583205380346439, 0.72537897330734802, 0.72563661816107139, 0.72080473210286133, 0.72395493307670489, 0.72534399308720798, 0.72488799929274628, 0.73274977145227138, 0.72373013407288356, 0.71058438923407108, 0.71439908378430961, 0.70776626403644605, 0.70277334428750549, 0.70767321785864712, 0.70498995556613919, 0.69378742584250541, 0.68778317051894544, 0.70740377352258355, 0.69652579830440142, 0.6923580771652138, 0.68693392499349115, 0.68118175647179979, 0.6893885712765867, 0.68897041487388122, 0.69018425433085984, 0.68106328532754667, 0.6902184485624262, 0.67968572788526127, 0.68536817379245629, 0.67589372664183278, 0.62981540625356203, 0.61910119485426762, 0.61785005524940972, 0.60607222547897921, 0.61731572522899625, 0.59597336458096639, 0.59704962861843602, 0.5968911721609248, 0.59714638403040821, 0.59218336160366347, 0.59289554567312541, 0.58540013630419996, 0.57660832370096271, 0.57741042806201914, 0.57828731017882218, 0.56554593923652563, 0.57761023107307785, 0.57945937032882988, 0.57167643722082151, 0.58319906823941825, 0.57760840542506897, 0.57039154704121164, 0.56982062726448746, 0.57739333774925583, 0.56597045762897646, 0.52902326369896913, 0.54059116260011608, 0.53977148692348575, 0.52213270259209166, 0.53644967398814991, 0.51470924233778925, 0.52011129270580514, 0.5254001266292524, 0.51972501902080015, 0.51936178896570195, 0.51745236393732907, 0.51299240452093275, 0.51379992839882382, 0.50873056986393073, 0.51392982771898899, 0.49965086151936283, 0.50690819475564353, 0.50970388892984253, 0.50069707534540375, 0.50457615289391333, 0.49329160558871732, 0.50121519662435043, 0.49590623414130175, 0.50550301565663491, 0.49582205122389927, 0.49088190769896817, 0.4888102834447286, 0.47426856840268161, 0.47260143269540106, 0.47291834163360108, 0.48046730509459706, 0.46992921991764369, 0.4663412320258184, 0.46577155888367433, 0.47371708834289855, 0.47319037944107128, 0.46410254723451289, 0.47264630036511635, 0.47301190556623995, 0.46155918827406012, 0.45958333802528872, 0.45941305606075306, 0.46735523212354152, 0.4568932335346173, 0.45588814775302006, 0.45732061741975305, 0.45860464948853658, 0.452923215734653, 0.46091638294909348, 0.45233760270737361, 0.45585609327572679, 0.44551194711373404, 0.45089241664379809, 0.44047003952868874, 0.44980336993168563, 0.43687503732736094, 0.43931878614127351, 0.43723825262143062, 0.42793741066178681, 0.43998085627189049, 0.44692603525015967, 0.44087330198440794, 0.43100078214455201, 0.42899546319756859, 0.4336488574389506, 0.4401000338487136, 0.43642999721821407, 0.43074423351041546, 0.43412132883897991, 0.43706978419135312, 0.43427975702867089, 0.43420247405165086, 0.42976231911243534, 0.42808488622910712, 0.43180042184007295, 0.42951539984116188, 0.42487862658332232, 0.4228264228655742, 0.43139503738954355, 0.42608675397047263, 0.41546895068425399, 0.42205545163743574, 0.4237979822765196, 0.42490139933476895, 0.4276837692224027, 0.41836334773084466, 0.42751628466141528, 0.42553383909359471, 0.42038664650106261, 0.42386565151203881, 0.43040162260845222, 0.41254860678544414, 0.42598829225045121, 0.42353910143688239, 0.4205173299951131, 0.42448090578501041, 0.41719159661748295, 0.42741337354434328, 0.41635042153871976, 0.42160182124267298, 0.41960078654912469, 0.41194804318440265, 0.41014588799926383, 0.41839745959450053, 0.41847622119463407, 0.42173014591242741, 0.40945811284491856, 0.42049211970506573, 0.41552025645751389, 0.41510364586319493, 0.41079767885771634, 0.41560299485625463, 0.41561625263629814, 0.41234184518293759, 0.41156267557612281, 0.41379097191079672, 0.40568870163552839, 0.40629402607115256, 0.4163874843181708, 0.40963088873685288, 0.41018482014885843, 0.41231382290522256, 0.41705809741552569, 0.40382133835892592, 0.41563782104522506, 0.40990564871530405, 0.41698659643148767, 0.41193259217473688, 0.41033915056582054, 0.41797500539284488, 0.40360250063228575, 0.41097423192358645, 0.41673540527865716, 0.41219030887146657, 0.40666142609460887, 0.41611860528206213, 0.40822796835744846, 0.41238393078333757, 0.41326445035151194, 0.40793805796250265, 0.40282012189169608, 0.40882554739226046, 0.40214861464263307, 0.41700631353335504, 0.41658358673063239, 0.41264107840582082, 0.4044132894430405, 0.41188931119988886, 0.41340002110353552, 0.41238015672125644], 'val_loss': [1.5353102275848389, 1.2511315492630004, 1.028788554763794, 0.96383356246948237, 0.91937717456817625, 0.76980556907653808, 0.83654710874557492, 0.78271584587097165, 0.72727183074951174, 0.71264669122695923, 0.70072856130599981, 0.66894534397125249, 0.64450131587982173, 0.73441424980163572, 0.68547839393615728, 0.70530412206649784, 0.66663007016181941, 0.61686542234420771, 0.61795299367904666, 0.64103055534362796, 0.68266279001235963, 0.60305770263671876, 0.59736551361083989, 0.60618323116302486, 0.59876878776550291, 0.59192597513198852, 0.60937264785766598, 0.58189999814033511, 0.62918865804672242, 0.61312822561264035, 0.58315999460220336, 0.60136109905242918, 0.60672340168952943, 0.57022309093475343, 0.57781283488273616, 0.59813888854980468, 0.58864321599006653, 0.55848334503173824, 0.55679802417755131, 0.54198075780868527, 0.5875032178878784, 0.57464510622024534, 0.58689665842056271, 0.54845898780822755, 0.57500582208633422, 0.56650073013305668, 0.61024129400253291, 0.55355809211730955, 0.54805481300354009, 0.5327681015968323, 0.5562429167747498, 0.58487686672210693, 0.58182134485244752, 0.55054294281005856, 0.556113961315155, 0.56470327196121217, 0.55299778871536254, 0.53757788753509517, 0.58668339662551883, 0.55767289810180665, 0.54755154113769533, 0.57886960363388063, 0.54630543565750123, 0.54542198128700259, 0.55735869455337528, 0.54078086557388305, 0.5453886782646179, 0.57167924842834472, 0.54897692785263064, 0.52695137462615971, 0.54972196292877196, 0.5429105225563049, 0.53015378160476689, 0.56819452819824223, 0.52338555965423583, 0.49031447439193726, 0.48221118144989011, 0.48199383821487429, 0.49629739665985106, 0.47982959904670713, 0.49188352088928222, 0.48731981363296512, 0.46622953548431395, 0.46758901386260987, 0.48106759223937989, 0.46330616216659548, 0.48355006980895998, 0.4701281744480133, 0.4628581986427307, 0.49494956598281858, 0.45978553729057314, 0.4753007730484009, 0.475117938041687, 0.4917827904701233, 0.45599929904937742, 0.45673161230087278, 0.48278125658035276, 0.47449865589141843, 0.48114862985610962, 0.46633908767700194, 0.44970391302108764, 0.44386673192977905, 0.45578243236541749, 0.44423342080116274, 0.44556299867630006, 0.44321089935302732, 0.44166280288696291, 0.44584662551879883, 0.44215706825256346, 0.44543801918029785, 0.4413318461418152, 0.4370399094581604, 0.43860529336929321, 0.44139083886146546, 0.43541822767257693, 0.44040260486602784, 0.42068512039184569, 0.43038094253540038, 0.42789761848449709, 0.43717806882858279, 0.42657497301101682, 0.41985793561935425, 0.42701638011932375, 0.42584333057403567, 0.42363173666000364, 0.4179924199104309, 0.41565817623138429, 0.41380558166503906, 0.41480811367034914, 0.41002955236434935, 0.41022308273315428, 0.40662778949737549, 0.41164528875350953, 0.41065315790176393, 0.41198443870544432, 0.41653238048553465, 0.41078140020370485, 0.40949231920242307, 0.40378858280181884, 0.40900740246772765, 0.40666375856399534, 0.40792637357711792, 0.40618552923202517, 0.41397294340133667, 0.4124524386405945, 0.40896399869918826, 0.41067860469818113, 0.41577745323181153, 0.40889778323173526, 0.41826866722106931, 0.40129365592002869, 0.40377383518218996, 0.4016814606666565, 0.40561274595260621, 0.40255386190414427, 0.40422762279510499, 0.4029849738121033, 0.39896407346725465, 0.39959831895828246, 0.40296825103759765, 0.39619181036949158, 0.39862356333732607, 0.40319503765106202, 0.40299601011276243, 0.39850150527954104, 0.39370406160354615, 0.39755279016494749, 0.39393311901092531, 0.39127146949768066, 0.40164073820114138, 0.39821912984848024, 0.39596660499572756, 0.39416948661804202, 0.39769418401718137, 0.38963532648086546, 0.389865350151062, 0.39149168624877928, 0.39265728464126587, 0.38818582630157472, 0.39075427389144896, 0.39440763301849363, 0.39038327693939207, 0.39165069866180419, 0.38918730821609498, 0.38964088602066038, 0.38991656417846682, 0.39129623060226443, 0.39125484018325807, 0.39713601875305177, 0.39466685819625852, 0.39130658502578736, 0.3907999183654785, 0.39038417873382569, 0.38972664775848387, 0.38628298215866091, 0.38862158412933351, 0.38925425138473513, 0.38882527351379392, 0.39188207082748411, 0.38381159696578981, 0.38626808977127075, 0.3888071350097656, 0.38886633129119874, 0.38587628898620607, 0.38844918756484986, 0.3884271119117737, 0.38852754116058352, 0.38902278566360471, 0.38716454658508298, 0.3866672863960266, 0.38584178867340085, 0.38505929365158081, 0.38669088020324704, 0.38608378734588622, 0.38527140455245973, 0.38486714029312136, 0.38637607784271238, 0.38349674787521365, 0.38593137311935427, 0.38660328607559202, 0.38406870594024656, 0.385812767124176, 0.38258369159698485, 0.38346449909210206, 0.38379973983764648, 0.38330631494522094, 0.38501011276245117, 0.38512233867645262, 0.38455464735031125, 0.3862081198692322, 0.38440557765960692, 0.38288817749023435, 0.38640591716766359, 0.38330367422103884, 0.38562303152084348, 0.38741232852935792, 0.38819014225006104, 0.38638743362426758, 0.38652705831527712, 0.38380824413299558, 0.38356945409774779, 0.38334254283905028, 0.38345659208297728, 0.38448628463745116, 0.38713516283035276, 0.38659954261779783, 0.38376051969528197, 0.38683960390090943, 0.38691363554000857, 0.38298448657989503], 'acc': [0.40365335256348905, 0.5464589348541532, 0.59907362848893164, 0.63007298684632662, 0.65200914338799976, 0.67382499196034495, 0.69459817135682045, 0.70885466793082941, 0.71725617581007384, 0.72535691371819211, 0.73403914017978678, 0.73887151104921245, 0.74480670516522296, 0.74665142765479631, 0.74785450752017812, 0.75523339745934892, 0.76291305742701321, 0.76425649659313144, 0.77093359002862027, 0.7733397497785065, 0.77384103302547169, 0.77686878408726334, 0.7797962784537682, 0.78202197627834602, 0.78422762267590329, 0.78709496308642779, 0.79150625605364433, 0.78950112289368135, 0.78637311515572517, 0.79473452035303027, 0.79355149183817919, 0.79407282647392019, 0.79696021815848572, 0.79908565924953179, 0.80099053579069768, 0.80056945785024358, 0.79964709660545696, 0.80175248636509466, 0.80427895416079864, 0.8025344882900225, 0.80548203404530294, 0.80455967274314899, 0.80800850178363959, 0.80800850178363959, 0.80774783449445275, 0.81097609885120603, 0.81041466153352582, 0.81103625284543124, 0.8137030798845043, 0.81256015399422521, 0.81262030795020557, 0.81460538979788255, 0.815607956330058, 0.81500641644517313, 0.8139436958996501, 0.8167308309080511, 0.81602903430875695, 0.8171274038461539, 0.81825473215245725, 0.81878211303789339, 0.82001924923990721, 0.8190968880716103, 0.81873596404889171, 0.82076114855938553, 0.82370869429554361, 0.82048042990054537, 0.8239292589219136, 0.82543310872017672, 0.82487980769230773, 0.82175256908131322, 0.82515239015694875, 0.82467115812665726, 0.8262953159898605, 0.82427013151761164, 0.82830044914982359, 0.84011068338761929, 0.84436156556971154, 0.84504331083759732, 0.84591346153846159, 0.84547607574848771, 0.84801090796253109, 0.8487179487179487, 0.85045717031786672, 0.84718208094399639, 0.84935897435897434, 0.85109585737983007, 0.84971527105575573, 0.85170035294167767, 0.85228184153339903, 0.85242220079589048, 0.85272297084350634, 0.85178055820994392, 0.85184294871794874, 0.85234023763648037, 0.85158004489585992, 0.85158004487673744, 0.85172040427308604, 0.85524839743589742, 0.85107578680771012, 0.85336461339120806, 0.8650641025641026, 0.8628773281569655, 0.86032242542816961, 0.86598557692307687, 0.86195407841323501, 0.86804887820512822, 0.86405197305101056, 0.86689145793192035, 0.86555582286839616, 0.86519489894128965, 0.86810897435897438, 0.86602842011509906, 0.86738049402656103, 0.87007211538461537, 0.86703195245356302, 0.87036814240641347, 0.86870388189951575, 0.86732034003233582, 0.86730028870092735, 0.86786172601860767, 0.87131410256410258, 0.8699020552152843, 0.87004732118036876, 0.87072906641000958, 0.87082932302880678, 0.87131055503997579, 0.8751203080075729, 0.87536057692307689, 0.87718770068734597, 0.87710336538461542, 0.87564226073872686, 0.87558148861084373, 0.87852903430875695, 0.87949149821636041, 0.87684472248957335, 0.87820821302534491, 0.88051411613731156, 0.87748636509464228, 0.87684294871794877, 0.87859264611432242, 0.88088942307692308, 0.88037375679833019, 0.87849229285176478, 0.88066907051282051, 0.88059432142470029, 0.87997752087347458, 0.87949149821636041, 0.8810296474358974, 0.88033879258805692, 0.88334135384677426, 0.88207811996804475, 0.88145032051282046, 0.88239894125145679, 0.88473426453486881, 0.88279246794871791, 0.88463391142543801, 0.88326114855938553, 0.88727964743589749, 0.88635998719306652, 0.8828125, 0.88471419392446715, 0.88429487179487176, 0.88744380222196828, 0.88620869423817628, 0.88588787291651927, 0.88481570512820518, 0.88595857414283585, 0.88677013157497897, 0.88386268850163763, 0.88699069622047133, 0.88654956687211917, 0.88827398143060932, 0.88794070512820511, 0.88774486185651447, 0.88691049083747042, 0.88611778846153844, 0.8886078998264626, 0.89038461538461533, 0.88590792430529508, 0.88808606296069514, 0.89242788461538458, 0.89114132178376648, 0.88862797051342768, 0.88693054218800127, 0.89003849853718164, 0.8893567533266632, 0.88814102564102559, 0.88729146613423016, 0.89102101379531606, 0.89019348101502593, 0.88837423803028404, 0.89102564102564108, 0.88807346811652532, 0.89027911451408259, 0.89075545917816012, 0.88713942307692306, 0.89078039785666008, 0.88830684001964866, 0.88948317307692304, 0.88971767727289208, 0.88892903016711478, 0.8916866987179487, 0.89005854990683497, 0.89025369296107604, 0.89066506410256407, 0.89188322108412232, 0.8930033718115562, 0.89190705128205128, 0.89202358032749141, 0.89184311834481578, 0.89113680152228492, 0.89007860121912097, 0.88972355769230771, 0.89334457285831426, 0.89194337504010268, 0.89152229706140373, 0.89132178374731963, 0.89224414504947358, 0.89088541666666665, 0.89282273597340589, 0.89224414497298388, 0.89268830128205123, 0.89094080843143753, 0.89266522293256034, 0.8910364483362907, 0.89051973047186095, 0.89024439102564101, 0.89316650625601535, 0.8919596981374438, 0.8911658653846154, 0.89334696820044612, 0.89324671158164903, 0.89051973049098343, 0.89260506901482495, 0.89203998067475465, 0.89106570512820515, 0.89330686553762939, 0.89258501762604925, 0.89136188641013647, 0.89216393970471752, 0.89350513814348553, 0.89214388839243164, 0.89338707088238545, 0.89024439102564101, 0.89195969811830289, 0.89224414497298388, 0.8943509615384615, 0.89206368299030825, 0.89189948622967541, 0.89236445303792411], 'val_acc': [0.46610000000000001, 0.62990000000000002, 0.68100000000000005, 0.71940000000000004, 0.73080000000000001, 0.76339999999999997, 0.74960000000000004, 0.76600000000000001, 0.77759999999999996, 0.7974, 0.79969999999999997, 0.80649999999999999, 0.81179999999999997, 0.80359999999999998, 0.80200000000000005, 0.81340000000000001, 0.82330000000000003, 0.82430000000000003, 0.82850000000000001, 0.82169999999999999, 0.81699999999999995, 0.83740000000000003, 0.8337, 0.8327, 0.8347, 0.84150000000000003, 0.83819999999999995, 0.84050000000000002, 0.83069999999999999, 0.83640000000000003, 0.8498, 0.83909999999999996, 0.84489999999999998, 0.84789999999999999, 0.84999999999999998, 0.84079999999999999, 0.84460000000000002, 0.85699999999999998, 0.8569, 0.85960000000000003, 0.84789999999999999, 0.85319999999999996, 0.85699999999999998, 0.85899999999999999, 0.85560000000000003, 0.85940000000000005, 0.83750000000000002, 0.86170000000000002, 0.86429999999999996, 0.86860000000000004, 0.86180000000000001, 0.8508, 0.85589999999999999, 0.8599, 0.86099999999999999, 0.85960000000000003, 0.86160000000000003, 0.86319999999999997, 0.85319999999999996, 0.86580000000000001, 0.86250000000000004, 0.85599999999999998, 0.86360000000000003, 0.86209999999999998, 0.86529999999999996, 0.86880000000000002, 0.86509999999999998, 0.86799999999999999, 0.85919999999999996, 0.87260000000000004, 0.8599, 0.86680000000000001, 0.8659, 0.8639, 0.86609999999999998, 0.8821, 0.88360000000000005, 0.88009999999999999, 0.88270000000000004, 0.88539999999999996, 0.88170000000000004, 0.88439999999999996, 0.88859999999999995, 0.88800000000000001, 0.8821, 0.88829999999999998, 0.88349999999999995, 0.88380000000000003, 0.88959999999999995, 0.88109999999999999, 0.89119999999999999, 0.88639999999999997, 0.88190000000000002, 0.87860000000000005, 0.89049999999999996, 0.88790000000000002, 0.87890000000000001, 0.88349999999999995, 0.87939999999999996, 0.88470000000000004, 0.89219999999999999, 0.89019999999999999, 0.88949999999999996, 0.89070000000000005, 0.89229999999999998, 0.89080000000000004, 0.89249999999999996, 0.88870000000000005, 0.89159999999999995, 0.89159999999999995, 0.89500000000000002, 0.89410000000000001, 0.89059999999999995, 0.89449999999999996, 0.8931, 0.89139999999999997, 0.89649999999999996, 0.89490000000000003, 0.89510000000000001, 0.89270000000000005, 0.89370000000000005, 0.89490000000000003, 0.8952, 0.89470000000000005, 0.8952, 0.90039999999999998, 0.89710000000000001, 0.89890000000000003, 0.89990000000000003, 0.89959999999999996, 0.90129999999999999, 0.90090000000000003, 0.90129999999999999, 0.90210000000000001, 0.89729999999999999, 0.89749999999999996, 0.90069999999999995, 0.8992, 0.90090000000000003, 0.89929999999999999, 0.9002, 0.89939999999999998, 0.90190000000000003, 0.90110000000000001, 0.90000000000000002, 0.89890000000000003, 0.9012, 0.90039999999999998, 0.90090000000000003, 0.90049999999999997, 0.9032, 0.90200000000000002, 0.9032, 0.90210000000000001, 0.90149999999999997, 0.90169999999999995, 0.9032, 0.90210000000000001, 0.90339999999999998, 0.9022, 0.90439999999999998, 0.90380000000000005, 0.9022, 0.90280000000000005, 0.90459999999999996, 0.90400000000000003, 0.90390000000000004, 0.90629999999999999, 0.90659999999999996, 0.90339999999999998, 0.90310000000000001, 0.90310000000000001, 0.90459999999999996, 0.90429999999999999, 0.90359999999999996, 0.90580000000000005, 0.90490000000000004, 0.90469999999999995, 0.90590000000000004, 0.90549999999999997, 0.90459999999999996, 0.90559999999999996, 0.90490000000000004, 0.90539999999999998, 0.90610000000000002, 0.9052, 0.90510000000000002, 0.9052, 0.90490000000000004, 0.90439999999999998, 0.90620000000000001, 0.90620000000000001, 0.90349999999999997, 0.90549999999999997, 0.90710000000000002, 0.90539999999999998, 0.9042, 0.90500000000000003, 0.90480000000000005, 0.90759999999999996, 0.90550000190734858, 0.90690000000000004, 0.90620000000000001, 0.90610000000000002, 0.90620000000000001, 0.90669999999999995, 0.90669999999999995, 0.90600000000000003, 0.90600000000000003, 0.90610000000000002, 0.90669999999999995, 0.90620000000000001, 0.90590000000000004, 0.90559999999999996, 0.90649999999999997, 0.90620000000000001, 0.90680000000000005, 0.90580000000000005, 0.90669999999999995, 0.90629999999999999, 0.90710000000000002, 0.90559999999999996, 0.90659999999999996, 0.90590000000000004, 0.90610000000000002, 0.90690000000000004, 0.9073, 0.90610000000000002, 0.90759999999999996, 0.90600000000000003, 0.90629999999999999, 0.90669999999999995, 0.90720000000000001, 0.90649999999999997, 0.90659999999999996, 0.90620000000000001, 0.90659999999999996, 0.90559999999999996, 0.90720000000000001, 0.90610000000000002, 0.90769999999999995, 0.9073, 0.90759999999999996, 0.90769999999999995, 0.90669999999999995, 0.90720000000000001, 0.90700000000000003, 0.90720000000000001, 0.90659999999999996, 0.90800000000000003]}\n"
     ]
    }
   ],
   "source": [
    "# Create model and get ready - reproducible cell\n",
    "act, act_name = leaky, \"leaky\"\n",
    "model = create(act)\n",
    "opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "\n",
    "# Training - only for 125 eppochs\n",
    "batch_size = 128\n",
    "epochs=250\n",
    "# lr scheduler\n",
    "lr_1 = keras.callbacks.LearningRateScheduler(schedule)\n",
    "\n",
    "# Fit the model\n",
    "his = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,verbose=1,validation_data=(x_test,y_test), callbacks=[lr_1])\n",
    "# Save the model\n",
    "model.save('models/leaky_relu/leaky_at_2.h5')\n",
    "# Display logs\n",
    "print(his.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Epoch 1/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 1.9464 - acc: 0.4039 - val_loss: 1.4471 - val_acc: 0.5117\n",
      "Epoch 2/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 1.4337 - acc: 0.5465 - val_loss: 1.1592 - val_acc: 0.6386\n",
      "Epoch 3/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 1.2999 - acc: 0.5991 - val_loss: 1.0106 - val_acc: 0.6816\n",
      "Epoch 4/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 1.1703 - acc: 0.6432 - val_loss: 0.8829 - val_acc: 0.7202\n",
      "Epoch 5/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 1.2003 - acc: 0.6546 - val_loss: 0.8272 - val_acc: 0.7360\n",
      "Epoch 6/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 1.1354 - acc: 0.6746 - val_loss: 0.8357 - val_acc: 0.7412\n",
      "Epoch 7/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 1.0873 - acc: 0.6887 - val_loss: 1.0684 - val_acc: 0.7244\n",
      "Epoch 8/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 1.0980 - acc: 0.6948 - val_loss: 0.8153 - val_acc: 0.7709\n",
      "Epoch 9/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 1.0361 - acc: 0.7049 - val_loss: 0.7569 - val_acc: 0.7854\n",
      "Epoch 10/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 1.0472 - acc: 0.7118 - val_loss: 0.7660 - val_acc: 0.7759\n",
      "Epoch 11/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 1.0057 - acc: 0.7235 - val_loss: 0.7519 - val_acc: 0.7772\n",
      "Epoch 12/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 1.0136 - acc: 0.7210 - val_loss: 0.6889 - val_acc: 0.8023\n",
      "Epoch 13/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.9482 - acc: 0.7350 - val_loss: 0.7510 - val_acc: 0.7869\n",
      "Epoch 14/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.9440 - acc: 0.7386 - val_loss: 0.7215 - val_acc: 0.8071\n",
      "Epoch 15/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.9716 - acc: 0.7392 - val_loss: 0.7619 - val_acc: 0.8019\n",
      "Epoch 16/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.9585 - acc: 0.7439 - val_loss: 0.7187 - val_acc: 0.7997\n",
      "Epoch 17/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.9323 - acc: 0.7487 - val_loss: 0.7043 - val_acc: 0.7924\n",
      "Epoch 18/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.9302 - acc: 0.7521 - val_loss: 0.7805 - val_acc: 0.7990\n",
      "Epoch 19/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.9052 - acc: 0.7570 - val_loss: 0.6689 - val_acc: 0.8126\n",
      "Epoch 20/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.9075 - acc: 0.7567 - val_loss: 0.6928 - val_acc: 0.8091\n",
      "Epoch 21/250\n",
      "390/390 [==============================] - 151s 386ms/step - loss: 0.9016 - acc: 0.7617 - val_loss: 0.6281 - val_acc: 0.8279\n",
      "Epoch 22/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.8688 - acc: 0.7662 - val_loss: 0.6257 - val_acc: 0.8270\n",
      "Epoch 23/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.8663 - acc: 0.7690 - val_loss: 0.6697 - val_acc: 0.8151\n",
      "Epoch 24/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.8672 - acc: 0.7700 - val_loss: 0.6610 - val_acc: 0.8273\n",
      "Epoch 25/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.8482 - acc: 0.7727 - val_loss: 0.6466 - val_acc: 0.8325\n",
      "Epoch 26/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.8230 - acc: 0.7750 - val_loss: 0.6025 - val_acc: 0.8389\n",
      "Epoch 27/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.8188 - acc: 0.7814 - val_loss: 0.6174 - val_acc: 0.8387\n",
      "Epoch 28/250\n",
      "390/390 [==============================] - 151s 386ms/step - loss: 0.8251 - acc: 0.7799 - val_loss: 0.6985 - val_acc: 0.8273\n",
      "Epoch 29/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.8033 - acc: 0.7831 - val_loss: 0.6251 - val_acc: 0.8335\n",
      "Epoch 30/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.8084 - acc: 0.7843 - val_loss: 0.5876 - val_acc: 0.8404\n",
      "Epoch 31/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.7992 - acc: 0.7869 - val_loss: 0.5997 - val_acc: 0.8415\n",
      "Epoch 32/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.8021 - acc: 0.7874 - val_loss: 0.6602 - val_acc: 0.8239\n",
      "Epoch 33/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7993 - acc: 0.7884 - val_loss: 0.5978 - val_acc: 0.8431\n",
      "Epoch 34/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.7899 - acc: 0.7920 - val_loss: 0.5976 - val_acc: 0.8474\n",
      "Epoch 35/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.7935 - acc: 0.7945 - val_loss: 0.6001 - val_acc: 0.8441\n",
      "Epoch 36/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.7906 - acc: 0.7925 - val_loss: 0.6192 - val_acc: 0.8378\n",
      "Epoch 37/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.8034 - acc: 0.7949 - val_loss: 0.6145 - val_acc: 0.8352\n",
      "Epoch 38/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.7828 - acc: 0.7953 - val_loss: 0.6108 - val_acc: 0.8417\n",
      "Epoch 39/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7778 - acc: 0.7965 - val_loss: 0.6288 - val_acc: 0.8310\n",
      "Epoch 40/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7822 - acc: 0.7974 - val_loss: 0.5776 - val_acc: 0.8559\n",
      "Epoch 41/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7711 - acc: 0.8010 - val_loss: 0.5688 - val_acc: 0.8552\n",
      "Epoch 42/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.7722 - acc: 0.7981 - val_loss: 0.5938 - val_acc: 0.8451\n",
      "Epoch 43/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7633 - acc: 0.8023 - val_loss: 0.6009 - val_acc: 0.8437\n",
      "Epoch 44/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.7612 - acc: 0.8019 - val_loss: 0.6213 - val_acc: 0.8505\n",
      "Epoch 45/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7631 - acc: 0.8023 - val_loss: 0.5930 - val_acc: 0.8543\n",
      "Epoch 46/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7547 - acc: 0.8043 - val_loss: 0.5747 - val_acc: 0.8528\n",
      "Epoch 47/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.7512 - acc: 0.8050 - val_loss: 0.5841 - val_acc: 0.8511\n",
      "Epoch 48/250\n",
      "390/390 [==============================] - 151s 388ms/step - loss: 0.7598 - acc: 0.8057 - val_loss: 0.6557 - val_acc: 0.8234\n",
      "Epoch 49/250\n",
      "390/390 [==============================] - 151s 388ms/step - loss: 0.7417 - acc: 0.8073 - val_loss: 0.5825 - val_acc: 0.8526\n",
      "Epoch 50/250\n",
      "390/390 [==============================] - 151s 388ms/step - loss: 0.7461 - acc: 0.8085 - val_loss: 0.5713 - val_acc: 0.8596\n",
      "Epoch 51/250\n",
      "390/390 [==============================] - 151s 388ms/step - loss: 0.7468 - acc: 0.8090 - val_loss: 0.5633 - val_acc: 0.8588\n",
      "Epoch 52/250\n",
      "390/390 [==============================] - 151s 388ms/step - loss: 0.7465 - acc: 0.8100 - val_loss: 0.5579 - val_acc: 0.8593\n",
      "Epoch 53/250\n",
      "390/390 [==============================] - 151s 388ms/step - loss: 0.7274 - acc: 0.8114 - val_loss: 0.6038 - val_acc: 0.8449\n",
      "Epoch 54/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7677 - acc: 0.8063 - val_loss: 0.6142 - val_acc: 0.8436\n",
      "Epoch 55/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7397 - acc: 0.8116 - val_loss: 0.5617 - val_acc: 0.8570\n",
      "Epoch 56/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7410 - acc: 0.8100 - val_loss: 0.5782 - val_acc: 0.8564\n",
      "Epoch 57/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.7385 - acc: 0.8117 - val_loss: 0.5911 - val_acc: 0.8505\n",
      "Epoch 58/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7209 - acc: 0.8128 - val_loss: 0.5822 - val_acc: 0.8581\n",
      "Epoch 59/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.7334 - acc: 0.8138 - val_loss: 0.5526 - val_acc: 0.8603\n",
      "Epoch 60/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7242 - acc: 0.8142 - val_loss: 0.5849 - val_acc: 0.8519\n",
      "Epoch 61/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7310 - acc: 0.8127 - val_loss: 0.5771 - val_acc: 0.8517\n",
      "Epoch 62/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7271 - acc: 0.8140 - val_loss: 0.5835 - val_acc: 0.8573\n",
      "Epoch 63/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.7344 - acc: 0.8147 - val_loss: 0.5623 - val_acc: 0.8591\n",
      "Epoch 64/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7208 - acc: 0.8131 - val_loss: 0.6005 - val_acc: 0.8474\n",
      "Epoch 65/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.7295 - acc: 0.8145 - val_loss: 0.5478 - val_acc: 0.8658\n",
      "Epoch 66/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7205 - acc: 0.8166 - val_loss: 0.5732 - val_acc: 0.8515\n",
      "Epoch 67/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.7202 - acc: 0.8177 - val_loss: 0.5824 - val_acc: 0.8591\n",
      "Epoch 68/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7201 - acc: 0.8168 - val_loss: 0.5585 - val_acc: 0.8619\n",
      "Epoch 69/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.7141 - acc: 0.8175 - val_loss: 0.6014 - val_acc: 0.8556\n",
      "Epoch 70/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7182 - acc: 0.8186 - val_loss: 0.5921 - val_acc: 0.8575\n",
      "Epoch 71/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7110 - acc: 0.8179 - val_loss: 0.5395 - val_acc: 0.8694\n",
      "Epoch 72/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.6989 - acc: 0.8210 - val_loss: 0.5662 - val_acc: 0.8603\n",
      "Epoch 73/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.7040 - acc: 0.8221 - val_loss: 0.5609 - val_acc: 0.8650\n",
      "Epoch 74/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.6987 - acc: 0.8195 - val_loss: 0.5930 - val_acc: 0.8519\n",
      "Epoch 75/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.6961 - acc: 0.8238 - val_loss: 0.5431 - val_acc: 0.8666\n",
      "Epoch 76/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.6533 - acc: 0.8330 - val_loss: 0.5148 - val_acc: 0.8762\n",
      "Epoch 77/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.6347 - acc: 0.8394 - val_loss: 0.5004 - val_acc: 0.8818\n",
      "Epoch 78/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.6305 - acc: 0.8409 - val_loss: 0.5053 - val_acc: 0.8803\n",
      "Epoch 79/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.6196 - acc: 0.8413 - val_loss: 0.5077 - val_acc: 0.8788\n",
      "Epoch 80/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.6129 - acc: 0.8448 - val_loss: 0.4916 - val_acc: 0.8808\n",
      "Epoch 81/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.6184 - acc: 0.8418 - val_loss: 0.5032 - val_acc: 0.8767\n",
      "Epoch 82/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.6142 - acc: 0.8455 - val_loss: 0.4992 - val_acc: 0.8775\n",
      "Epoch 83/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.6171 - acc: 0.8451 - val_loss: 0.4844 - val_acc: 0.8808\n",
      "Epoch 84/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.6130 - acc: 0.8453 - val_loss: 0.4899 - val_acc: 0.8808\n",
      "Epoch 85/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.6121 - acc: 0.8457 - val_loss: 0.4904 - val_acc: 0.8814\n",
      "Epoch 86/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.5953 - acc: 0.8486 - val_loss: 0.4730 - val_acc: 0.8833\n",
      "Epoch 87/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5972 - acc: 0.8469 - val_loss: 0.4764 - val_acc: 0.8820\n",
      "Epoch 88/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.6041 - acc: 0.8462 - val_loss: 0.5025 - val_acc: 0.8782\n",
      "Epoch 89/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5887 - acc: 0.8479 - val_loss: 0.4772 - val_acc: 0.8811\n",
      "Epoch 90/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5965 - acc: 0.8454 - val_loss: 0.4880 - val_acc: 0.8836\n",
      "Epoch 91/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5916 - acc: 0.8473 - val_loss: 0.4718 - val_acc: 0.8844\n",
      "Epoch 92/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5906 - acc: 0.8489 - val_loss: 0.4795 - val_acc: 0.8803\n",
      "Epoch 93/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5864 - acc: 0.8490 - val_loss: 0.4707 - val_acc: 0.8852\n",
      "Epoch 94/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5910 - acc: 0.8500 - val_loss: 0.4623 - val_acc: 0.8876\n",
      "Epoch 95/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.5893 - acc: 0.8482 - val_loss: 0.4962 - val_acc: 0.8780\n",
      "Epoch 96/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5817 - acc: 0.8498 - val_loss: 0.4829 - val_acc: 0.8783\n",
      "Epoch 97/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.5853 - acc: 0.8501 - val_loss: 0.4820 - val_acc: 0.8802\n",
      "Epoch 98/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5754 - acc: 0.8505 - val_loss: 0.4649 - val_acc: 0.8855\n",
      "Epoch 99/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.5756 - acc: 0.8507 - val_loss: 0.4841 - val_acc: 0.8822\n",
      "Epoch 100/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5713 - acc: 0.8512 - val_loss: 0.4757 - val_acc: 0.8854\n",
      "Epoch 101/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.5601 - acc: 0.8549 - val_loss: 0.4440 - val_acc: 0.8912\n",
      "Epoch 102/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5436 - acc: 0.8610 - val_loss: 0.4473 - val_acc: 0.8918\n",
      "Epoch 103/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.5513 - acc: 0.8566 - val_loss: 0.4710 - val_acc: 0.8861\n",
      "Epoch 104/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5385 - acc: 0.8635 - val_loss: 0.4469 - val_acc: 0.8905\n",
      "Epoch 105/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.5465 - acc: 0.8597 - val_loss: 0.4358 - val_acc: 0.8918\n",
      "Epoch 106/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5331 - acc: 0.8618 - val_loss: 0.4478 - val_acc: 0.8921\n",
      "Epoch 107/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5450 - acc: 0.8615 - val_loss: 0.4447 - val_acc: 0.8909\n",
      "Epoch 108/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5324 - acc: 0.8619 - val_loss: 0.4516 - val_acc: 0.8861\n",
      "Epoch 109/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.5253 - acc: 0.8651 - val_loss: 0.4420 - val_acc: 0.8910\n",
      "Epoch 110/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5280 - acc: 0.8645 - val_loss: 0.4367 - val_acc: 0.8924\n",
      "Epoch 111/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.5257 - acc: 0.8646 - val_loss: 0.4337 - val_acc: 0.8912\n",
      "Epoch 112/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5271 - acc: 0.8634 - val_loss: 0.4486 - val_acc: 0.8881\n",
      "Epoch 113/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.5326 - acc: 0.8624 - val_loss: 0.4426 - val_acc: 0.8921\n",
      "Epoch 114/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.5271 - acc: 0.8639 - val_loss: 0.4402 - val_acc: 0.8913\n",
      "Epoch 115/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5200 - acc: 0.8650 - val_loss: 0.4504 - val_acc: 0.8871\n",
      "Epoch 116/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.5238 - acc: 0.8645 - val_loss: 0.4257 - val_acc: 0.8924\n",
      "Epoch 117/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5129 - acc: 0.8646 - val_loss: 0.4404 - val_acc: 0.8895\n",
      "Epoch 118/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5104 - acc: 0.8669 - val_loss: 0.4286 - val_acc: 0.8939\n",
      "Epoch 119/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5167 - acc: 0.8657 - val_loss: 0.4324 - val_acc: 0.8929\n",
      "Epoch 120/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.5191 - acc: 0.8634 - val_loss: 0.4302 - val_acc: 0.8926\n",
      "Epoch 121/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.5078 - acc: 0.8670 - val_loss: 0.4280 - val_acc: 0.8932\n",
      "Epoch 122/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5073 - acc: 0.8676 - val_loss: 0.4491 - val_acc: 0.8842\n",
      "Epoch 123/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.5094 - acc: 0.8648 - val_loss: 0.4348 - val_acc: 0.8922\n",
      "Epoch 124/250\n",
      "390/390 [==============================] - 151s 386ms/step - loss: 0.5093 - acc: 0.8678 - val_loss: 0.4241 - val_acc: 0.8952\n",
      "Epoch 125/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.5110 - acc: 0.8656 - val_loss: 0.4235 - val_acc: 0.8941\n",
      "Epoch 126/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4944 - acc: 0.8697 - val_loss: 0.4176 - val_acc: 0.8961\n",
      "Epoch 127/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4844 - acc: 0.8712 - val_loss: 0.4103 - val_acc: 0.8981\n",
      "Epoch 128/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4929 - acc: 0.8717 - val_loss: 0.4085 - val_acc: 0.9005\n",
      "Epoch 129/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4832 - acc: 0.8735 - val_loss: 0.4064 - val_acc: 0.8992\n",
      "Epoch 130/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4796 - acc: 0.8728 - val_loss: 0.4111 - val_acc: 0.8991\n",
      "Epoch 131/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4854 - acc: 0.8730 - val_loss: 0.4134 - val_acc: 0.8958\n",
      "Epoch 132/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4709 - acc: 0.8761 - val_loss: 0.4135 - val_acc: 0.8980\n",
      "Epoch 133/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4762 - acc: 0.8753 - val_loss: 0.4077 - val_acc: 0.8970\n",
      "Epoch 134/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4778 - acc: 0.8751 - val_loss: 0.4094 - val_acc: 0.8992\n",
      "Epoch 135/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4733 - acc: 0.8764 - val_loss: 0.4021 - val_acc: 0.8991\n",
      "Epoch 136/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4734 - acc: 0.8757 - val_loss: 0.4050 - val_acc: 0.8986\n",
      "Epoch 137/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4797 - acc: 0.8740 - val_loss: 0.4082 - val_acc: 0.8968\n",
      "Epoch 138/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4680 - acc: 0.8774 - val_loss: 0.4085 - val_acc: 0.8989\n",
      "Epoch 139/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4694 - acc: 0.8762 - val_loss: 0.4076 - val_acc: 0.8980\n",
      "Epoch 140/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4655 - acc: 0.8767 - val_loss: 0.4038 - val_acc: 0.8994\n",
      "Epoch 141/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4672 - acc: 0.8783 - val_loss: 0.4033 - val_acc: 0.8993\n",
      "Epoch 142/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4653 - acc: 0.8786 - val_loss: 0.4080 - val_acc: 0.8982\n",
      "Epoch 143/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4662 - acc: 0.8780 - val_loss: 0.4041 - val_acc: 0.8974\n",
      "Epoch 144/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4581 - acc: 0.8784 - val_loss: 0.4007 - val_acc: 0.8994\n",
      "Epoch 145/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4682 - acc: 0.8768 - val_loss: 0.3926 - val_acc: 0.9025\n",
      "Epoch 146/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4675 - acc: 0.8773 - val_loss: 0.4023 - val_acc: 0.8991\n",
      "Epoch 147/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4656 - acc: 0.8782 - val_loss: 0.3987 - val_acc: 0.9009\n",
      "Epoch 148/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4671 - acc: 0.8786 - val_loss: 0.4059 - val_acc: 0.8996\n",
      "Epoch 149/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 0.4552 - acc: 0.8775 - val_loss: 0.4030 - val_acc: 0.9004\n",
      "Epoch 150/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4580 - acc: 0.8793 - val_loss: 0.4005 - val_acc: 0.9003\n",
      "Epoch 151/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 151s 387ms/step - loss: 0.4607 - acc: 0.8783 - val_loss: 0.4059 - val_acc: 0.8984\n",
      "Epoch 152/250\n",
      "390/390 [==============================] - 153s 392ms/step - loss: 0.4492 - acc: 0.8814 - val_loss: 0.3894 - val_acc: 0.9032\n",
      "Epoch 153/250\n",
      "390/390 [==============================] - 152s 389ms/step - loss: 0.4503 - acc: 0.8802 - val_loss: 0.3909 - val_acc: 0.9024\n",
      "Epoch 154/250\n",
      "390/390 [==============================] - 151s 388ms/step - loss: 0.4454 - acc: 0.8822 - val_loss: 0.3923 - val_acc: 0.9018\n",
      "Epoch 155/250\n",
      "390/390 [==============================] - 151s 388ms/step - loss: 0.4506 - acc: 0.8808 - val_loss: 0.3891 - val_acc: 0.9003\n",
      "Epoch 156/250\n",
      "390/390 [==============================] - 151s 388ms/step - loss: 0.4438 - acc: 0.8824 - val_loss: 0.3917 - val_acc: 0.9016\n",
      "Epoch 157/250\n",
      "390/390 [==============================] - 151s 386ms/step - loss: 0.4510 - acc: 0.8817 - val_loss: 0.3913 - val_acc: 0.9025\n",
      "Epoch 158/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4477 - acc: 0.8821 - val_loss: 0.3866 - val_acc: 0.9041\n",
      "Epoch 159/250\n",
      "390/390 [==============================] - 151s 388ms/step - loss: 0.4458 - acc: 0.8828 - val_loss: 0.3881 - val_acc: 0.9037\n",
      "Epoch 160/250\n",
      "390/390 [==============================] - 151s 387ms/step - loss: 0.4471 - acc: 0.8826 - val_loss: 0.3872 - val_acc: 0.9028\n",
      "Epoch 161/250\n",
      "390/390 [==============================] - 151s 387ms/step - loss: 0.4432 - acc: 0.8817 - val_loss: 0.3897 - val_acc: 0.9027\n",
      "Epoch 162/250\n",
      "390/390 [==============================] - 151s 387ms/step - loss: 0.4501 - acc: 0.8809 - val_loss: 0.3891 - val_acc: 0.9017\n",
      "Epoch 163/250\n",
      "390/390 [==============================] - 163s 418ms/step - loss: 0.4477 - acc: 0.8837 - val_loss: 0.3925 - val_acc: 0.9000\n",
      "Epoch 164/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4352 - acc: 0.8840 - val_loss: 0.3939 - val_acc: 0.9003\n",
      "Epoch 165/250\n",
      "390/390 [==============================] - 153s 392ms/step - loss: 0.4457 - acc: 0.8846 - val_loss: 0.3868 - val_acc: 0.9054\n",
      "Epoch 166/250\n",
      "390/390 [==============================] - 157s 403ms/step - loss: 0.4419 - acc: 0.8840 - val_loss: 0.3854 - val_acc: 0.9026\n",
      "Epoch 167/250\n",
      "390/390 [==============================] - 152s 389ms/step - loss: 0.4436 - acc: 0.8835 - val_loss: 0.3859 - val_acc: 0.9037\n",
      "Epoch 168/250\n",
      "390/390 [==============================] - 152s 389ms/step - loss: 0.4362 - acc: 0.8854 - val_loss: 0.3852 - val_acc: 0.9029\n",
      "Epoch 169/250\n",
      "390/390 [==============================] - 153s 391ms/step - loss: 0.4415 - acc: 0.8821 - val_loss: 0.3849 - val_acc: 0.9041\n",
      "Epoch 170/250\n",
      "390/390 [==============================] - 152s 390ms/step - loss: 0.4369 - acc: 0.8830 - val_loss: 0.3851 - val_acc: 0.9034\n",
      "Epoch 171/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4394 - acc: 0.8829 - val_loss: 0.3862 - val_acc: 0.9031\n",
      "Epoch 172/250\n",
      "390/390 [==============================] - 152s 390ms/step - loss: 0.4356 - acc: 0.8847 - val_loss: 0.3832 - val_acc: 0.9039\n",
      "Epoch 173/250\n",
      "390/390 [==============================] - 153s 391ms/step - loss: 0.4382 - acc: 0.8844 - val_loss: 0.3853 - val_acc: 0.9028\n",
      "Epoch 174/250\n",
      "390/390 [==============================] - 152s 390ms/step - loss: 0.4344 - acc: 0.8861 - val_loss: 0.3901 - val_acc: 0.9039\n",
      "Epoch 175/250\n",
      "390/390 [==============================] - 152s 390ms/step - loss: 0.4374 - acc: 0.8860 - val_loss: 0.3843 - val_acc: 0.9034\n",
      "Epoch 176/250\n",
      "390/390 [==============================] - 152s 391ms/step - loss: 0.4265 - acc: 0.8886 - val_loss: 0.3860 - val_acc: 0.9032\n",
      "Epoch 177/250\n",
      "390/390 [==============================] - 152s 391ms/step - loss: 0.4369 - acc: 0.8857 - val_loss: 0.3814 - val_acc: 0.9030\n",
      "Epoch 178/250\n",
      "390/390 [==============================] - 152s 390ms/step - loss: 0.4316 - acc: 0.8864 - val_loss: 0.3828 - val_acc: 0.9027\n",
      "Epoch 179/250\n",
      "390/390 [==============================] - 153s 393ms/step - loss: 0.4217 - acc: 0.8892 - val_loss: 0.3823 - val_acc: 0.9026\n",
      "Epoch 180/250\n",
      "390/390 [==============================] - 153s 392ms/step - loss: 0.4346 - acc: 0.8844 - val_loss: 0.3807 - val_acc: 0.9029\n",
      "Epoch 181/250\n",
      "390/390 [==============================] - 152s 390ms/step - loss: 0.4319 - acc: 0.8852 - val_loss: 0.3838 - val_acc: 0.9027\n",
      "Epoch 182/250\n",
      "390/390 [==============================] - 153s 392ms/step - loss: 0.4275 - acc: 0.8876 - val_loss: 0.3810 - val_acc: 0.9041\n",
      "Epoch 183/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4293 - acc: 0.8875 - val_loss: 0.3820 - val_acc: 0.9043\n",
      "Epoch 184/250\n",
      "390/390 [==============================] - 151s 388ms/step - loss: 0.4264 - acc: 0.8870 - val_loss: 0.3802 - val_acc: 0.9032\n",
      "Epoch 185/250\n",
      "390/390 [==============================] - 151s 388ms/step - loss: 0.4263 - acc: 0.8865 - val_loss: 0.3815 - val_acc: 0.9031\n",
      "Epoch 186/250\n",
      "390/390 [==============================] - 151s 387ms/step - loss: 0.4337 - acc: 0.8851 - val_loss: 0.3805 - val_acc: 0.9024\n",
      "Epoch 187/250\n",
      "390/390 [==============================] - 151s 387ms/step - loss: 0.4205 - acc: 0.8881 - val_loss: 0.3809 - val_acc: 0.9030\n",
      "Epoch 188/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4230 - acc: 0.8878 - val_loss: 0.3808 - val_acc: 0.9033\n",
      "Epoch 189/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4274 - acc: 0.8885 - val_loss: 0.3825 - val_acc: 0.9036\n",
      "Epoch 190/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4264 - acc: 0.8882 - val_loss: 0.3806 - val_acc: 0.9029\n",
      "Epoch 191/250\n",
      "390/390 [==============================] - 150s 385ms/step - loss: 0.4297 - acc: 0.8857 - val_loss: 0.3772 - val_acc: 0.9037\n",
      "Epoch 192/250\n",
      "390/390 [==============================] - 153s 393ms/step - loss: 0.4217 - acc: 0.8888 - val_loss: 0.3802 - val_acc: 0.9037\n",
      "Epoch 193/250\n",
      "390/390 [==============================] - 153s 392ms/step - loss: 0.4214 - acc: 0.8875 - val_loss: 0.3770 - val_acc: 0.9033\n",
      "Epoch 194/250\n",
      "390/390 [==============================] - 155s 397ms/step - loss: 0.4186 - acc: 0.8887 - val_loss: 0.3815 - val_acc: 0.9022\n",
      "Epoch 195/250\n",
      "101/390 [======>.......................] - ETA: 1:49 - loss: 0.4259 - acc: 0.8858"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m                 \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    607\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mworker\u001b[1;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget_index\u001b[1;34m(uid, i)\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0m_SHARED_SEQUENCES\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_SHARED_SEQUENCES\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    760\u001b[0m                                        self.batch_size * (idx + 1)]\n\u001b[1;32m--> 761\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    762\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[1;34m(self, index_array)\u001b[0m\n\u001b[0;32m    870\u001b[0m         batch_x = np.zeros(tuple([len(index_array)] + list(self.x.shape)[1:]),\n\u001b[1;32m--> 871\u001b[1;33m                            dtype=K.floatx())\n\u001b[0m\u001b[0;32m    872\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-6435fc5d5a97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m his = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,\n\u001b[1;32m---> 15\u001b[1;33m                     epochs=epochs,verbose=1,validation_data=(x_test,y_test), callbacks=[lr_1])\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;31m# Save the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'swish_at_3.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1225\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1227\u001b[1;33m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1229\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2113\u001b[0m                 \u001b[0mbatch_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2114\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2115\u001b[1;33m                     \u001b[0mgenerator_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2117\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    555\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m             \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_send_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create model and get ready - reproducible cell\n",
    "act, act_name = swish, \"swish\"\n",
    "model = create(act)\n",
    "opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "\n",
    "# Training - only for 125 eppochs\n",
    "batch_size = 128\n",
    "epochs=250\n",
    "# lr scheduler\n",
    "lr_1 = keras.callbacks.LearningRateScheduler(schedule)\n",
    "\n",
    "# Fit the model\n",
    "his = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,verbose=1,validation_data=(x_test,y_test), callbacks=[lr_1])\n",
    "# Save the model\n",
    "model.save('swish_at_3.h5')\n",
    "# Display logs\n",
    "print(his.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 195/250\n",
      "390/390 [==============================] - 155s 396ms/step - loss: 0.4274 - acc: 0.8867 - val_loss: 0.3809 - val_acc: 0.9027\n",
      "Epoch 196/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4235 - acc: 0.8865 - val_loss: 0.3774 - val_acc: 0.9031\n",
      "Epoch 197/250\n",
      "390/390 [==============================] - 153s 391ms/step - loss: 0.4232 - acc: 0.8900 - val_loss: 0.3790 - val_acc: 0.9041\n",
      "Epoch 198/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4214 - acc: 0.8881 - val_loss: 0.3795 - val_acc: 0.9042\n",
      "Epoch 199/250\n",
      "390/390 [==============================] - 154s 396ms/step - loss: 0.4217 - acc: 0.8890 - val_loss: 0.3781 - val_acc: 0.9037\n",
      "Epoch 200/250\n",
      "390/390 [==============================] - 151s 388ms/step - loss: 0.4260 - acc: 0.8870 - val_loss: 0.3800 - val_acc: 0.9041\n",
      "Epoch 201/250\n",
      "390/390 [==============================] - 153s 392ms/step - loss: 0.4196 - acc: 0.8889 - val_loss: 0.3769 - val_acc: 0.9046\n",
      "Epoch 202/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4254 - acc: 0.8864 - val_loss: 0.3769 - val_acc: 0.9042\n",
      "Epoch 203/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4228 - acc: 0.8877 - val_loss: 0.3789 - val_acc: 0.9046\n",
      "Epoch 204/250\n",
      "390/390 [==============================] - 156s 401ms/step - loss: 0.4245 - acc: 0.8876 - val_loss: 0.3789 - val_acc: 0.9039\n",
      "Epoch 205/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4176 - acc: 0.8877 - val_loss: 0.3784 - val_acc: 0.9034\n",
      "Epoch 206/250\n",
      "390/390 [==============================] - 153s 393ms/step - loss: 0.4190 - acc: 0.8886 - val_loss: 0.3772 - val_acc: 0.9044\n",
      "Epoch 207/250\n",
      "390/390 [==============================] - 155s 397ms/step - loss: 0.4167 - acc: 0.8885 - val_loss: 0.3787 - val_acc: 0.9044\n",
      "Epoch 208/250\n",
      "390/390 [==============================] - 153s 393ms/step - loss: 0.4230 - acc: 0.8861 - val_loss: 0.3787 - val_acc: 0.9047\n",
      "Epoch 209/250\n",
      "390/390 [==============================] - 152s 391ms/step - loss: 0.4198 - acc: 0.8888 - val_loss: 0.3768 - val_acc: 0.9032\n",
      "Epoch 210/250\n",
      "390/390 [==============================] - 152s 391ms/step - loss: 0.4179 - acc: 0.8890 - val_loss: 0.3802 - val_acc: 0.9048\n",
      "Epoch 211/250\n",
      "390/390 [==============================] - 180s 462ms/step - loss: 0.4177 - acc: 0.8913 - val_loss: 0.3791 - val_acc: 0.9044\n",
      "Epoch 212/250\n",
      "390/390 [==============================] - 153s 392ms/step - loss: 0.4149 - acc: 0.8887 - val_loss: 0.3782 - val_acc: 0.9053\n",
      "Epoch 213/250\n",
      "390/390 [==============================] - 153s 391ms/step - loss: 0.4192 - acc: 0.8881 - val_loss: 0.3769 - val_acc: 0.9053\n",
      "Epoch 214/250\n",
      "390/390 [==============================] - 152s 390ms/step - loss: 0.4174 - acc: 0.8885 - val_loss: 0.3773 - val_acc: 0.9044\n",
      "Epoch 215/250\n",
      "390/390 [==============================] - 152s 391ms/step - loss: 0.4217 - acc: 0.8880 - val_loss: 0.3770 - val_acc: 0.9035\n",
      "Epoch 216/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4172 - acc: 0.8909 - val_loss: 0.3761 - val_acc: 0.9050\n",
      "Epoch 217/250\n",
      "390/390 [==============================] - 153s 394ms/step - loss: 0.4163 - acc: 0.8887 - val_loss: 0.3762 - val_acc: 0.9047\n",
      "Epoch 218/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4229 - acc: 0.8880 - val_loss: 0.3808 - val_acc: 0.9040\n",
      "Epoch 219/250\n",
      "390/390 [==============================] - 153s 394ms/step - loss: 0.4227 - acc: 0.8872 - val_loss: 0.3775 - val_acc: 0.9039\n",
      "Epoch 220/250\n",
      "390/390 [==============================] - 153s 392ms/step - loss: 0.4204 - acc: 0.8892 - val_loss: 0.3767 - val_acc: 0.9031\n",
      "Epoch 221/250\n",
      "390/390 [==============================] - 153s 392ms/step - loss: 0.4183 - acc: 0.8887 - val_loss: 0.3776 - val_acc: 0.9033\n",
      "Epoch 222/250\n",
      "390/390 [==============================] - 152s 391ms/step - loss: 0.4151 - acc: 0.8895 - val_loss: 0.3769 - val_acc: 0.9029\n",
      "Epoch 223/250\n",
      "390/390 [==============================] - 152s 391ms/step - loss: 0.4096 - acc: 0.8894 - val_loss: 0.3775 - val_acc: 0.9038\n",
      "Epoch 224/250\n",
      "390/390 [==============================] - 152s 390ms/step - loss: 0.4236 - acc: 0.8862 - val_loss: 0.3784 - val_acc: 0.9032\n",
      "Epoch 225/250\n",
      "390/390 [==============================] - 152s 390ms/step - loss: 0.4150 - acc: 0.8903 - val_loss: 0.3776 - val_acc: 0.9037\n",
      "Epoch 226/250\n",
      "390/390 [==============================] - 152s 390ms/step - loss: 0.4156 - acc: 0.8905 - val_loss: 0.3743 - val_acc: 0.9048\n",
      "Epoch 227/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4156 - acc: 0.8879 - val_loss: 0.3746 - val_acc: 0.9051\n",
      "Epoch 228/250\n",
      "390/390 [==============================] - 153s 393ms/step - loss: 0.4133 - acc: 0.8908 - val_loss: 0.3765 - val_acc: 0.9049\n",
      "Epoch 229/250\n",
      "390/390 [==============================] - 152s 390ms/step - loss: 0.4146 - acc: 0.8908 - val_loss: 0.3769 - val_acc: 0.9040\n",
      "Epoch 230/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4145 - acc: 0.8910 - val_loss: 0.3768 - val_acc: 0.9041\n",
      "Epoch 231/250\n",
      "390/390 [==============================] - 153s 392ms/step - loss: 0.4158 - acc: 0.8897 - val_loss: 0.3776 - val_acc: 0.9046\n",
      "Epoch 232/250\n",
      "390/390 [==============================] - 153s 392ms/step - loss: 0.4158 - acc: 0.8882 - val_loss: 0.3761 - val_acc: 0.9033\n",
      "Epoch 233/250\n",
      "390/390 [==============================] - 152s 391ms/step - loss: 0.4128 - acc: 0.8890 - val_loss: 0.3767 - val_acc: 0.9052\n",
      "Epoch 234/250\n",
      "390/390 [==============================] - 151s 387ms/step - loss: 0.4170 - acc: 0.8887 - val_loss: 0.3762 - val_acc: 0.9046\n",
      "Epoch 235/250\n",
      "390/390 [==============================] - 151s 388ms/step - loss: 0.4162 - acc: 0.8914 - val_loss: 0.3770 - val_acc: 0.9042\n",
      "Epoch 236/250\n",
      "390/390 [==============================] - 151s 387ms/step - loss: 0.4121 - acc: 0.8912 - val_loss: 0.3768 - val_acc: 0.9045\n",
      "Epoch 237/250\n",
      "390/390 [==============================] - 153s 393ms/step - loss: 0.4196 - acc: 0.8874 - val_loss: 0.3745 - val_acc: 0.9046\n",
      "Epoch 238/250\n",
      "390/390 [==============================] - 153s 393ms/step - loss: 0.4158 - acc: 0.8895 - val_loss: 0.3744 - val_acc: 0.9051\n",
      "Epoch 239/250\n",
      "390/390 [==============================] - 152s 391ms/step - loss: 0.4136 - acc: 0.8900 - val_loss: 0.3757 - val_acc: 0.9050\n",
      "Epoch 240/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4102 - acc: 0.8907 - val_loss: 0.3749 - val_acc: 0.9052\n",
      "Epoch 241/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4164 - acc: 0.8908 - val_loss: 0.3775 - val_acc: 0.9049\n",
      "Epoch 242/250\n",
      "390/390 [==============================] - 153s 393ms/step - loss: 0.4113 - acc: 0.8910 - val_loss: 0.3759 - val_acc: 0.9040\n",
      "Epoch 243/250\n",
      "390/390 [==============================] - 152s 391ms/step - loss: 0.4134 - acc: 0.8884 - val_loss: 0.3764 - val_acc: 0.9042\n",
      "Epoch 244/250\n",
      "390/390 [==============================] - 153s 391ms/step - loss: 0.4140 - acc: 0.8896 - val_loss: 0.3752 - val_acc: 0.9045\n",
      "Epoch 245/250\n",
      "390/390 [==============================] - 153s 393ms/step - loss: 0.4085 - acc: 0.8919 - val_loss: 0.3754 - val_acc: 0.9046\n",
      "Epoch 246/250\n",
      "390/390 [==============================] - 155s 397ms/step - loss: 0.4109 - acc: 0.8895 - val_loss: 0.3764 - val_acc: 0.9035\n",
      "Epoch 247/250\n",
      "390/390 [==============================] - 152s 391ms/step - loss: 0.4145 - acc: 0.8910 - val_loss: 0.3773 - val_acc: 0.9046\n",
      "Epoch 248/250\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 0.4094 - acc: 0.8910 - val_loss: 0.3773 - val_acc: 0.9052\n",
      "Epoch 249/250\n",
      "390/390 [==============================] - 155s 396ms/step - loss: 0.4137 - acc: 0.8919 - val_loss: 0.3756 - val_acc: 0.9052\n",
      "Epoch 250/250\n",
      "390/390 [==============================] - 154s 394ms/step - loss: 0.4158 - acc: 0.8898 - val_loss: 0.3764 - val_acc: 0.9043\n",
      "{'loss': [0.42736591107576827, 0.42361747660161136, 0.42311377273104611, 0.42147118119922705, 0.42160200728488956, 0.42600957449330784, 0.41950118040709605, 0.42541198789557089, 0.42289348014050432, 0.42439042618345824, 0.41743921531788003, 0.41912914580705268, 0.41670897996001111, 0.4229592672525308, 0.41957258734093617, 0.41791497504133651, 0.41768184573863765, 0.41487685794397389, 0.41893954698879593, 0.41731131751834866, 0.42182604434553689, 0.41729834920556108, 0.41634731345120096, 0.42287347672036874, 0.42270558933856817, 0.42027598470306027, 0.41838503157362023, 0.41512466962803318, 0.40978968724307241, 0.42374543432130773, 0.41501511246644807, 0.41573981847524416, 0.41557714002691554, 0.41327204276353885, 0.41474434646299885, 0.41451695096733709, 0.41589793780884915, 0.41588689298403353, 0.41275330283223416, 0.41703742060823473, 0.41626410563480865, 0.41218234296980139, 0.41952734816560083, 0.4157894280858529, 0.41354787993171027, 0.40997884972361931, 0.41638916751915422, 0.41125042024697483, 0.41346816515402418, 0.4140684687731776, 0.40846215314590012, 0.41093260214272931, 0.41440584079416881, 0.40943719666089085, 0.41378215953521252, 0.41565658036890724], 'val_loss': [0.38088713307380678, 0.37742783207893371, 0.37902091193199156, 0.3794846113204956, 0.37810357868671418, 0.38000050787925721, 0.37688323235511778, 0.37691866998672485, 0.37894276747703554, 0.37893698806762693, 0.378410378074646, 0.37720713467597961, 0.37869062676429749, 0.37869433007240294, 0.37681407442092896, 0.38017564320564268, 0.37905069117546081, 0.37821787242889404, 0.37690585675239563, 0.37730798220634459, 0.37704683861732485, 0.37614070692062379, 0.3762400686264038, 0.38075507221221921, 0.37752712478637696, 0.37666785120964053, 0.37764133720397947, 0.37692554392814637, 0.37747814779281619, 0.37837951045036317, 0.37764669251441957, 0.37425794653892519, 0.37462101230621336, 0.37646334600448611, 0.37690650424957273, 0.37678988690376281, 0.37755104074478152, 0.37612463812828062, 0.3766683038711548, 0.37622128429412843, 0.37702558937072755, 0.37682935872077944, 0.37450999126434326, 0.3744369225025177, 0.37574220390319824, 0.37493446455001833, 0.37754197945594786, 0.37594537429809571, 0.37638072562217711, 0.37520519165992738, 0.37540770578384397, 0.37641409044265745, 0.37725325574874879, 0.37729198694229127, 0.37560435285568239, 0.37644505438804626], 'acc': [0.88666987487969207, 0.88648941285877148, 0.89005854984946764, 0.88805341670862714, 0.88897577801078109, 0.88693054220712375, 0.88891562403567836, 0.88638915620172942, 0.88771254407468425, 0.88767244148835722, 0.88773259548258243, 0.888554700070327, 0.88855469999383729, 0.88607772435897436, 0.88884874761066301, 0.88893567534796425, 0.89128168112274775, 0.88865495667000172, 0.88817372475444489, 0.88851459735014293, 0.88799326271440193, 0.89082050048123196, 0.88865495665087924, 0.88801331408405515, 0.88715110685261622, 0.88923644526172307, 0.88863490539596068, 0.88951716393968561, 0.88933670197613235, 0.88616859161360428, 0.89027911457144993, 0.89041947385306386, 0.88787295474507388, 0.89078525641025641, 0.89071531793821601, 0.89098091117074407, 0.88971767729201456, 0.8881536734421589, 0.88907603468694552, 0.88871511070247178, 0.89136188646750381, 0.89120147581623654, 0.88735162010933288, 0.88950320512820513, 0.88999839593173224, 0.89075545921644184, 0.8907803978375376, 0.89098091111337674, 0.888354186756243, 0.88953721530933894, 0.89194711538461535, 0.88945700992633792, 0.89101637764932562, 0.89098091109425437, 0.89188322108412232, 0.88977783128623977], 'val_acc': [0.90269999999999995, 0.90310000000000001, 0.90410000000000001, 0.9042, 0.90370000028610231, 0.90410000000000001, 0.90459999999999996, 0.9042, 0.90459999999999996, 0.90390000000000004, 0.90339999999999998, 0.90439999999999998, 0.90439999999999998, 0.90469999999999995, 0.9032, 0.90480000000000005, 0.90439999999999998, 0.90529999999999999, 0.90529999999999999, 0.90439999999999998, 0.90349999999999997, 0.90500000000000003, 0.90469999999999995, 0.90400000000000003, 0.90390000000000004, 0.90310000000000001, 0.90329999999999999, 0.90290000000000004, 0.90380000000000005, 0.9032, 0.90369999999999995, 0.90480000000000005, 0.90510000000000002, 0.90490000000000004, 0.90400000000000003, 0.90410000000000001, 0.90459999999999996, 0.90329999999999999, 0.9052, 0.90459999999999996, 0.9042, 0.90449999999999997, 0.90459999999999996, 0.90510000000000002, 0.90500000000000003, 0.9052, 0.90490000000000004, 0.90400000000000003, 0.9042, 0.90449999999999997, 0.90459999999999996, 0.90349999999999997, 0.90459999999999996, 0.9052, 0.9052, 0.90429999999999999]}\n"
     ]
    }
   ],
   "source": [
    "# model.save(\"swish_at_3.h5\")\n",
    "# Create model and get ready - reproducible cell\n",
    "act, act_name = swish, \"swish\"\n",
    "model = load_model(\"swish_at_3.h5\", custom_objects={\"swish\": swish})\n",
    "opt_rms = keras.optimizers.rmsprop(lr=0.000035,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "\n",
    "# Training - only for 125 eppochs\n",
    "batch_size = 128\n",
    "epochs=250\n",
    "# lr scheduler\n",
    "lr_1 = keras.callbacks.LearningRateScheduler(schedule)\n",
    "\n",
    "# Fit the model\n",
    "his = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,verbose=1,validation_data=(x_test,y_test), callbacks=[lr_1], initial_epoch=194)\n",
    "# Save the model\n",
    "model.save('swish_at_3.h5')\n",
    "# Display logs\n",
    "print(his.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_test):\n",
    "    y_hat = np.argmax(y_pred, axis=1)\n",
    "    y = np.argmax(y_test, axis=1)\n",
    "\n",
    "    good = np.sum(np.equal(y, y_hat))\n",
    "    return float(good/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_25 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "0.9173\n"
     ]
    }
   ],
   "source": [
    "import keras.regularizers as regularizers\n",
    "ensembler = 0\n",
    "for i in range(3):\n",
    "    if i == 0:\n",
    "        model = create(swish)\n",
    "        model.load_weights('swish_at_'+str(i+1)+'.h5')\n",
    "        model.save('swish_at_'+str(i+1)+'.h5')\n",
    "    else:\n",
    "        model = load_model('swish_at_'+str(i+1)+'.h5', custom_objects={\"swish\": swish})\n",
    "    ensembler += model.predict_proba(x_test)\n",
    "    \n",
    "print(accuracy(ensembler, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
