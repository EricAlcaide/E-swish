{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Performing a large-scale experiment on cifar10 \"\"\"\n",
    "\n",
    "import keras\n",
    "import logging\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import cifar10\n",
    "import keras.regularizers as regularizers\n",
    "from keras.models import load_model\n",
    "\n",
    "# Record settings\n",
    "LOG_FORMAT = \"%(levelname)s %(asctime)s - %(message)s\"\n",
    "logging.basicConfig(filename=\"swish_first_exp_log.txt\",format = LOG_FORMAT, level = logging.DEBUG, filemode = \"a\")\n",
    "logs = logging.getLogger()\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#z-score\n",
    "mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "std = np.std(x_train,axis=(0,1,2,3))\n",
    "x_train = (x_train-mean)/(std+1e-7)\n",
    "x_test = (x_test-mean)/(std+1e-7)\n",
    "\n",
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train,num_classes)\n",
    "y_test = np_utils.to_categorical(y_test,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Swish activation function\n",
    "# x*sigmoid(x)\n",
    "def swish(x):\n",
    "    return x*K.sigmoid(x)\n",
    "\n",
    "# Custom activation function 1\n",
    "# mix between relu and positive part of swish mirrored across x=1\n",
    "def e_swish_1(x):\n",
    "    return K.maximum(0.0, x*(2-K.sigmoid(x)))\n",
    "\n",
    "# Custom activation function 2\n",
    "# positive part of swish mirrored across x=1\n",
    "def e_swish_2(x):\n",
    "    return K.maximum(x*K.sigmoid(x), x*(2-K.sigmoid(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create(act):\n",
    "    baseMapNum = 32\n",
    "    weight_decay = 1e-4\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(2*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(2*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(4*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(4*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False\n",
    "    )\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def schedule(x):\n",
    "    if x < 75: \n",
    "        return 0.001\n",
    "    elif x < 100:\n",
    "        return 0.0005\n",
    "    elif x < 125:\n",
    "        return 0.0003\n",
    "    elif x<150:\n",
    "        return 0.00015\n",
    "    elif x<175:\n",
    "        return 0.000075\n",
    "    elif x<200: \n",
    "        return 0.000035\n",
    "    elif x<225:\n",
    "        return 0.000017\n",
    "    else:\n",
    "        return 0.000012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Epoch 1/250\n",
      "390/390 [==============================] - 150s 386ms/step - loss: 2.0441 - acc: 0.3842 - val_loss: 1.4719 - val_acc: 0.5211\n",
      "Epoch 2/250\n",
      "390/390 [==============================] - 145s 372ms/step - loss: 1.4238 - acc: 0.5356 - val_loss: 1.0515 - val_acc: 0.6488\n",
      "Epoch 3/250\n",
      "390/390 [==============================] - 145s 373ms/step - loss: 1.2189 - acc: 0.6031 - val_loss: 0.9504 - val_acc: 0.7042\n",
      "Epoch 4/250\n",
      "390/390 [==============================] - 145s 373ms/step - loss: 1.0990 - acc: 0.6478 - val_loss: 0.8618 - val_acc: 0.7261\n",
      "Epoch 5/250\n",
      "390/390 [==============================] - 145s 373ms/step - loss: 1.0263 - acc: 0.6726 - val_loss: 0.8084 - val_acc: 0.7480\n",
      "Epoch 6/250\n",
      "390/390 [==============================] - 145s 372ms/step - loss: 0.9630 - acc: 0.6924 - val_loss: 0.7860 - val_acc: 0.7603\n",
      "Epoch 7/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.9235 - acc: 0.7081 - val_loss: 0.6973 - val_acc: 0.7842\n",
      "Epoch 8/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.8852 - acc: 0.7216 - val_loss: 0.6893 - val_acc: 0.7946\n",
      "Epoch 9/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.8515 - acc: 0.7357 - val_loss: 0.6928 - val_acc: 0.7892\n",
      "Epoch 10/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.8301 - acc: 0.7441 - val_loss: 0.6603 - val_acc: 0.8061\n",
      "Epoch 11/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.8091 - acc: 0.7520 - val_loss: 0.6253 - val_acc: 0.8187\n",
      "Epoch 12/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.7971 - acc: 0.7569 - val_loss: 0.6954 - val_acc: 0.8001\n",
      "Epoch 13/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.7799 - acc: 0.7634 - val_loss: 0.5979 - val_acc: 0.8261\n",
      "Epoch 14/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.7614 - acc: 0.7706 - val_loss: 0.6250 - val_acc: 0.8186\n",
      "Epoch 15/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.7527 - acc: 0.7751 - val_loss: 0.5733 - val_acc: 0.8386\n",
      "Epoch 16/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.7396 - acc: 0.7779 - val_loss: 0.5829 - val_acc: 0.8377\n",
      "Epoch 17/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.7265 - acc: 0.7854 - val_loss: 0.5548 - val_acc: 0.8456\n",
      "Epoch 18/250\n",
      "390/390 [==============================] - 145s 371ms/step - loss: 0.7186 - acc: 0.7886 - val_loss: 0.5842 - val_acc: 0.8381\n",
      "Epoch 19/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.7163 - acc: 0.7896 - val_loss: 0.5739 - val_acc: 0.8398\n",
      "Epoch 20/250\n",
      "390/390 [==============================] - 145s 371ms/step - loss: 0.7048 - acc: 0.7939 - val_loss: 0.6158 - val_acc: 0.8335\n",
      "Epoch 21/250\n",
      "390/390 [==============================] - 145s 371ms/step - loss: 0.6969 - acc: 0.7984 - val_loss: 0.5708 - val_acc: 0.8421\n",
      "Epoch 22/250\n",
      "390/390 [==============================] - 144s 371ms/step - loss: 0.6889 - acc: 0.8009 - val_loss: 0.5664 - val_acc: 0.8473\n",
      "Epoch 23/250\n",
      "390/390 [==============================] - 145s 371ms/step - loss: 0.6866 - acc: 0.8027 - val_loss: 0.5506 - val_acc: 0.8510\n",
      "Epoch 24/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.6791 - acc: 0.8056 - val_loss: 0.5621 - val_acc: 0.8458\n",
      "Epoch 25/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.6746 - acc: 0.8072 - val_loss: 0.5372 - val_acc: 0.8546\n",
      "Epoch 26/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.6681 - acc: 0.8083 - val_loss: 0.5762 - val_acc: 0.8479\n",
      "Epoch 27/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.6676 - acc: 0.8107 - val_loss: 0.5505 - val_acc: 0.8512\n",
      "Epoch 28/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.6618 - acc: 0.8104 - val_loss: 0.5256 - val_acc: 0.8609\n",
      "Epoch 29/250\n",
      "390/390 [==============================] - 145s 371ms/step - loss: 0.6612 - acc: 0.8126 - val_loss: 0.5718 - val_acc: 0.8452\n",
      "Epoch 30/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.6555 - acc: 0.8128 - val_loss: 0.5674 - val_acc: 0.8545\n",
      "Epoch 31/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.6514 - acc: 0.8168 - val_loss: 0.5625 - val_acc: 0.8516\n",
      "Epoch 32/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.6471 - acc: 0.8175 - val_loss: 0.5254 - val_acc: 0.8627\n",
      "Epoch 33/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.6470 - acc: 0.8197 - val_loss: 0.5403 - val_acc: 0.8556\n",
      "Epoch 34/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.6451 - acc: 0.8187 - val_loss: 0.5181 - val_acc: 0.8650\n",
      "Epoch 35/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.6431 - acc: 0.8182 - val_loss: 0.5238 - val_acc: 0.8636\n",
      "Epoch 36/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.6379 - acc: 0.8209 - val_loss: 0.5368 - val_acc: 0.8582\n",
      "Epoch 37/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.6329 - acc: 0.8231 - val_loss: 0.5277 - val_acc: 0.8629\n",
      "Epoch 38/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.6327 - acc: 0.8235 - val_loss: 0.5277 - val_acc: 0.8651\n",
      "Epoch 39/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.6304 - acc: 0.8246 - val_loss: 0.5318 - val_acc: 0.8627\n",
      "Epoch 40/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.6307 - acc: 0.8248 - val_loss: 0.5306 - val_acc: 0.8656\n",
      "Epoch 41/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.6274 - acc: 0.8278 - val_loss: 0.5323 - val_acc: 0.8643\n",
      "Epoch 42/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.6223 - acc: 0.8277 - val_loss: 0.5071 - val_acc: 0.8714\n",
      "Epoch 43/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.6218 - acc: 0.8302 - val_loss: 0.5173 - val_acc: 0.8649\n",
      "Epoch 44/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.6146 - acc: 0.8301 - val_loss: 0.5140 - val_acc: 0.8694\n",
      "Epoch 45/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.6198 - acc: 0.8296 - val_loss: 0.5175 - val_acc: 0.8683\n",
      "Epoch 46/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.6170 - acc: 0.8324 - val_loss: 0.4919 - val_acc: 0.8749\n",
      "Epoch 47/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.6108 - acc: 0.8323 - val_loss: 0.5256 - val_acc: 0.8659\n",
      "Epoch 48/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.6161 - acc: 0.8301 - val_loss: 0.5015 - val_acc: 0.8734\n",
      "Epoch 49/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.6109 - acc: 0.8328 - val_loss: 0.5102 - val_acc: 0.8717\n",
      "Epoch 50/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.6092 - acc: 0.8332 - val_loss: 0.5145 - val_acc: 0.8699\n",
      "Epoch 51/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.6080 - acc: 0.8348 - val_loss: 0.5603 - val_acc: 0.8636\n",
      "Epoch 52/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.6029 - acc: 0.8363 - val_loss: 0.5127 - val_acc: 0.8714\n",
      "Epoch 53/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.6085 - acc: 0.8351 - val_loss: 0.4978 - val_acc: 0.8766\n",
      "Epoch 54/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.5973 - acc: 0.8379 - val_loss: 0.5223 - val_acc: 0.8674\n",
      "Epoch 55/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.6008 - acc: 0.8366 - val_loss: 0.5106 - val_acc: 0.8746\n",
      "Epoch 56/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.6051 - acc: 0.8357 - val_loss: 0.5408 - val_acc: 0.8644\n",
      "Epoch 57/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.5980 - acc: 0.8393 - val_loss: 0.5141 - val_acc: 0.8680\n",
      "Epoch 58/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.5972 - acc: 0.8389 - val_loss: 0.5128 - val_acc: 0.8746\n",
      "Epoch 59/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.5945 - acc: 0.8382 - val_loss: 0.4964 - val_acc: 0.8768\n",
      "Epoch 60/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.5907 - acc: 0.8415 - val_loss: 0.5129 - val_acc: 0.8765\n",
      "Epoch 61/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.5982 - acc: 0.8372 - val_loss: 0.4905 - val_acc: 0.8772\n",
      "Epoch 62/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.5951 - acc: 0.8389 - val_loss: 0.5285 - val_acc: 0.8691\n",
      "Epoch 63/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.5974 - acc: 0.8370 - val_loss: 0.4888 - val_acc: 0.8815\n",
      "Epoch 64/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.5892 - acc: 0.8407 - val_loss: 0.5252 - val_acc: 0.8683\n",
      "Epoch 65/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.5923 - acc: 0.8400 - val_loss: 0.4876 - val_acc: 0.8810\n",
      "Epoch 66/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.5899 - acc: 0.8406 - val_loss: 0.5073 - val_acc: 0.8763\n",
      "Epoch 67/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.5891 - acc: 0.8426 - val_loss: 0.4959 - val_acc: 0.8780\n",
      "Epoch 68/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.5862 - acc: 0.8430 - val_loss: 0.5050 - val_acc: 0.8772\n",
      "Epoch 69/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.5914 - acc: 0.8410 - val_loss: 0.4895 - val_acc: 0.8812\n",
      "Epoch 70/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.5892 - acc: 0.8421 - val_loss: 0.4879 - val_acc: 0.8824\n",
      "Epoch 71/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.5862 - acc: 0.8419 - val_loss: 0.5059 - val_acc: 0.8743\n",
      "Epoch 72/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.5842 - acc: 0.8434 - val_loss: 0.4915 - val_acc: 0.8783\n",
      "Epoch 73/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.5826 - acc: 0.8449 - val_loss: 0.5082 - val_acc: 0.8733\n",
      "Epoch 74/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.5813 - acc: 0.8451 - val_loss: 0.5218 - val_acc: 0.8700\n",
      "Epoch 75/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.5816 - acc: 0.8450 - val_loss: 0.4915 - val_acc: 0.8785\n",
      "Epoch 76/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.5421 - acc: 0.8578 - val_loss: 0.4608 - val_acc: 0.8905\n",
      "Epoch 77/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.5309 - acc: 0.8612 - val_loss: 0.4514 - val_acc: 0.8920\n",
      "Epoch 78/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.5215 - acc: 0.8632 - val_loss: 0.4533 - val_acc: 0.8925\n",
      "Epoch 79/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.5188 - acc: 0.8634 - val_loss: 0.4519 - val_acc: 0.8911\n",
      "Epoch 80/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.5147 - acc: 0.8639 - val_loss: 0.4607 - val_acc: 0.8902\n",
      "Epoch 81/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.5144 - acc: 0.8652 - val_loss: 0.4347 - val_acc: 0.8941\n",
      "Epoch 82/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.5039 - acc: 0.8663 - val_loss: 0.4351 - val_acc: 0.8945\n",
      "Epoch 83/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.5048 - acc: 0.8671 - val_loss: 0.4459 - val_acc: 0.8901\n",
      "Epoch 84/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.5002 - acc: 0.8674 - val_loss: 0.4368 - val_acc: 0.8954\n",
      "Epoch 85/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.5013 - acc: 0.8669 - val_loss: 0.4411 - val_acc: 0.8957\n",
      "Epoch 86/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4961 - acc: 0.8680 - val_loss: 0.4401 - val_acc: 0.8938\n",
      "Epoch 87/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4984 - acc: 0.8678 - val_loss: 0.4317 - val_acc: 0.8958\n",
      "Epoch 88/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4963 - acc: 0.8674 - val_loss: 0.4217 - val_acc: 0.8972\n",
      "Epoch 89/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4971 - acc: 0.8665 - val_loss: 0.4300 - val_acc: 0.8946\n",
      "Epoch 90/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4877 - acc: 0.8701 - val_loss: 0.4226 - val_acc: 0.8973\n",
      "Epoch 91/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4855 - acc: 0.8720 - val_loss: 0.4363 - val_acc: 0.8944\n",
      "Epoch 92/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4901 - acc: 0.8698 - val_loss: 0.4319 - val_acc: 0.8958\n",
      "Epoch 93/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4876 - acc: 0.8706 - val_loss: 0.4279 - val_acc: 0.8962\n",
      "Epoch 94/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4809 - acc: 0.8711 - val_loss: 0.4216 - val_acc: 0.8980\n",
      "Epoch 95/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4864 - acc: 0.8692 - val_loss: 0.4304 - val_acc: 0.8971\n",
      "Epoch 96/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4841 - acc: 0.8692 - val_loss: 0.4368 - val_acc: 0.8938\n",
      "Epoch 97/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4802 - acc: 0.8717 - val_loss: 0.4323 - val_acc: 0.8953\n",
      "Epoch 98/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4823 - acc: 0.8695 - val_loss: 0.4223 - val_acc: 0.8978\n",
      "Epoch 99/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4767 - acc: 0.8717 - val_loss: 0.4190 - val_acc: 0.8988\n",
      "Epoch 100/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4779 - acc: 0.8706 - val_loss: 0.4294 - val_acc: 0.8920\n",
      "Epoch 101/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4594 - acc: 0.8772 - val_loss: 0.4142 - val_acc: 0.8980\n",
      "Epoch 102/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4542 - acc: 0.8781 - val_loss: 0.4034 - val_acc: 0.9022\n",
      "Epoch 103/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.4450 - acc: 0.8830 - val_loss: 0.4006 - val_acc: 0.9026\n",
      "Epoch 104/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4517 - acc: 0.8792 - val_loss: 0.4006 - val_acc: 0.9025\n",
      "Epoch 105/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4433 - acc: 0.8827 - val_loss: 0.4007 - val_acc: 0.9026\n",
      "Epoch 106/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.4448 - acc: 0.8809 - val_loss: 0.3926 - val_acc: 0.9059\n",
      "Epoch 107/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4449 - acc: 0.8825 - val_loss: 0.4019 - val_acc: 0.9019\n",
      "Epoch 108/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.4400 - acc: 0.8811 - val_loss: 0.4080 - val_acc: 0.9015\n",
      "Epoch 109/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4418 - acc: 0.8804 - val_loss: 0.3966 - val_acc: 0.9051\n",
      "Epoch 110/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4382 - acc: 0.8818 - val_loss: 0.4004 - val_acc: 0.9008\n",
      "Epoch 111/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4407 - acc: 0.8812 - val_loss: 0.3947 - val_acc: 0.9044\n",
      "Epoch 112/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4398 - acc: 0.8814 - val_loss: 0.4007 - val_acc: 0.9036\n",
      "Epoch 113/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4357 - acc: 0.8835 - val_loss: 0.4011 - val_acc: 0.9025\n",
      "Epoch 114/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4339 - acc: 0.8834 - val_loss: 0.3874 - val_acc: 0.9038\n",
      "Epoch 115/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4319 - acc: 0.8833 - val_loss: 0.3963 - val_acc: 0.9025\n",
      "Epoch 116/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4266 - acc: 0.8861 - val_loss: 0.3837 - val_acc: 0.9057\n",
      "Epoch 117/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4330 - acc: 0.8819 - val_loss: 0.3996 - val_acc: 0.9010\n",
      "Epoch 118/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4281 - acc: 0.8832 - val_loss: 0.3858 - val_acc: 0.9062\n",
      "Epoch 119/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4251 - acc: 0.8856 - val_loss: 0.3886 - val_acc: 0.9044\n",
      "Epoch 120/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4245 - acc: 0.8865 - val_loss: 0.4014 - val_acc: 0.9011\n",
      "Epoch 121/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.4242 - acc: 0.8857 - val_loss: 0.3973 - val_acc: 0.9031\n",
      "Epoch 122/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4293 - acc: 0.8832 - val_loss: 0.3985 - val_acc: 0.9017\n",
      "Epoch 123/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4222 - acc: 0.8854 - val_loss: 0.3964 - val_acc: 0.9017\n",
      "Epoch 124/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4207 - acc: 0.8852 - val_loss: 0.3904 - val_acc: 0.9041\n",
      "Epoch 125/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4229 - acc: 0.8852 - val_loss: 0.3934 - val_acc: 0.9033\n",
      "Epoch 126/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4062 - acc: 0.8913 - val_loss: 0.3796 - val_acc: 0.9083\n",
      "Epoch 127/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4117 - acc: 0.8895 - val_loss: 0.3836 - val_acc: 0.9078\n",
      "Epoch 128/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4012 - acc: 0.8913 - val_loss: 0.3852 - val_acc: 0.9050\n",
      "Epoch 129/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4029 - acc: 0.8912 - val_loss: 0.3786 - val_acc: 0.9083\n",
      "Epoch 130/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.4041 - acc: 0.8909 - val_loss: 0.3794 - val_acc: 0.9067\n",
      "Epoch 131/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3971 - acc: 0.8930 - val_loss: 0.3796 - val_acc: 0.9054\n",
      "Epoch 132/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3993 - acc: 0.8931 - val_loss: 0.3711 - val_acc: 0.9096\n",
      "Epoch 133/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3976 - acc: 0.8927 - val_loss: 0.3741 - val_acc: 0.9085\n",
      "Epoch 134/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.3926 - acc: 0.8939 - val_loss: 0.3732 - val_acc: 0.9095\n",
      "Epoch 135/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3918 - acc: 0.8939 - val_loss: 0.3751 - val_acc: 0.9085\n",
      "Epoch 136/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.4003 - acc: 0.8903 - val_loss: 0.3719 - val_acc: 0.9092\n",
      "Epoch 137/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3933 - acc: 0.8928 - val_loss: 0.3764 - val_acc: 0.9086\n",
      "Epoch 138/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3967 - acc: 0.8937 - val_loss: 0.3722 - val_acc: 0.9072\n",
      "Epoch 139/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3876 - acc: 0.8957 - val_loss: 0.3736 - val_acc: 0.9068\n",
      "Epoch 140/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3858 - acc: 0.8963 - val_loss: 0.3749 - val_acc: 0.9071\n",
      "Epoch 141/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.3851 - acc: 0.8974 - val_loss: 0.3740 - val_acc: 0.9064\n",
      "Epoch 142/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3885 - acc: 0.8944 - val_loss: 0.3702 - val_acc: 0.9087\n",
      "Epoch 143/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3848 - acc: 0.8981 - val_loss: 0.3763 - val_acc: 0.9074\n",
      "Epoch 144/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.3908 - acc: 0.8927 - val_loss: 0.3703 - val_acc: 0.9080\n",
      "Epoch 145/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3864 - acc: 0.8963 - val_loss: 0.3789 - val_acc: 0.9059\n",
      "Epoch 146/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3863 - acc: 0.8945 - val_loss: 0.3679 - val_acc: 0.9067\n",
      "Epoch 147/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.3830 - acc: 0.8970 - val_loss: 0.3714 - val_acc: 0.9078\n",
      "Epoch 148/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3825 - acc: 0.8972 - val_loss: 0.3729 - val_acc: 0.9081\n",
      "Epoch 149/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3864 - acc: 0.8962 - val_loss: 0.3684 - val_acc: 0.9077\n",
      "Epoch 150/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3819 - acc: 0.8967 - val_loss: 0.3705 - val_acc: 0.9082\n",
      "Epoch 151/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3798 - acc: 0.8975 - val_loss: 0.3664 - val_acc: 0.9089\n",
      "Epoch 152/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3735 - acc: 0.8973 - val_loss: 0.3659 - val_acc: 0.9083\n",
      "Epoch 153/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.3717 - acc: 0.8992 - val_loss: 0.3662 - val_acc: 0.9100\n",
      "Epoch 154/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3753 - acc: 0.8977 - val_loss: 0.3625 - val_acc: 0.9095\n",
      "Epoch 155/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3735 - acc: 0.8987 - val_loss: 0.3633 - val_acc: 0.9116\n",
      "Epoch 156/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.3626 - acc: 0.9030 - val_loss: 0.3689 - val_acc: 0.9097\n",
      "Epoch 157/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3709 - acc: 0.8987 - val_loss: 0.3671 - val_acc: 0.9090\n",
      "Epoch 158/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.3681 - acc: 0.8996 - val_loss: 0.3662 - val_acc: 0.9102\n",
      "Epoch 159/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3658 - acc: 0.9003 - val_loss: 0.3619 - val_acc: 0.9114\n",
      "Epoch 160/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3726 - acc: 0.8985 - val_loss: 0.3648 - val_acc: 0.9097\n",
      "Epoch 161/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3644 - acc: 0.9008 - val_loss: 0.3606 - val_acc: 0.9104\n",
      "Epoch 162/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3685 - acc: 0.8993 - val_loss: 0.3623 - val_acc: 0.9108\n",
      "Epoch 163/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.3685 - acc: 0.9008 - val_loss: 0.3645 - val_acc: 0.9113\n",
      "Epoch 164/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3629 - acc: 0.9016 - val_loss: 0.3614 - val_acc: 0.9101\n",
      "Epoch 165/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3651 - acc: 0.9024 - val_loss: 0.3647 - val_acc: 0.9084\n",
      "Epoch 166/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.3660 - acc: 0.9010 - val_loss: 0.3606 - val_acc: 0.9104\n",
      "Epoch 167/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3651 - acc: 0.9004 - val_loss: 0.3647 - val_acc: 0.9093\n",
      "Epoch 168/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3649 - acc: 0.9008 - val_loss: 0.3616 - val_acc: 0.9075\n",
      "Epoch 169/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3630 - acc: 0.9018 - val_loss: 0.3633 - val_acc: 0.9100\n",
      "Epoch 170/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3680 - acc: 0.9001 - val_loss: 0.3641 - val_acc: 0.9091\n",
      "Epoch 171/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3610 - acc: 0.9028 - val_loss: 0.3647 - val_acc: 0.9101\n",
      "Epoch 172/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.3680 - acc: 0.8991 - val_loss: 0.3599 - val_acc: 0.9102\n",
      "Epoch 173/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3671 - acc: 0.9002 - val_loss: 0.3618 - val_acc: 0.9108\n",
      "Epoch 174/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3610 - acc: 0.9027 - val_loss: 0.3623 - val_acc: 0.9107\n",
      "Epoch 175/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3593 - acc: 0.9033 - val_loss: 0.3603 - val_acc: 0.9114\n",
      "Epoch 176/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3629 - acc: 0.8998 - val_loss: 0.3590 - val_acc: 0.9111\n",
      "Epoch 177/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3536 - acc: 0.9047 - val_loss: 0.3570 - val_acc: 0.9115\n",
      "Epoch 178/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3582 - acc: 0.9030 - val_loss: 0.3554 - val_acc: 0.9118\n",
      "Epoch 179/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.3546 - acc: 0.9038 - val_loss: 0.3578 - val_acc: 0.9123\n",
      "Epoch 180/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3512 - acc: 0.9063 - val_loss: 0.3602 - val_acc: 0.9123\n",
      "Epoch 181/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3535 - acc: 0.9036 - val_loss: 0.3587 - val_acc: 0.9122\n",
      "Epoch 182/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3536 - acc: 0.9045 - val_loss: 0.3577 - val_acc: 0.9117\n",
      "Epoch 183/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3526 - acc: 0.9051 - val_loss: 0.3570 - val_acc: 0.9120\n",
      "Epoch 184/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3529 - acc: 0.9043 - val_loss: 0.3576 - val_acc: 0.9122\n",
      "Epoch 185/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3532 - acc: 0.9040 - val_loss: 0.3559 - val_acc: 0.9122\n",
      "Epoch 186/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3550 - acc: 0.9033 - val_loss: 0.3572 - val_acc: 0.9117\n",
      "Epoch 187/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.3556 - acc: 0.9025 - val_loss: 0.3555 - val_acc: 0.9121\n",
      "Epoch 188/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3511 - acc: 0.9049 - val_loss: 0.3542 - val_acc: 0.9124\n",
      "Epoch 189/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.3535 - acc: 0.9050 - val_loss: 0.3540 - val_acc: 0.9118\n",
      "Epoch 190/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3499 - acc: 0.9055 - val_loss: 0.3572 - val_acc: 0.9113\n",
      "Epoch 191/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3486 - acc: 0.9057 - val_loss: 0.3572 - val_acc: 0.9124\n",
      "Epoch 192/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3518 - acc: 0.9048 - val_loss: 0.3552 - val_acc: 0.9117\n",
      "Epoch 193/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3482 - acc: 0.9055 - val_loss: 0.3564 - val_acc: 0.9126\n",
      "Epoch 194/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3545 - acc: 0.9039 - val_loss: 0.3591 - val_acc: 0.9126\n",
      "Epoch 195/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.3492 - acc: 0.9067 - val_loss: 0.3602 - val_acc: 0.9124\n",
      "Epoch 196/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3489 - acc: 0.9060 - val_loss: 0.3614 - val_acc: 0.9120\n",
      "Epoch 197/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3529 - acc: 0.9040 - val_loss: 0.3562 - val_acc: 0.9123\n",
      "Epoch 198/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3500 - acc: 0.9058 - val_loss: 0.3563 - val_acc: 0.9131\n",
      "Epoch 199/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3472 - acc: 0.9052 - val_loss: 0.3557 - val_acc: 0.9138\n",
      "Epoch 200/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3528 - acc: 0.9040 - val_loss: 0.3571 - val_acc: 0.9120\n",
      "Epoch 201/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3497 - acc: 0.9047 - val_loss: 0.3574 - val_acc: 0.9128\n",
      "Epoch 202/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3499 - acc: 0.9052 - val_loss: 0.3576 - val_acc: 0.9130\n",
      "Epoch 203/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3486 - acc: 0.9059 - val_loss: 0.3552 - val_acc: 0.9125\n",
      "Epoch 204/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.3488 - acc: 0.9051 - val_loss: 0.3553 - val_acc: 0.9134\n",
      "Epoch 205/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3490 - acc: 0.9058 - val_loss: 0.3567 - val_acc: 0.9129\n",
      "Epoch 206/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3453 - acc: 0.9063 - val_loss: 0.3544 - val_acc: 0.9121\n",
      "Epoch 207/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3421 - acc: 0.9072 - val_loss: 0.3544 - val_acc: 0.9129\n",
      "Epoch 208/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3450 - acc: 0.9070 - val_loss: 0.3546 - val_acc: 0.9129\n",
      "Epoch 209/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3394 - acc: 0.9089 - val_loss: 0.3529 - val_acc: 0.9130\n",
      "Epoch 210/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3492 - acc: 0.9059 - val_loss: 0.3545 - val_acc: 0.9130\n",
      "Epoch 211/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3455 - acc: 0.9068 - val_loss: 0.3543 - val_acc: 0.9129\n",
      "Epoch 212/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.3464 - acc: 0.9073 - val_loss: 0.3538 - val_acc: 0.9133\n",
      "Epoch 213/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3447 - acc: 0.9067 - val_loss: 0.3537 - val_acc: 0.9136\n",
      "Epoch 214/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.3458 - acc: 0.9064 - val_loss: 0.3564 - val_acc: 0.9132\n",
      "Epoch 215/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3503 - acc: 0.9051 - val_loss: 0.3545 - val_acc: 0.9141\n",
      "Epoch 216/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3438 - acc: 0.9045 - val_loss: 0.3542 - val_acc: 0.9129\n",
      "Epoch 217/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3457 - acc: 0.9066 - val_loss: 0.3537 - val_acc: 0.9135\n",
      "Epoch 218/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.3424 - acc: 0.9074 - val_loss: 0.3548 - val_acc: 0.9132\n",
      "Epoch 219/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3528 - acc: 0.9035 - val_loss: 0.3546 - val_acc: 0.9127\n",
      "Epoch 220/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3428 - acc: 0.9066 - val_loss: 0.3552 - val_acc: 0.9130\n",
      "Epoch 221/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.3465 - acc: 0.9078 - val_loss: 0.3540 - val_acc: 0.9131\n",
      "Epoch 222/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3475 - acc: 0.9046 - val_loss: 0.3535 - val_acc: 0.9133\n",
      "Epoch 223/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3500 - acc: 0.9031 - val_loss: 0.3548 - val_acc: 0.9126\n",
      "Epoch 224/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3434 - acc: 0.9074 - val_loss: 0.3535 - val_acc: 0.9128\n",
      "Epoch 225/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3490 - acc: 0.9055 - val_loss: 0.3539 - val_acc: 0.9129\n",
      "Epoch 226/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3395 - acc: 0.9078 - val_loss: 0.3538 - val_acc: 0.9128\n",
      "Epoch 227/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.3440 - acc: 0.9076 - val_loss: 0.3531 - val_acc: 0.9129\n",
      "Epoch 228/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3424 - acc: 0.9075 - val_loss: 0.3537 - val_acc: 0.9130\n",
      "Epoch 229/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3439 - acc: 0.9086 - val_loss: 0.3539 - val_acc: 0.9120\n",
      "Epoch 230/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.3475 - acc: 0.9047 - val_loss: 0.3524 - val_acc: 0.9124\n",
      "Epoch 231/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3366 - acc: 0.9087 - val_loss: 0.3533 - val_acc: 0.9128\n",
      "Epoch 232/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.3450 - acc: 0.9072 - val_loss: 0.3535 - val_acc: 0.9127\n",
      "Epoch 233/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3454 - acc: 0.9062 - val_loss: 0.3536 - val_acc: 0.9131\n",
      "Epoch 234/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3420 - acc: 0.9076 - val_loss: 0.3534 - val_acc: 0.9134\n",
      "Epoch 235/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3473 - acc: 0.9059 - val_loss: 0.3530 - val_acc: 0.9127\n",
      "Epoch 236/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3404 - acc: 0.9078 - val_loss: 0.3533 - val_acc: 0.9125\n",
      "Epoch 237/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3433 - acc: 0.9066 - val_loss: 0.3526 - val_acc: 0.9130\n",
      "Epoch 238/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3413 - acc: 0.9084 - val_loss: 0.3523 - val_acc: 0.9137\n",
      "Epoch 239/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3371 - acc: 0.9084 - val_loss: 0.3528 - val_acc: 0.9135\n",
      "Epoch 240/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.3409 - acc: 0.9073 - val_loss: 0.3522 - val_acc: 0.9135\n",
      "Epoch 241/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3405 - acc: 0.9081 - val_loss: 0.3531 - val_acc: 0.9130\n",
      "Epoch 242/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.3371 - acc: 0.9082 - val_loss: 0.3538 - val_acc: 0.9136\n",
      "Epoch 243/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3472 - acc: 0.9062 - val_loss: 0.3519 - val_acc: 0.9135\n",
      "Epoch 244/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.3375 - acc: 0.9084 - val_loss: 0.3533 - val_acc: 0.9130\n",
      "Epoch 245/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3408 - acc: 0.9088 - val_loss: 0.3549 - val_acc: 0.9130\n",
      "Epoch 246/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3400 - acc: 0.9076 - val_loss: 0.3528 - val_acc: 0.9129\n",
      "Epoch 247/250\n",
      "390/390 [==============================] - 144s 369ms/step - loss: 0.3383 - acc: 0.9073 - val_loss: 0.3532 - val_acc: 0.9134\n",
      "Epoch 248/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3399 - acc: 0.9073 - val_loss: 0.3541 - val_acc: 0.9132\n",
      "Epoch 249/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3405 - acc: 0.9086 - val_loss: 0.3534 - val_acc: 0.9132\n",
      "Epoch 250/250\n",
      "390/390 [==============================] - 144s 370ms/step - loss: 0.3421 - acc: 0.9075 - val_loss: 0.3538 - val_acc: 0.9128\n",
      "{'val_loss': [1.4718593357086183, 1.0514937415122987, 0.95037446107864376, 0.86177803821563725, 0.80842532548904422, 0.78602883882522578, 0.69727212886810308, 0.68933542566299444, 0.69282410154342655, 0.66033310613632201, 0.62528653259277345, 0.69543025283813475, 0.5978508562088013, 0.62500986747741705, 0.57327001986503601, 0.58293239631652827, 0.55482385692596436, 0.58415137672424311, 0.57387900228500366, 0.6157508569717407, 0.57082568535804745, 0.56642306375503537, 0.55059183130264278, 0.56210118932723996, 0.53718005847930905, 0.57620832715034487, 0.55054056768417359, 0.5256480438709259, 0.57175481681823725, 0.56741761970520022, 0.56251787691116328, 0.5254380583763123, 0.54031899394989014, 0.5180606034755707, 0.52375785570144651, 0.53682527284622195, 0.52766881041526792, 0.52765581102371217, 0.53182461080551147, 0.53063018150329588, 0.53233265247344974, 0.50708232440948486, 0.51732424497604368, 0.51403101339340207, 0.51749609022140508, 0.49191327052116396, 0.525629962348938, 0.50152582035064697, 0.51020282368659975, 0.51452962121963497, 0.56025553197860722, 0.51274463405609128, 0.49775783805847168, 0.52229725990295406, 0.5105972100257874, 0.54077107105255129, 0.51407930660247803, 0.51283702669143671, 0.49637826452255251, 0.51294572229385371, 0.49047606830596924, 0.52845205545425411, 0.48878448052406309, 0.52522406387329101, 0.48756978435516357, 0.50729723129272464, 0.49586632843017581, 0.50503287487030024, 0.48954618163108826, 0.48790230045318606, 0.50593755912780758, 0.49153957490921019, 0.50820958366394042, 0.52176487274169925, 0.4914972092628479, 0.46078949832916261, 0.45141368303298951, 0.45326056098937989, 0.45193242368698122, 0.46073491830825808, 0.43468972444534304, 0.43506527814865115, 0.44586916365623475, 0.43682718296051026, 0.44110050868988038, 0.44007573099136355, 0.43174301834106443, 0.42169713735580444, 0.43002926540374758, 0.4225868245124817, 0.43628883886337283, 0.43192856121063233, 0.42786718006134034, 0.42155724463462829, 0.43035985603332522, 0.43678454084396362, 0.43228070921897888, 0.42230127439498899, 0.41898105626106263, 0.42939009027481079, 0.41418144540786744, 0.40339341621398928, 0.40063959121704101, 0.40057067813873293, 0.40066780576705935, 0.3926440532684326, 0.4019302302837372, 0.40802432885169981, 0.39664387121200562, 0.40040602254867552, 0.39470301809310915, 0.40073921337127688, 0.40106978974342344, 0.38744718170166015, 0.39626618986129758, 0.38370875806808474, 0.39955538082122805, 0.38578771057128908, 0.38859185056686402, 0.4014449788093567, 0.39728432626724242, 0.39852446861267088, 0.39639752917289733, 0.39044842853546141, 0.39339845466613771, 0.37963965511322023, 0.38362531747817991, 0.38519815573692323, 0.37857988424301148, 0.37942408542633055, 0.37956429982185363, 0.37105607833862303, 0.37413320503234865, 0.37322186737060548, 0.37506135454177858, 0.37186159400939939, 0.37635409154891969, 0.37222820129394529, 0.37355233955383299, 0.3748937891960144, 0.37402182788848876, 0.37020301790237425, 0.37633154678344727, 0.37032401456832886, 0.37890963287353513, 0.36789979491233826, 0.37143629579544069, 0.37290492019653321, 0.36837918367385863, 0.37051939430236819, 0.36641132373809815, 0.3659345682144165, 0.36623450422286985, 0.36249664945602417, 0.36325228090286255, 0.36892235565185549, 0.36706646614074706, 0.36623025832176209, 0.36186942887306212, 0.3647621609687805, 0.36061852555274965, 0.36228421878814698, 0.36449502506256104, 0.361355667591095, 0.36468326387405398, 0.36061326723098752, 0.3646522752761841, 0.36156421594619753, 0.36333943753242492, 0.36414438228607177, 0.36468675184249877, 0.35994077553749082, 0.3618465672016144, 0.3622946217060089, 0.36032666382789613, 0.35903035020828244, 0.35695750660896303, 0.35538293991088865, 0.357827511882782, 0.36017020587921145, 0.35867274255752563, 0.35773987350463865, 0.3570171043395996, 0.35757122068405151, 0.35586460809707643, 0.35718093304634096, 0.35552883024215698, 0.35417377600669858, 0.35402473096847537, 0.35717312650680544, 0.35719106936454775, 0.3552218523979187, 0.35638556962013246, 0.35906431040763853, 0.36016556391716004, 0.36143336849212648, 0.35619633035659792, 0.35625310635566709, 0.35571403532028201, 0.35711304211616518, 0.35743853414058685, 0.35759568462371827, 0.35519243183135984, 0.35532589378356932, 0.35673055987358093, 0.35437110013961792, 0.35444885158538819, 0.35463809728622436, 0.35293301191329957, 0.35448174924850462, 0.35431797437667845, 0.35376982040405275, 0.35371475172042849, 0.35638793239593508, 0.35447802267074585, 0.35419353771209716, 0.35374911556243899, 0.35481942791938781, 0.35455091705322267, 0.35523338904380797, 0.35398974647521975, 0.35352244577407838, 0.35479941034317014, 0.35348759422302245, 0.35392078371047975, 0.35378090696334841, 0.35305011119842528, 0.35369210538864138, 0.35385427017211912, 0.35242461032867434, 0.35332067966461184, 0.35348392353057861, 0.35355563573837279, 0.3533513557434082, 0.35295201702117918, 0.35331990890502929, 0.35258343772888184, 0.35230127267837524, 0.35278460063934325, 0.3521700284957886, 0.35306939554214478, 0.353781201839447, 0.35192057714462283, 0.35331153726577758, 0.35493725185394287, 0.35281816625595092, 0.35323933629989623, 0.35405698814392089, 0.35336747579574584, 0.3538320436477661], 'val_acc': [0.52110000000000001, 0.64880000000000004, 0.70420000000000005, 0.72609999999999997, 0.748, 0.76029999999999998, 0.78420000000000001, 0.79459999999999997, 0.78920000000000001, 0.80610000000000004, 0.81869999999999998, 0.80010000000000003, 0.82609999999999995, 0.81859999999999999, 0.83860000000000001, 0.8377, 0.84560000000000002, 0.83809999999999996, 0.83979999999999999, 0.83350000000000002, 0.84209999999999996, 0.84730000000000005, 0.85099999999999998, 0.8458, 0.85460000000000003, 0.84789999999999999, 0.85119999999999996, 0.8609, 0.84519999999999995, 0.85450000000000004, 0.85160000000000002, 0.86270000000000002, 0.85560000000000003, 0.86499999999999999, 0.86360000000000003, 0.85819999999999996, 0.8629, 0.86509999999999998, 0.86270000000000002, 0.86560000000000004, 0.86429999999999996, 0.87139999999999995, 0.8649, 0.86939999999999995, 0.86829999999999996, 0.87490000000000001, 0.8659, 0.87339999999999995, 0.87170000000000003, 0.86990000000000001, 0.86360000000000003, 0.87139999999999995, 0.87660000000000005, 0.86739999999999995, 0.87460000000000004, 0.86439999999999995, 0.86799999999999999, 0.87460000000000004, 0.87680000000000002, 0.87649999999999995, 0.87719999999999998, 0.86909999999999998, 0.88149999999999995, 0.86829999999999996, 0.88100000000000001, 0.87629999999999997, 0.878, 0.87719999999999998, 0.88119999999999998, 0.88239999999999996, 0.87429999999999997, 0.87829999999999997, 0.87329999999999997, 0.87, 0.87849999999999995, 0.89049999999999996, 0.89200000000000002, 0.89249999999999996, 0.8911, 0.89019999999999999, 0.89410000000000001, 0.89449999999999996, 0.8901, 0.89539999999999997, 0.89570000000000005, 0.89380000000000004, 0.89580000000000004, 0.8972, 0.89459999999999995, 0.89729999999999999, 0.89439999999999997, 0.89580000000000004, 0.8962, 0.89800000000000002, 0.89710000000000001, 0.89380000000000004, 0.89529999999999998, 0.89780000000000004, 0.89880000000000004, 0.89200000000000002, 0.89800000000000002, 0.9022, 0.90259999999999996, 0.90249999999999997, 0.90259999999999996, 0.90590000000000004, 0.90190000000000003, 0.90149999999999997, 0.90510000000000002, 0.90080000000000005, 0.90439999999999998, 0.90359999999999996, 0.90249999999999997, 0.90380000000000005, 0.90249999999999997, 0.90569999999999995, 0.90100000000000002, 0.90620000000000001, 0.90439999999999998, 0.90110000000000001, 0.90310000000000001, 0.90169999999999995, 0.90169999999999995, 0.90410000000000001, 0.90329999999999999, 0.9083, 0.90780000000000005, 0.90500000000000003, 0.9083, 0.90669999999999995, 0.90539999999999998, 0.90959999999999996, 0.90849999999999997, 0.90949999999999998, 0.90849999999999997, 0.90920000000000001, 0.90859999999999996, 0.90720000000000001, 0.90680000000000005, 0.90710000000000002, 0.90639999999999998, 0.90869999999999995, 0.90739999999999998, 0.90800000000000003, 0.90590000000000004, 0.90669999999999995, 0.90780000000000005, 0.90810000000000002, 0.90769999999999995, 0.90820000000000001, 0.90890000000000004, 0.9083, 0.91000000000000003, 0.90949999999999998, 0.91159999999999997, 0.90969999999999995, 0.90900000000000003, 0.91020000000000001, 0.91139999999999999, 0.90969999999999995, 0.91039999999999999, 0.91080000000000005, 0.9113, 0.91010000000000002, 0.90839999999999999, 0.91039999999999999, 0.9093, 0.90749999999999997, 0.91000000000000003, 0.90910000000000002, 0.91010000000000002, 0.91020000000000001, 0.91080000000000005, 0.91069999999999995, 0.91139999999999999, 0.91110000000000002, 0.91149999999999998, 0.91180000000000005, 0.9123, 0.9123, 0.91220000000000001, 0.91169999999999995, 0.91200000000000003, 0.91220000000000001, 0.91220000000000001, 0.91169999999999995, 0.91210000000000002, 0.91239999999999999, 0.91180000000000005, 0.9113, 0.91239999999999999, 0.91169999999999995, 0.91259999999999997, 0.91259999999999997, 0.91239999999999999, 0.91200000000000003, 0.9123, 0.91310000000000002, 0.91379999999999995, 0.91200000000000003, 0.91280000209808354, 0.91300000000000003, 0.91249999999999998, 0.91339999999999999, 0.91290000000000004, 0.91210000000000002, 0.91290000000000004, 0.91290000000000004, 0.91300000000000003, 0.91300000000000003, 0.91290000000000004, 0.9133, 0.91359999999999997, 0.91320000000000001, 0.91410000000000002, 0.91290000000000004, 0.91349999999999998, 0.91320000000000001, 0.91269999999999996, 0.91300000000000003, 0.91310000000000002, 0.9133, 0.91259999999999997, 0.91279999999999994, 0.91290000000000004, 0.91279999999999994, 0.91290000000000004, 0.91300000000000003, 0.91200000000000003, 0.91239999999999999, 0.91279999999999994, 0.91269999999999996, 0.91310000000000002, 0.91339999999999999, 0.91269999999999996, 0.91249999999999998, 0.91300000000000003, 0.91369999999999996, 0.91349999999999998, 0.91349999999999998, 0.91300000000000003, 0.91359999999999997, 0.91349999999999998, 0.91300000000000003, 0.91300000000000003, 0.91290000000000004, 0.91339999999999999, 0.91320000000000001, 0.91320000000000001, 0.91279999999999994], 'acc': [0.38422361244786651, 0.53565126723458378, 0.60308389477061275, 0.64787856917523556, 0.67258180939390144, 0.69227221691344543, 0.70807266606326891, 0.72150705803041093, 0.73578360603144044, 0.74414501124786803, 0.75200513317908546, 0.75695781196047185, 0.76341434069310088, 0.77063281998100441, 0.77504411294822095, 0.77781119664433607, 0.78529034325967129, 0.78859881294205814, 0.78960137951247844, 0.79391241582265293, 0.79836381139531898, 0.80085017641347156, 0.80271495029182049, 0.8056424446965702, 0.80708614051973049, 0.80828922038511242, 0.81075553418659119, 0.8104146615526483, 0.81256015395598036, 0.8127807186205952, 0.81683108756509315, 0.81757298684632662, 0.81969842799474002, 0.81873596408713656, 0.8181544754189255, 0.82094161056118364, 0.82310715433416892, 0.82352823223637817, 0.82451074751363496, 0.8248516201858227, 0.8278593198970835, 0.82772435897435892, 0.83020231217701002, 0.83012512028886598, 0.8295837343408391, 0.83239092073801579, 0.83237086944485228, 0.83012820512820518, 0.83285163778407345, 0.83329323063227168, 0.83483573717948723, 0.83630378931933058, 0.83507779916586466, 0.83786493419338814, 0.83666185438537355, 0.83575954443375045, 0.83924847613705789, 0.83888755211433919, 0.83820580688469837, 0.84153432791761607, 0.83726339425113594, 0.83884744950888968, 0.83700272698107159, 0.84077237732409671, 0.8400705806865576, 0.84057186399089012, 0.84265720245736431, 0.84301812636534834, 0.84107314721873294, 0.84212740384615381, 0.8419637123246031, 0.84335899907578094, 0.84494305421880012, 0.8451034648509449, 0.84494305419967763, 0.85779595768983297, 0.86119791666666667, 0.86323859983326612, 0.8634103304650641, 0.86387151110657978, 0.86519489894128965, 0.86634615384615388, 0.86701957013769948, 0.86750080207237878, 0.86691152863802634, 0.86798203396881324, 0.86784167470632168, 0.86746069935219461, 0.86649823544459115, 0.87010747511722664, 0.87197224893820835, 0.86982675651575381, 0.87068896372807036, 0.87106993904395247, 0.86924526790491008, 0.86922521659262408, 0.87173163298042988, 0.86958614048148564, 0.87167147902444952, 0.87062880977209001, 0.87718349358974357, 0.87814805901199722, 0.88302825943506447, 0.87919072826435674, 0.88269230769230766, 0.88096098267810041, 0.88253205128205126, 0.88116168906922976, 0.88041385948026951, 0.88181745271710277, 0.88119586142431972, 0.8814364773629757, 0.88354186719910321, 0.8833012512030799, 0.88330125122220238, 0.88616859163272677, 0.88181745269798029, 0.88310073788899579, 0.88557692307692304, 0.88656961818440505, 0.88567758511869132, 0.88322104589656869, 0.88540664102008493, 0.88514597369265324, 0.88518607639371494, 0.89122152710940006, 0.88945700990721555, 0.89130173245415611, 0.89122596153846156, 0.89097623639024059, 0.89304619824844256, 0.89314645494372946, 0.8927483974358974, 0.89388648048761032, 0.89382820021161524, 0.8903593198970835, 0.89286573630401178, 0.89364773824806198, 0.8957130253831278, 0.8963141025641026, 0.8974189145601783, 0.89446984279756181, 0.89805689102564101, 0.89270231208130568, 0.8962945139366042, 0.89449118589743593, 0.89695728962132004, 0.89711661852434887, 0.89625441127378735, 0.89669554062213963, 0.89753769646480297, 0.89729567307692304, 0.89920520227385703, 0.89763795316008987, 0.89867788461538467, 0.90289820165073709, 0.89871794871794874, 0.89968689789968037, 0.9003248315305713, 0.89854026305434564, 0.90084134615384615, 0.89930221366698748, 0.90081085424508966, 0.90154796282297378, 0.90236378205128209, 0.90095134872845062, 0.90036493426987785, 0.90078601217208709, 0.90180863011216061, 0.90008421557279283, 0.90276442307692306, 0.89912491969826436, 0.90018028846153841, 0.90269088871325287, 0.9033325312418321, 0.89976339425113594, 0.90473612451691021, 0.90297160727648085, 0.90374116893372014, 0.90632017962168454, 0.90367340395226481, 0.90449550850176452, 0.90507699713173073, 0.90431504651908889, 0.90397417386602352, 0.90326522435897438, 0.90247671804752727, 0.90490785256410255, 0.90498554919037055, 0.90553817777324652, 0.90568910256410251, 0.90481632980429905, 0.90547802372165398, 0.90393407118408431, 0.90663134232498399, 0.90598958333333335, 0.90401427656708522, 0.90577879373102488, 0.90515720239999697, 0.90389396852126747, 0.90470456007052169, 0.90527751046493721, 0.90594951923076927, 0.90508590243378739, 0.90571863967943234, 0.90636028232274624, 0.90716233559820492, 0.90699118589743588, 0.90888675011845022, 0.90589910166210807, 0.90672120626897512, 0.90735388565844421, 0.90671073717948714, 0.90639049457906529, 0.90515720241911946, 0.90453561120282622, 0.90655048076923073, 0.90741409760449432, 0.9035731472569779, 0.90657051282051282, 0.90775529861297066, 0.90467597048444015, 0.90313201800423781, 0.90740295157510575, 0.90548878205128203, 0.90788418354802991, 0.90757466283224297, 0.90745192307692313, 0.90856592881591569, 0.90472463071920506, 0.90871394230769231, 0.90717324985857573, 0.90621992298376497, 0.90762351620147574, 0.9058991017194753, 0.907824029477315, 0.90659054487179491, 0.90842556947693442, 0.90842556949605691, 0.90729367378895764, 0.90809294871794877, 0.90815671168234235, 0.90625, 0.90847784198471271, 0.90875400641025639, 0.90762351623972071, 0.90735388573500764, 0.90733173076923079, 0.90858598012820169, 0.90754331089496465], 'loss': [2.0437201394230118, 1.4236115582829911, 1.2188943526207119, 1.0989373841268877, 1.0263970732918373, 0.96322464002108088, 0.92354366356339024, 0.88531852137556744, 0.85150189616950156, 0.83009534791221895, 0.80893644565261635, 0.79702972933616556, 0.77981066336645544, 0.76132048275901376, 0.75282510027963456, 0.73967603668403192, 0.72670981415120461, 0.71859404095709989, 0.71644338694833432, 0.70460337714044419, 0.69695686873312668, 0.68888646754309812, 0.68651466194467359, 0.67890465097011266, 0.67474983730567051, 0.66819794408625655, 0.66752734720075901, 0.66171808458116521, 0.66128921369195248, 0.65556457500102849, 0.65137295368048942, 0.64697841719094096, 0.6469428772952337, 0.64504780062200318, 0.64326187657514444, 0.6378788683404516, 0.63295850099414286, 0.63272253978240933, 0.63043468601626385, 0.63043429391063632, 0.62746842662072699, 0.62234188241836352, 0.62160640715710536, 0.61469005989577097, 0.61972569206948847, 0.61708331649258308, 0.61065598321259307, 0.61614446334349804, 0.61080109583083531, 0.6091733168515403, 0.60804504087338085, 0.60288711432929831, 0.60844341628934695, 0.59749090930648363, 0.60077321479670087, 0.60509095380885269, 0.59814890240070184, 0.59713945698195026, 0.59462841773476782, 0.59080152491326898, 0.59807183152022259, 0.59526700457396098, 0.59727305280959564, 0.58900247958366558, 0.5922942395618721, 0.58997118775187529, 0.58893692164532752, 0.58616172772093311, 0.59118429808269379, 0.58917706089142041, 0.58610316703314957, 0.58432742940870885, 0.58256050832462036, 0.58135058333332967, 0.58175922785388146, 0.54210375014657886, 0.53090600310227809, 0.52130446913690998, 0.51881810096276282, 0.51464409135188816, 0.51439994468419725, 0.50389317274093626, 0.50492375083255736, 0.50002196116595843, 0.5011926947320664, 0.49622208140085322, 0.49848859427561926, 0.49627636035053935, 0.49712927459255413, 0.48772105523933384, 0.48553679684128637, 0.48998223090730192, 0.48762290457482899, 0.4809238399149785, 0.48642913664920917, 0.48403921858178983, 0.48022428684636953, 0.48205039116446696, 0.476736955203649, 0.47788821655586433, 0.45944054348346514, 0.45412115103012102, 0.44510307597019155, 0.4517120231620616, 0.44329811846598599, 0.44465757338910356, 0.4448918607754585, 0.43998961121934915, 0.44182639952688063, 0.4382303096676699, 0.44074643130557906, 0.43988450759299336, 0.43576149907791351, 0.43406642503489046, 0.43196893021354393, 0.42655838369596527, 0.43311099374871781, 0.42831628103458158, 0.42507661756796716, 0.42429652918138527, 0.42421315200794335, 0.42925713582034292, 0.42210005495767772, 0.42073492848762228, 0.42279033862439186, 0.40624850634622928, 0.41174015506055062, 0.40130524013644425, 0.40294734736283622, 0.40394638980246539, 0.39716312407797105, 0.39920661150739861, 0.3976329722847694, 0.39261292398703274, 0.39184376394706616, 0.40033696359994514, 0.39322255563766556, 0.39678186526235804, 0.38758716551317479, 0.38579266655903593, 0.38518508704105248, 0.38844610739679797, 0.38477029085923464, 0.3909188552724297, 0.38640298144841062, 0.38626598627903524, 0.38311903862020641, 0.38257792347281416, 0.38645471557425043, 0.38195974656814252, 0.37978170613734663, 0.37350438913473716, 0.37170552348929792, 0.37539113269695151, 0.37347506682078041, 0.36277576182312743, 0.37090517515555405, 0.36800132990457651, 0.36572519308066653, 0.37260871526137124, 0.36438054476792997, 0.36845555908220257, 0.36845311713892448, 0.36292789229644501, 0.36506829193005197, 0.36614835513043881, 0.36506414236569579, 0.36493369811691073, 0.36293896372057438, 0.36799320765018922, 0.36096496578210441, 0.36776697210723497, 0.36714794658697569, 0.36103900848270265, 0.35921890621082808, 0.36300723991372624, 0.35361141741830338, 0.35815044906722993, 0.35472661194103838, 0.35114883057077534, 0.35343147426566368, 0.35369438268566955, 0.35263941343067012, 0.35299222011156178, 0.3532347936430611, 0.35504886538554459, 0.35562089276030495, 0.3510676188728748, 0.35339816270650926, 0.3499354455461095, 0.34855109032912129, 0.35180573675507798, 0.34820492148284621, 0.35456269130440604, 0.34931848516870784, 0.34890832698498014, 0.35298261125767733, 0.35003240295125004, 0.3473098398660443, 0.35289090812053286, 0.34962745685568203, 0.34978825767949717, 0.34864133275472203, 0.34888931637056836, 0.3490966454621573, 0.3452451056419828, 0.34209572297470442, 0.3450234984358152, 0.33936104841021186, 0.34920548344790497, 0.34560451085058053, 0.3462232201717263, 0.34473606030910442, 0.34590539896074746, 0.35032319528267941, 0.34376759089488879, 0.34566224484871594, 0.34239728421489629, 0.35269426257326542, 0.34281007295999771, 0.34655916845867402, 0.34750986015104429, 0.35008297438494546, 0.34339030620032179, 0.34901241747996747, 0.33938983547446588, 0.34402459708635214, 0.34236931055784225, 0.34387240902827571, 0.34750044090325205, 0.33663757126300764, 0.34498906709784949, 0.34537754503052165, 0.34196631436857383, 0.34727403949880431, 0.34018241672665789, 0.34329651571237124, 0.34138398040247375, 0.33716778740654935, 0.34087086346912016, 0.3405397279140277, 0.33714289869455166, 0.34724537141812151, 0.33726613356581697, 0.34077551945661888, 0.33995574893460351, 0.33820469167344297, 0.3398886503317417, 0.34057080467465523, 0.34205238050209014]}\n"
     ]
    }
   ],
   "source": [
    "# Create model and get ready - reproducible cell\n",
    "act, act_name = \"elu\", \"elu\"\n",
    "model = create(act)\n",
    "opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "\n",
    "# Training - only for 125 eppochs\n",
    "batch_size = 128\n",
    "epochs=250\n",
    "# lr scheduler\n",
    "lr_1 = keras.callbacks.LearningRateScheduler(schedule)\n",
    "\n",
    "# Fit the model\n",
    "his = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,verbose=1,validation_data=(x_test,y_test), callbacks=[lr_1])\n",
    "# Save the model\n",
    "model.save('elu_at_2.h5')\n",
    "# Display logs\n",
    "print(his.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Epoch 1/250\n",
      "241/390 [=================>............] - ETA: 55s - loss: 2.2732 - acc: 0.3320"
     ]
    }
   ],
   "source": [
    "# Create model and get ready - reproducible cell\n",
    "act, act_name = \"elu\", \"elu\"\n",
    "model = create(act)\n",
    "opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "\n",
    "# Training - only for 125 eppochs\n",
    "batch_size = 128\n",
    "epochs=250\n",
    "# lr scheduler\n",
    "lr_1 = keras.callbacks.LearningRateScheduler(schedule)\n",
    "\n",
    "# Fit the model\n",
    "his = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,verbose=1,validation_data=(x_test,y_test), callbacks=[lr_1])\n",
    "# Save the model\n",
    "model.save('models/elu/elu_at_3.h5')\n",
    "# Display logs\n",
    "print(his.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_test):\n",
    "    y_hat = np.argmax(y_pred, axis=1)\n",
    "    y = np.argmax(y_test, axis=1)\n",
    "\n",
    "    good = np.sum(np.equal(y, y_hat))\n",
    "    return float(good/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "0.9187\n"
     ]
    }
   ],
   "source": [
    "import keras.regularizers as regularizers\n",
    "ensembler = 0\n",
    "for i in range(3):\n",
    "    if i == 0:\n",
    "        model = create(\"relu\")\n",
    "        model.load_weights('relu_at_'+str(i+1)+'.h5')\n",
    "        model.save('relu_at_'+str(i+1)+'.h5')\n",
    "    else:\n",
    "        model = load_model('relu_at_'+str(i+1)+'.h5')\n",
    "    ensembler += model.predict_proba(x_test)\n",
    "    \n",
    "print(accuracy(ensembler, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "390/390 [==============================] - 176s 452ms/step - loss: 2.0201 - acc: 0.3906 - val_loss: 1.3828 - val_acc: 0.5563\n",
      "Epoch 2/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 1.4731 - acc: 0.5308 - val_loss: 1.1149 - val_acc: 0.6337\n",
      "Epoch 3/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 1.2980 - acc: 0.5922 - val_loss: 1.0801 - val_acc: 0.6678\n",
      "Epoch 4/200\n",
      "390/390 [==============================] - 175s 449ms/step - loss: 1.2151 - acc: 0.6298 - val_loss: 0.9754 - val_acc: 0.6856\n",
      "Epoch 5/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 1.1390 - acc: 0.6510 - val_loss: 1.5834 - val_acc: 0.6358\n",
      "Epoch 6/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 1.1029 - acc: 0.6706 - val_loss: 0.7973 - val_acc: 0.7567\n",
      "Epoch 7/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 1.0241 - acc: 0.6928 - val_loss: 0.7856 - val_acc: 0.7628\n",
      "Epoch 8/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.9956 - acc: 0.7041 - val_loss: 1.2215 - val_acc: 0.7283\n",
      "Epoch 9/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 1.0520 - acc: 0.7052 - val_loss: 1.6160 - val_acc: 0.7280\n",
      "Epoch 10/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 1.0199 - acc: 0.7127 - val_loss: 0.7939 - val_acc: 0.7845\n",
      "Epoch 11/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 1.0037 - acc: 0.7170 - val_loss: 0.7445 - val_acc: 0.7803\n",
      "Epoch 12/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.9561 - acc: 0.7259 - val_loss: 0.7351 - val_acc: 0.7896\n",
      "Epoch 13/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.9214 - acc: 0.7366 - val_loss: 0.6897 - val_acc: 0.8018\n",
      "Epoch 14/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.9240 - acc: 0.7409 - val_loss: 0.7333 - val_acc: 0.7836\n",
      "Epoch 15/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.9455 - acc: 0.7394 - val_loss: 0.6761 - val_acc: 0.8085\n",
      "Epoch 16/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.9299 - acc: 0.7462 - val_loss: 0.6588 - val_acc: 0.8135\n",
      "Epoch 17/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.9478 - acc: 0.7482 - val_loss: 0.7012 - val_acc: 0.8151\n",
      "Epoch 18/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.9236 - acc: 0.7535 - val_loss: 0.6909 - val_acc: 0.8094\n",
      "Epoch 19/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.9234 - acc: 0.7549 - val_loss: 0.6782 - val_acc: 0.8189\n",
      "Epoch 20/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.8722 - acc: 0.7605 - val_loss: 0.6582 - val_acc: 0.8214\n",
      "Epoch 21/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.8591 - acc: 0.7642 - val_loss: 0.6625 - val_acc: 0.8172\n",
      "Epoch 22/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.8269 - acc: 0.7725 - val_loss: 0.6077 - val_acc: 0.8343\n",
      "Epoch 23/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.8260 - acc: 0.7745 - val_loss: 0.6269 - val_acc: 0.8280\n",
      "Epoch 24/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.8197 - acc: 0.7769 - val_loss: 0.6442 - val_acc: 0.8197\n",
      "Epoch 25/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.8047 - acc: 0.7790 - val_loss: 0.6241 - val_acc: 0.8293\n",
      "Epoch 26/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.8188 - acc: 0.7806 - val_loss: 0.6076 - val_acc: 0.8352\n",
      "Epoch 27/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.8053 - acc: 0.7823 - val_loss: 0.6519 - val_acc: 0.8270\n",
      "Epoch 28/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.8434 - acc: 0.7808 - val_loss: 0.6330 - val_acc: 0.8318\n",
      "Epoch 29/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7910 - acc: 0.7879 - val_loss: 0.6119 - val_acc: 0.8378\n",
      "Epoch 30/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7811 - acc: 0.7891 - val_loss: 0.5965 - val_acc: 0.8451\n",
      "Epoch 31/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7707 - acc: 0.7902 - val_loss: 0.6395 - val_acc: 0.8271\n",
      "Epoch 32/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7675 - acc: 0.7932 - val_loss: 0.5810 - val_acc: 0.8464\n",
      "Epoch 33/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7828 - acc: 0.7923 - val_loss: 0.6336 - val_acc: 0.8310\n",
      "Epoch 34/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7781 - acc: 0.7948 - val_loss: 0.5732 - val_acc: 0.8480\n",
      "Epoch 35/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7599 - acc: 0.7993 - val_loss: 0.5962 - val_acc: 0.8421\n",
      "Epoch 36/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7571 - acc: 0.7990 - val_loss: 0.6151 - val_acc: 0.8365\n",
      "Epoch 37/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7565 - acc: 0.7981 - val_loss: 0.5864 - val_acc: 0.8464\n",
      "Epoch 38/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7765 - acc: 0.7984 - val_loss: 0.6086 - val_acc: 0.8371\n",
      "Epoch 39/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7530 - acc: 0.7973 - val_loss: 0.5760 - val_acc: 0.8520\n",
      "Epoch 40/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7489 - acc: 0.8030 - val_loss: 0.6164 - val_acc: 0.8407\n",
      "Epoch 41/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7573 - acc: 0.8010 - val_loss: 0.6014 - val_acc: 0.8493\n",
      "Epoch 42/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7443 - acc: 0.8053 - val_loss: 0.5915 - val_acc: 0.8531\n",
      "Epoch 43/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7395 - acc: 0.8055 - val_loss: 0.5853 - val_acc: 0.8527\n",
      "Epoch 44/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7227 - acc: 0.8074 - val_loss: 0.5628 - val_acc: 0.8596\n",
      "Epoch 45/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7235 - acc: 0.8086 - val_loss: 0.5740 - val_acc: 0.8546\n",
      "Epoch 46/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7244 - acc: 0.8098 - val_loss: 0.5851 - val_acc: 0.8470\n",
      "Epoch 47/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7256 - acc: 0.8081 - val_loss: 0.5751 - val_acc: 0.8535\n",
      "Epoch 48/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7238 - acc: 0.8093 - val_loss: 0.5957 - val_acc: 0.8468\n",
      "Epoch 49/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7146 - acc: 0.8112 - val_loss: 0.5766 - val_acc: 0.8543\n",
      "Epoch 50/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7047 - acc: 0.8129 - val_loss: 0.5858 - val_acc: 0.8507\n",
      "Epoch 51/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7186 - acc: 0.8132 - val_loss: 0.5443 - val_acc: 0.8671\n",
      "Epoch 52/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7213 - acc: 0.8124 - val_loss: 0.5546 - val_acc: 0.8629\n",
      "Epoch 53/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7064 - acc: 0.8124 - val_loss: 0.5631 - val_acc: 0.8606\n",
      "Epoch 54/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7087 - acc: 0.8195 - val_loss: 0.5347 - val_acc: 0.8653\n",
      "Epoch 55/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7108 - acc: 0.8152 - val_loss: 0.5700 - val_acc: 0.8593\n",
      "Epoch 56/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7082 - acc: 0.8148 - val_loss: 0.5746 - val_acc: 0.8537\n",
      "Epoch 57/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6987 - acc: 0.8188 - val_loss: 0.5549 - val_acc: 0.8612\n",
      "Epoch 58/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6983 - acc: 0.8193 - val_loss: 0.5453 - val_acc: 0.8599\n",
      "Epoch 59/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7052 - acc: 0.8172 - val_loss: 0.5706 - val_acc: 0.8608\n",
      "Epoch 60/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6905 - acc: 0.8199 - val_loss: 0.5770 - val_acc: 0.8546\n",
      "Epoch 61/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6847 - acc: 0.8224 - val_loss: 0.5566 - val_acc: 0.8653\n",
      "Epoch 62/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7043 - acc: 0.8194 - val_loss: 0.5629 - val_acc: 0.8581\n",
      "Epoch 63/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6949 - acc: 0.8193 - val_loss: 0.5522 - val_acc: 0.8611\n",
      "Epoch 64/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6857 - acc: 0.8212 - val_loss: 0.5427 - val_acc: 0.8663\n",
      "Epoch 65/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6768 - acc: 0.8262 - val_loss: 0.5607 - val_acc: 0.8605\n",
      "Epoch 66/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6842 - acc: 0.8230 - val_loss: 0.5553 - val_acc: 0.8618\n",
      "Epoch 67/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6684 - acc: 0.8280 - val_loss: 0.5316 - val_acc: 0.8725\n",
      "Epoch 68/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6789 - acc: 0.8240 - val_loss: 0.5659 - val_acc: 0.8556\n",
      "Epoch 69/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6752 - acc: 0.8236 - val_loss: 0.5545 - val_acc: 0.8627\n",
      "Epoch 70/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6796 - acc: 0.8253 - val_loss: 0.5433 - val_acc: 0.8695\n",
      "Epoch 71/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6776 - acc: 0.8265 - val_loss: 0.5272 - val_acc: 0.8718\n",
      "Epoch 72/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6764 - acc: 0.8266 - val_loss: 0.5555 - val_acc: 0.8612\n",
      "Epoch 73/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6706 - acc: 0.8262 - val_loss: 0.5707 - val_acc: 0.8638\n",
      "Epoch 74/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6767 - acc: 0.8271 - val_loss: 0.5402 - val_acc: 0.8703\n",
      "Epoch 75/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6697 - acc: 0.8265 - val_loss: 0.5153 - val_acc: 0.8720\n",
      "Epoch 76/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6226 - acc: 0.8408 - val_loss: 0.4931 - val_acc: 0.8828\n",
      "Epoch 77/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6095 - acc: 0.8447 - val_loss: 0.4853 - val_acc: 0.8847\n",
      "Epoch 78/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5955 - acc: 0.8486 - val_loss: 0.4940 - val_acc: 0.8829\n",
      "Epoch 79/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5962 - acc: 0.8474 - val_loss: 0.4920 - val_acc: 0.8826\n",
      "Epoch 80/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5897 - acc: 0.8493 - val_loss: 0.4860 - val_acc: 0.8858\n",
      "Epoch 81/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5869 - acc: 0.8470 - val_loss: 0.4761 - val_acc: 0.8886\n",
      "Epoch 82/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5713 - acc: 0.8523 - val_loss: 0.4877 - val_acc: 0.8820\n",
      "Epoch 83/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5799 - acc: 0.8501 - val_loss: 0.4911 - val_acc: 0.8843\n",
      "Epoch 84/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5708 - acc: 0.8521 - val_loss: 0.4748 - val_acc: 0.8885\n",
      "Epoch 85/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5667 - acc: 0.8513 - val_loss: 0.4805 - val_acc: 0.8842\n",
      "Epoch 86/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5677 - acc: 0.8520 - val_loss: 0.4675 - val_acc: 0.8858\n",
      "Epoch 87/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5537 - acc: 0.8549 - val_loss: 0.4713 - val_acc: 0.8847\n",
      "Epoch 88/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5667 - acc: 0.8520 - val_loss: 0.4694 - val_acc: 0.8878\n",
      "Epoch 89/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5627 - acc: 0.8526 - val_loss: 0.4810 - val_acc: 0.8814\n",
      "Epoch 90/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5565 - acc: 0.8538 - val_loss: 0.4680 - val_acc: 0.8862\n",
      "Epoch 91/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5535 - acc: 0.8546 - val_loss: 0.4712 - val_acc: 0.8847\n",
      "Epoch 92/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5423 - acc: 0.8576 - val_loss: 0.4586 - val_acc: 0.8902\n",
      "Epoch 93/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5474 - acc: 0.8566 - val_loss: 0.4638 - val_acc: 0.8858\n",
      "Epoch 94/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5499 - acc: 0.8535 - val_loss: 0.4542 - val_acc: 0.8877\n",
      "Epoch 95/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5362 - acc: 0.8586 - val_loss: 0.4561 - val_acc: 0.8869\n",
      "Epoch 96/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5433 - acc: 0.8543 - val_loss: 0.4426 - val_acc: 0.8915\n",
      "Epoch 97/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5360 - acc: 0.8568 - val_loss: 0.4597 - val_acc: 0.8866\n",
      "Epoch 98/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5389 - acc: 0.8566 - val_loss: 0.4537 - val_acc: 0.8865\n",
      "Epoch 99/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5435 - acc: 0.8535 - val_loss: 0.4490 - val_acc: 0.8913\n",
      "Epoch 100/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5295 - acc: 0.8574 - val_loss: 0.4588 - val_acc: 0.8837\n",
      "Epoch 101/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5126 - acc: 0.8649 - val_loss: 0.4349 - val_acc: 0.8947\n",
      "Epoch 102/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5037 - acc: 0.8662 - val_loss: 0.4344 - val_acc: 0.8927\n",
      "Epoch 103/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5049 - acc: 0.8671 - val_loss: 0.4230 - val_acc: 0.8981\n",
      "Epoch 104/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4946 - acc: 0.8680 - val_loss: 0.4370 - val_acc: 0.8915\n",
      "Epoch 105/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4985 - acc: 0.8676 - val_loss: 0.4267 - val_acc: 0.8978\n",
      "Epoch 106/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4964 - acc: 0.8688 - val_loss: 0.4269 - val_acc: 0.8958\n",
      "Epoch 107/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4917 - acc: 0.8703 - val_loss: 0.4228 - val_acc: 0.8984\n",
      "Epoch 108/200\n",
      "390/390 [==============================] - 176s 452ms/step - loss: 0.4929 - acc: 0.8693 - val_loss: 0.4239 - val_acc: 0.8957\n",
      "Epoch 109/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.4857 - acc: 0.8693 - val_loss: 0.4249 - val_acc: 0.8974\n",
      "Epoch 110/200\n",
      "390/390 [==============================] - 176s 452ms/step - loss: 0.4896 - acc: 0.8700 - val_loss: 0.4235 - val_acc: 0.8971\n",
      "Epoch 111/200\n",
      "390/390 [==============================] - 176s 452ms/step - loss: 0.4832 - acc: 0.8697 - val_loss: 0.4233 - val_acc: 0.8974\n",
      "Epoch 112/200\n",
      "390/390 [==============================] - 175s 448ms/step - loss: 0.4821 - acc: 0.8717 - val_loss: 0.4369 - val_acc: 0.8953\n",
      "Epoch 113/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4878 - acc: 0.8698 - val_loss: 0.4254 - val_acc: 0.8970\n",
      "Epoch 114/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4785 - acc: 0.8715 - val_loss: 0.4166 - val_acc: 0.8970\n",
      "Epoch 115/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4844 - acc: 0.8698 - val_loss: 0.4253 - val_acc: 0.8967\n",
      "Epoch 116/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4799 - acc: 0.8711 - val_loss: 0.4380 - val_acc: 0.8919\n",
      "Epoch 117/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4786 - acc: 0.8706 - val_loss: 0.4211 - val_acc: 0.8948\n",
      "Epoch 118/200\n",
      "390/390 [==============================] - 175s 447ms/step - loss: 0.4725 - acc: 0.8729 - val_loss: 0.4296 - val_acc: 0.8936\n",
      "Epoch 119/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4752 - acc: 0.8712 - val_loss: 0.4253 - val_acc: 0.8922\n",
      "Epoch 120/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4740 - acc: 0.8720 - val_loss: 0.4342 - val_acc: 0.8906\n",
      "Epoch 121/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4714 - acc: 0.8740 - val_loss: 0.4373 - val_acc: 0.8936\n",
      "Epoch 122/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4708 - acc: 0.8729 - val_loss: 0.4101 - val_acc: 0.8962\n",
      "Epoch 123/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4622 - acc: 0.8761 - val_loss: 0.4203 - val_acc: 0.8945\n",
      "Epoch 124/200\n",
      "390/390 [==============================] - 175s 447ms/step - loss: 0.4684 - acc: 0.8729 - val_loss: 0.4186 - val_acc: 0.8958\n",
      "Epoch 125/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4613 - acc: 0.8746 - val_loss: 0.4081 - val_acc: 0.8997\n",
      "Epoch 126/200\n",
      "390/390 [==============================] - 177s 455ms/step - loss: 0.4521 - acc: 0.8776 - val_loss: 0.4075 - val_acc: 0.8980\n",
      "Epoch 127/200\n",
      "390/390 [==============================] - 177s 453ms/step - loss: 0.4516 - acc: 0.8782 - val_loss: 0.4002 - val_acc: 0.8997\n",
      "Epoch 128/200\n",
      "390/390 [==============================] - 181s 464ms/step - loss: 0.4431 - acc: 0.8817 - val_loss: 0.4046 - val_acc: 0.9000\n",
      "Epoch 129/200\n",
      "390/390 [==============================] - 179s 458ms/step - loss: 0.4360 - acc: 0.8839 - val_loss: 0.3950 - val_acc: 0.9007\n",
      "Epoch 130/200\n",
      "390/390 [==============================] - 179s 458ms/step - loss: 0.4390 - acc: 0.8811 - val_loss: 0.3987 - val_acc: 0.9011\n",
      "Epoch 131/200\n",
      "390/390 [==============================] - 178s 457ms/step - loss: 0.4398 - acc: 0.8813 - val_loss: 0.3989 - val_acc: 0.8995\n",
      "Epoch 132/200\n",
      "390/390 [==============================] - 177s 455ms/step - loss: 0.4388 - acc: 0.8825 - val_loss: 0.4080 - val_acc: 0.8963\n",
      "Epoch 133/200\n",
      "390/390 [==============================] - 176s 452ms/step - loss: 0.4423 - acc: 0.8800 - val_loss: 0.4072 - val_acc: 0.8986\n",
      "Epoch 134/200\n",
      "390/390 [==============================] - 176s 450ms/step - loss: 0.4373 - acc: 0.8807 - val_loss: 0.4090 - val_acc: 0.8962\n",
      "Epoch 135/200\n",
      "390/390 [==============================] - 177s 455ms/step - loss: 0.4330 - acc: 0.8823 - val_loss: 0.4040 - val_acc: 0.8990\n",
      "Epoch 136/200\n",
      "390/390 [==============================] - 177s 454ms/step - loss: 0.4277 - acc: 0.8842 - val_loss: 0.3929 - val_acc: 0.9014\n",
      "Epoch 137/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.4341 - acc: 0.8825 - val_loss: 0.4060 - val_acc: 0.9006\n",
      "Epoch 138/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.4285 - acc: 0.8839 - val_loss: 0.3915 - val_acc: 0.9013\n",
      "Epoch 139/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.4296 - acc: 0.8833 - val_loss: 0.4029 - val_acc: 0.9015\n",
      "Epoch 140/200\n",
      "390/390 [==============================] - 176s 450ms/step - loss: 0.4305 - acc: 0.8839 - val_loss: 0.4011 - val_acc: 0.9009\n",
      "Epoch 141/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4266 - acc: 0.8849 - val_loss: 0.3881 - val_acc: 0.9016\n",
      "Epoch 142/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4307 - acc: 0.8837 - val_loss: 0.3954 - val_acc: 0.9019\n",
      "Epoch 143/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4255 - acc: 0.8869 - val_loss: 0.3977 - val_acc: 0.9002\n",
      "Epoch 144/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4284 - acc: 0.8838 - val_loss: 0.4010 - val_acc: 0.9011\n",
      "Epoch 145/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4236 - acc: 0.8841 - val_loss: 0.3886 - val_acc: 0.9028\n",
      "Epoch 146/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4223 - acc: 0.8849 - val_loss: 0.3961 - val_acc: 0.9010\n",
      "Epoch 147/200\n",
      "390/390 [==============================] - 175s 447ms/step - loss: 0.4214 - acc: 0.8852 - val_loss: 0.3977 - val_acc: 0.9002\n",
      "Epoch 148/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4208 - acc: 0.8858 - val_loss: 0.3911 - val_acc: 0.9024\n",
      "Epoch 149/200\n",
      "390/390 [==============================] - 175s 447ms/step - loss: 0.4228 - acc: 0.8851 - val_loss: 0.3914 - val_acc: 0.9003\n",
      "Epoch 150/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4217 - acc: 0.8861 - val_loss: 0.3957 - val_acc: 0.9004\n",
      "Epoch 151/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4115 - acc: 0.8871 - val_loss: 0.3882 - val_acc: 0.9029\n",
      "Epoch 152/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4149 - acc: 0.8877 - val_loss: 0.3919 - val_acc: 0.9037\n",
      "Epoch 153/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4113 - acc: 0.8888 - val_loss: 0.3920 - val_acc: 0.9013\n",
      "Epoch 154/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4110 - acc: 0.8882 - val_loss: 0.3838 - val_acc: 0.9035\n",
      "Epoch 155/200\n",
      "390/390 [==============================] - 176s 452ms/step - loss: 0.4095 - acc: 0.8899 - val_loss: 0.3948 - val_acc: 0.9024\n",
      "Epoch 156/200\n",
      "390/390 [==============================] - 176s 452ms/step - loss: 0.4084 - acc: 0.8893 - val_loss: 0.3909 - val_acc: 0.9042\n",
      "Epoch 157/200\n",
      "390/390 [==============================] - 176s 452ms/step - loss: 0.4045 - acc: 0.8903 - val_loss: 0.3851 - val_acc: 0.9046\n",
      "Epoch 158/200\n",
      "390/390 [==============================] - 176s 450ms/step - loss: 0.4040 - acc: 0.8916 - val_loss: 0.3865 - val_acc: 0.9032\n",
      "Epoch 159/200\n",
      "390/390 [==============================] - 175s 449ms/step - loss: 0.4052 - acc: 0.8896 - val_loss: 0.3857 - val_acc: 0.9046\n",
      "Epoch 160/200\n",
      "390/390 [==============================] - 175s 450ms/step - loss: 0.4019 - acc: 0.8913 - val_loss: 0.3871 - val_acc: 0.9056\n",
      "Epoch 161/200\n",
      "390/390 [==============================] - 175s 449ms/step - loss: 0.4055 - acc: 0.8913 - val_loss: 0.3808 - val_acc: 0.9062\n",
      "Epoch 162/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4025 - acc: 0.8923 - val_loss: 0.3901 - val_acc: 0.9044\n",
      "Epoch 163/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4029 - acc: 0.8910 - val_loss: 0.3853 - val_acc: 0.9044\n",
      "Epoch 164/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4027 - acc: 0.8910 - val_loss: 0.3823 - val_acc: 0.9061\n",
      "Epoch 165/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3982 - acc: 0.8928 - val_loss: 0.3831 - val_acc: 0.9062\n",
      "Epoch 166/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4029 - acc: 0.8903 - val_loss: 0.3881 - val_acc: 0.9056\n",
      "Epoch 167/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3985 - acc: 0.8912 - val_loss: 0.3836 - val_acc: 0.9070\n",
      "Epoch 168/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3989 - acc: 0.8925 - val_loss: 0.3874 - val_acc: 0.9035\n",
      "Epoch 169/200\n",
      "390/390 [==============================] - 177s 453ms/step - loss: 0.4048 - acc: 0.8895 - val_loss: 0.3850 - val_acc: 0.9034\n",
      "Epoch 170/200\n",
      "390/390 [==============================] - 177s 454ms/step - loss: 0.3951 - acc: 0.8937 - val_loss: 0.3849 - val_acc: 0.9045\n",
      "Epoch 171/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.3978 - acc: 0.8920 - val_loss: 0.3835 - val_acc: 0.9052\n",
      "Epoch 172/200\n",
      "390/390 [==============================] - 177s 453ms/step - loss: 0.3997 - acc: 0.8901 - val_loss: 0.3865 - val_acc: 0.9044\n",
      "Epoch 173/200\n",
      "390/390 [==============================] - 177s 453ms/step - loss: 0.3989 - acc: 0.8913 - val_loss: 0.3794 - val_acc: 0.9049\n",
      "Epoch 174/200\n",
      "390/390 [==============================] - 176s 450ms/step - loss: 0.3992 - acc: 0.8915 - val_loss: 0.3835 - val_acc: 0.9040\n",
      "Epoch 175/200\n",
      "390/390 [==============================] - 175s 449ms/step - loss: 0.3927 - acc: 0.8924 - val_loss: 0.3854 - val_acc: 0.9039\n",
      "Epoch 176/200\n",
      "390/390 [==============================] - 175s 450ms/step - loss: 0.3928 - acc: 0.8938 - val_loss: 0.3804 - val_acc: 0.9056\n",
      "Epoch 177/200\n",
      "390/390 [==============================] - 175s 449ms/step - loss: 0.3919 - acc: 0.8931 - val_loss: 0.3817 - val_acc: 0.9057\n",
      "Epoch 178/200\n",
      "390/390 [==============================] - 175s 450ms/step - loss: 0.3892 - acc: 0.8952 - val_loss: 0.3801 - val_acc: 0.9065\n",
      "Epoch 179/200\n",
      "390/390 [==============================] - 175s 450ms/step - loss: 0.3919 - acc: 0.8941 - val_loss: 0.3808 - val_acc: 0.9056\n",
      "Epoch 180/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.3892 - acc: 0.8936 - val_loss: 0.3816 - val_acc: 0.9055\n",
      "Epoch 181/200\n",
      "390/390 [==============================] - 176s 452ms/step - loss: 0.3855 - acc: 0.8959 - val_loss: 0.3810 - val_acc: 0.9069\n",
      "Epoch 182/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.3895 - acc: 0.8943 - val_loss: 0.3812 - val_acc: 0.9051\n",
      "Epoch 183/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.3922 - acc: 0.8943 - val_loss: 0.3776 - val_acc: 0.9068\n",
      "Epoch 184/200\n",
      "390/390 [==============================] - 177s 453ms/step - loss: 0.3883 - acc: 0.8951 - val_loss: 0.3796 - val_acc: 0.9074\n",
      "Epoch 185/200\n",
      "390/390 [==============================] - 180s 461ms/step - loss: 0.3942 - acc: 0.8928 - val_loss: 0.3794 - val_acc: 0.9056\n",
      "Epoch 186/200\n",
      "390/390 [==============================] - 179s 459ms/step - loss: 0.3870 - acc: 0.8960 - val_loss: 0.3777 - val_acc: 0.9072\n",
      "Epoch 187/200\n",
      "390/390 [==============================] - 177s 454ms/step - loss: 0.3899 - acc: 0.8941 - val_loss: 0.3780 - val_acc: 0.9066\n",
      "Epoch 188/200\n",
      "390/390 [==============================] - 178s 456ms/step - loss: 0.3853 - acc: 0.8971 - val_loss: 0.3778 - val_acc: 0.9066\n",
      "Epoch 189/200\n",
      "390/390 [==============================] - 178s 458ms/step - loss: 0.3878 - acc: 0.8958 - val_loss: 0.3792 - val_acc: 0.9076\n",
      "Epoch 190/200\n",
      "390/390 [==============================] - 177s 453ms/step - loss: 0.3873 - acc: 0.8956 - val_loss: 0.3781 - val_acc: 0.9085\n",
      "Epoch 191/200\n",
      "390/390 [==============================] - 176s 450ms/step - loss: 0.3876 - acc: 0.8948 - val_loss: 0.3769 - val_acc: 0.9078\n",
      "Epoch 192/200\n",
      "390/390 [==============================] - 175s 450ms/step - loss: 0.3837 - acc: 0.8973 - val_loss: 0.3768 - val_acc: 0.9075\n",
      "Epoch 193/200\n",
      "390/390 [==============================] - 175s 449ms/step - loss: 0.3876 - acc: 0.8946 - val_loss: 0.3755 - val_acc: 0.9085\n",
      "Epoch 194/200\n",
      "390/390 [==============================] - 175s 448ms/step - loss: 0.3926 - acc: 0.8941 - val_loss: 0.3756 - val_acc: 0.9087\n",
      "Epoch 195/200\n",
      "390/390 [==============================] - 175s 448ms/step - loss: 0.3839 - acc: 0.8957 - val_loss: 0.3782 - val_acc: 0.9072\n",
      "Epoch 196/200\n",
      "390/390 [==============================] - 175s 449ms/step - loss: 0.3867 - acc: 0.8934 - val_loss: 0.3760 - val_acc: 0.9079\n",
      "Epoch 197/200\n",
      "390/390 [==============================] - 175s 450ms/step - loss: 0.3884 - acc: 0.8948 - val_loss: 0.3788 - val_acc: 0.9064\n",
      "Epoch 198/200\n",
      "390/390 [==============================] - 175s 449ms/step - loss: 0.3857 - acc: 0.8943 - val_loss: 0.3773 - val_acc: 0.9082\n",
      "Epoch 199/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.3882 - acc: 0.8937 - val_loss: 0.3750 - val_acc: 0.9078\n",
      "Epoch 200/200\n",
      "390/390 [==============================] - 179s 458ms/step - loss: 0.3839 - acc: 0.8945 - val_loss: 0.3787 - val_acc: 0.9079\n",
      "{'loss': [2.0201412051543235, 1.4730862967280345, 1.2979482970713498, 1.2147280058112708, 1.139269012727147, 1.1029223366199796, 1.0239467595990437, 0.9956905530514073, 1.0515424083125946, 1.0200822336917739, 1.0034755292824533, 0.95621412508679693, 0.92128756541948809, 0.92403700344448558, 0.94516318151114798, 0.92964696276061687, 0.94772970659211919, 0.92375287334238987, 0.92339898792642561, 0.87195690629724742, 0.859061746097047, 0.82688941326820586, 0.8256743555463657, 0.81982975744994591, 0.80444548877340349, 0.81845859334680715, 0.80536214093730585, 0.84325594457633068, 0.79105372026557907, 0.78110432831339249, 0.77067945223035128, 0.76768239300947827, 0.78279785326669182, 0.77768564098035065, 0.76009057261907864, 0.75718800974534728, 0.75644770315300014, 0.77648020761135295, 0.75273463773130223, 0.74871308745426135, 0.75726116016889233, 0.74442003553466451, 0.73964915829742039, 0.72267108520789902, 0.7226381308381512, 0.72409669221652251, 0.72532766114682545, 0.72381119559972718, 0.71486470597098095, 0.70444184782645014, 0.71877365401818427, 0.72133107475259961, 0.70614163073202263, 0.70862732673249718, 0.7107884484987993, 0.70803516726120208, 0.69877851644233901, 0.69822317631458064, 0.70533661596431618, 0.69062833964614334, 0.68457893972256112, 0.70421360586559234, 0.6949754285628803, 0.68565615660722801, 0.67631265889156467, 0.68436918155023374, 0.66813396508777068, 0.6790378262953074, 0.67522547011349421, 0.67955225855255497, 0.67753855320441247, 0.67637158430727928, 0.67057134340970947, 0.67663838314795688, 0.66971177993676601, 0.62225125015126492, 0.60931481372709639, 0.59555977097221546, 0.59627075963269682, 0.58966724574565887, 0.58701672822753814, 0.57123861715967239, 0.57973551568519932, 0.57068959424462806, 0.56646132809556671, 0.56766898072909422, 0.55372014759364785, 0.566767399495406, 0.56289558686772556, 0.55642736568105189, 0.55349861041911341, 0.54233321058444484, 0.54742682563929357, 0.54975251402654512, 0.53632926970557748, 0.5433435326967484, 0.53580377313596128, 0.53897503449590234, 0.54353466469507949, 0.52961370136671282, 0.51228564876199301, 0.50358446919080502, 0.50495722298060697, 0.49455489364999738, 0.49852474362260452, 0.49637954658447719, 0.49158382002726786, 0.49286298759472674, 0.48583477858977969, 0.48968430696713222, 0.48323571493620288, 0.48210255549504205, 0.48791403813663492, 0.47857385481784542, 0.48428838068886149, 0.47997421168692034, 0.47857773732328246, 0.47253711353509853, 0.47528362542840202, 0.47406349281183147, 0.47121877701296283, 0.47093387618828247, 0.46223789463067688, 0.46835307028049078, 0.46146275175827473, 0.45219135473735983, 0.45157169251630119, 0.44313616960294205, 0.43602040043243995, 0.43912871536070824, 0.43975773578964444, 0.43873938171382437, 0.44233149824998319, 0.43719054341201491, 0.43305040742481293, 0.42767494873848955, 0.43417735627763043, 0.42849205241663779, 0.42967979190706784, 0.43050655899500673, 0.42658326962055304, 0.43048616991107402, 0.42554172776928689, 0.42844590824844364, 0.42353532736524008, 0.42230899435494013, 0.42144462462419119, 0.42079654560880775, 0.42279730167908547, 0.42176053559967519, 0.41150930931572738, 0.41490956218264829, 0.41129129154559896, 0.4111400498131107, 0.40955936027177448, 0.40845767976306824, 0.40462306337858339, 0.40404183234159763, 0.40509160658297971, 0.40194697009447294, 0.40556918224349725, 0.40244734728990322, 0.40287989163804139, 0.40252238456584111, 0.39835389512526825, 0.40276435301553987, 0.39822499628513719, 0.39888648662499826, 0.40480971340189237, 0.39509131220670846, 0.39762133907337716, 0.39969274554497158, 0.39883648013180467, 0.39916594200409378, 0.39281760699755119, 0.39283689818321132, 0.39197154339781776, 0.38883699598263377, 0.39187523818168885, 0.38914170988530933, 0.38552773503156806, 0.38947023925086721, 0.39224786360867331, 0.3883218056007498, 0.39438599584457884, 0.38695328732331596, 0.38983766656122532, 0.38517484679832498, 0.38784106591591516, 0.38731854684077777, 0.38764726293531071, 0.38371324646167265, 0.38736854988409297, 0.39255421047027295, 0.38406787209772653, 0.3866855384908045, 0.38842436121060298, 0.38571512767011856, 0.38810719323533066, 0.38372915907593669], 'val_acc': [0.55630000000000002, 0.63370000000000004, 0.66779999999999995, 0.68560000133514409, 0.63580000000000003, 0.75670000000000004, 0.76280000000000003, 0.72829999999999995, 0.72799999999999998, 0.78449999999999998, 0.78029999999999999, 0.78959999999999997, 0.80179999999999996, 0.78359999999999996, 0.8085, 0.8135, 0.81510000000000005, 0.80940000000000001, 0.81889999999999996, 0.82140000000000002, 0.81720000000000004, 0.83430000000000004, 0.82799999999999996, 0.81969999999999998, 0.82930000000000004, 0.83520000000000005, 0.82699999999999996, 0.83179999999999998, 0.83779999999999999, 0.84509999999999996, 0.82709999999999995, 0.84640000000000004, 0.83099999999999996, 0.84799999999999998, 0.84209999999999996, 0.83650000000000002, 0.84640000000000004, 0.83709999999999996, 0.85199999999999998, 0.84069999885559077, 0.84930000000000005, 0.85309999999999997, 0.85270000000000001, 0.85960000000000003, 0.85460000000000003, 0.84699999999999998, 0.85350000000000004, 0.8468, 0.85429999999999995, 0.85070000000000001, 0.86709999999999998, 0.8629, 0.86060000000000003, 0.86529999999999996, 0.85929999999999995, 0.85370000000000001, 0.86119999999999997, 0.8599, 0.86080000000000001, 0.85460000000000003, 0.86529999999999996, 0.85809999999999997, 0.86109999999999998, 0.86629999999999996, 0.86050000000000004, 0.86180000000000001, 0.87250000000000005, 0.85560000000000003, 0.86270000000000002, 0.86950000000000005, 0.87180000000000002, 0.86119999999999997, 0.86380000000000001, 0.87029999999999996, 0.872, 0.88280000000000003, 0.88470000000000004, 0.88290000000000002, 0.88260000000000005, 0.88580000000000003, 0.88859999999999995, 0.88200000000000001, 0.88429999999999997, 0.88849999999999996, 0.88419999999999999, 0.88580000000000003, 0.88470000000000004, 0.88780000000000003, 0.88139999999999996, 0.88619999999999999, 0.88470000000000004, 0.89019999999999999, 0.88580000000000003, 0.88770000000000004, 0.88690000000000002, 0.89149999999999996, 0.88660000000000005, 0.88649999999999995, 0.89129999999999998, 0.88370000000000004, 0.89470000000000005, 0.89270000000000005, 0.89810000000000001, 0.89149999999999996, 0.89780000000000004, 0.89580000000000004, 0.89839999999999998, 0.89570000000000005, 0.89739999999999998, 0.89710000000000001, 0.89739999999999998, 0.89529999999999998, 0.89700000000000002, 0.89700000000000002, 0.89670000000000005, 0.89190000000000003, 0.89480000000000004, 0.89359999999999995, 0.89219999999999999, 0.89059999999999995, 0.89359999999999995, 0.8962, 0.89449999999999996, 0.89580000000000004, 0.89970000000000006, 0.89800000000000002, 0.89970000000000006, 0.90000000000000002, 0.90069999999999995, 0.90110000000000001, 0.89949999999999997, 0.89629999999999999, 0.89859999999999995, 0.8962, 0.89900000000000002, 0.90139999999999998, 0.90059999999999996, 0.90129999999999999, 0.90149999999999997, 0.90090000000000003, 0.90159999999999996, 0.90190000000000003, 0.9002, 0.90110000000000001, 0.90280000000000005, 0.90100000000000002, 0.9002, 0.90239999999999998, 0.90029999999999999, 0.90039999999999998, 0.90290000000000004, 0.90369999999999995, 0.90129999999999999, 0.90349999999999997, 0.90239999999999998, 0.9042, 0.90459999999999996, 0.9032, 0.90459999999999996, 0.90559999999999996, 0.90620000000000001, 0.90439999999999998, 0.90439999999999998, 0.90610000000000002, 0.90620000000000001, 0.90559999999999996, 0.90700000000000003, 0.90349999999999997, 0.90339999999999998, 0.90449999999999997, 0.9052, 0.90439999999999998, 0.90490000000000004, 0.90400000000000003, 0.90390000000000004, 0.90559999999999996, 0.90569999999999995, 0.90649999999999997, 0.90559999999999996, 0.90549999999999997, 0.90690000000000004, 0.90510000000000002, 0.90680000000000005, 0.90739999999999998, 0.90559999999999996, 0.90720000000000001, 0.90659999999999996, 0.90659999999999996, 0.90759999999999996, 0.90849999999999997, 0.90780000000000005, 0.90749999999999997, 0.90849999999999997, 0.90869999999999995, 0.90720000000000001, 0.90790000000000004, 0.90639999999999998, 0.90820000000000001, 0.90780000000000005, 0.90790000000000004], 'val_loss': [1.3828066045761109, 1.1149481589317323, 1.0800856992721557, 0.97542974424362183, 1.5834138217926026, 0.79725623674392698, 0.78560814018249514, 1.2214744918823242, 1.6160156175613403, 0.79388591880798343, 0.74454604263305668, 0.73505972900390626, 0.68965172548294063, 0.73329002418518063, 0.67607630386352535, 0.65875459671020509, 0.70115664048194881, 0.69093471612930302, 0.67816493215560913, 0.65818354363441467, 0.66248381967544556, 0.60771105213165288, 0.62694776649475092, 0.64422553310394282, 0.62405767631530762, 0.60759727907180783, 0.65194696111679074, 0.63301668167114256, 0.61188423109054568, 0.59650948715209962, 0.63950466070175171, 0.58104936571121213, 0.63357513017654421, 0.57316147651672367, 0.59621555624008182, 0.61514090642929076, 0.58635173683166508, 0.60856553611755371, 0.57603694801330563, 0.61638281726837163, 0.60140842514038084, 0.59149319067001338, 0.58529000835418699, 0.5628053354263306, 0.57404150180816649, 0.58506468496322628, 0.57508620924949649, 0.59566203317642208, 0.57657867317199707, 0.58575306692123408, 0.54433997879028317, 0.55463515682220454, 0.56305325117111205, 0.53474248580932615, 0.5700468914031982, 0.5745738435745239, 0.55489535708427429, 0.54529797568321225, 0.57056295785903932, 0.57702060060501104, 0.55660605654716488, 0.56287061157226559, 0.5521870588302612, 0.54272465662956237, 0.56068901176452635, 0.55527155532836914, 0.53158151836395262, 0.56585892472267152, 0.55454040355682377, 0.54332448596954341, 0.52718350553512572, 0.55550083250999449, 0.57069375772476194, 0.54018706378936765, 0.51529249505996699, 0.49314017248153685, 0.48527694230079649, 0.49399032845497132, 0.49200497441291807, 0.48597445030212405, 0.47611082477569577, 0.48771644668579101, 0.49108706951141357, 0.47482822098731997, 0.48054147644042966, 0.46746555290222169, 0.47129365463256834, 0.46943403997421262, 0.48097795677185057, 0.46799174003601074, 0.47117161979675293, 0.4585959650993347, 0.46382991485595704, 0.45420955719947814, 0.45610471382141116, 0.44262643203735352, 0.45972531499862673, 0.45369298830032351, 0.44902858839035037, 0.45879654793739316, 0.43493123550415042, 0.43437450113296511, 0.42297578268051145, 0.43704067068099978, 0.42673912315368651, 0.42688748083114625, 0.42282746934890747, 0.42388535056114196, 0.42489575619697573, 0.42348796381950377, 0.4232664306640625, 0.43689974832534789, 0.42539454760551454, 0.41659560451507566, 0.42528611431121827, 0.43801623072624207, 0.42114722270965577, 0.42956444597244264, 0.42530335659980772, 0.43420361781120298, 0.43727777376174926, 0.41013834371566771, 0.4203367037296295, 0.41857446441650392, 0.40812465415000915, 0.40748909015655516, 0.40023262891769407, 0.40463097295761108, 0.39495950913429262, 0.3986885751247406, 0.3988555054664612, 0.40797302427291871, 0.40717832231521606, 0.40899492053985598, 0.40401787815093992, 0.39292701058387758, 0.40599501729011533, 0.39150876207351687, 0.40290233316421509, 0.40105933027267454, 0.38806440310478213, 0.39535146393775938, 0.39765832982063293, 0.40104306230545045, 0.38862742729187011, 0.39606499812602997, 0.39765700259208681, 0.39107732410430907, 0.39139668631553648, 0.39573748216629029, 0.3882148777484894, 0.39186592593193054, 0.39201340079307556, 0.38379741756916047, 0.39475980768203733, 0.39089255442619325, 0.38506788740158082, 0.38652037987709048, 0.38571582112312319, 0.38711544823646543, 0.38080652561187744, 0.3900695188522339, 0.38527872791290285, 0.38229644722938538, 0.38306267404556277, 0.38811155185699464, 0.38364315128326415, 0.38736110658645628, 0.38501691925525666, 0.3849272357940674, 0.38351216254234316, 0.3865489935398102, 0.37935949974060057, 0.38349194710254669, 0.38540293841362, 0.38042077803611757, 0.38166472558975217, 0.38014628505706788, 0.38078072862625123, 0.38159099376201627, 0.38099342994689939, 0.38124294857978819, 0.37758006939888, 0.37961545908451078, 0.37935590014457704, 0.3776896659374237, 0.37800100386142732, 0.37784726963043214, 0.37919219954013822, 0.37809970812797544, 0.37685366349220278, 0.37684274215698244, 0.37554384021759035, 0.37560194835662841, 0.37815707368850709, 0.37599482469558715, 0.37884682497978212, 0.37729874968528748, 0.37496673526763918, 0.37871543755531312], 'acc': [0.39059993583573949, 0.53091915301263892, 0.59221607316637936, 0.62987247357048737, 0.65096647413564024, 0.67065688157869452, 0.69281360284234994, 0.7040623997050981, 0.70522537694590803, 0.71272457493089658, 0.71711581647109246, 0.72589829964709651, 0.73664581332024681, 0.7409167468528699, 0.73947305104883199, 0.74629050370856742, 0.74821543148552949, 0.75348893170330744, 0.75489252488277336, 0.76050689761975965, 0.76417629132486509, 0.77249759387847583, 0.77458293226846031, 0.77684873279409983, 0.77899422519743189, 0.78071863969855482, 0.78230269487981885, 0.78083894774437257, 0.78785691373731448, 0.78901989088251223, 0.79022297084350634, 0.79317051650317461, 0.79230830930998053, 0.79485482837972554, 0.79924606991992153, 0.79898540264985707, 0.79806304138594808, 0.79843750000000002, 0.79740687215182748, 0.80303577153698769, 0.80102163461538467, 0.80521435448965661, 0.80544193132511877, 0.80742701319191823, 0.80867019568187215, 0.80977301894757925, 0.80810875840243679, 0.80929487179487181, 0.81103484262697345, 0.8129210779404541, 0.81308148859172136, 0.81235964069926359, 0.81252005136965333, 0.81947786333012507, 0.8151642628205128, 0.81480812454116447, 0.81877606676907577, 0.81929740136657192, 0.81717196023728089, 0.81985883860776243, 0.82236525503381308, 0.81945781196047185, 0.8193375039911438, 0.82124238045581988, 0.82623516199563529, 0.82298684632659613, 0.82801973054835076, 0.82392925890279112, 0.82358838628797071, 0.82525264673750098, 0.8265559833172923, 0.82661613729239503, 0.82624198717948716, 0.82719171487450527, 0.82650240384615381, 0.84092003853564545, 0.84474254092383849, 0.84853224250265979, 0.84738931661238071, 0.8492588141025641, 0.84696130378933843, 0.85232194413884843, 0.85009624635251546, 0.85210137956984577, 0.85141963422547018, 0.85204122553737571, 0.85488851459736925, 0.85194096886121118, 0.85256256013487175, 0.85382579401360137, 0.85456769325658988, 0.85757211538461542, 0.85657514450867056, 0.85352502409984254, 0.85859801094616917, 0.85428685897435896, 0.85683606297983594, 0.8565327237728585, 0.85354567307692308, 0.85739493098517505, 0.86496467560089474, 0.86627767081822116, 0.86707972409367984, 0.8680221366889973, 0.86760105872942084, 0.86876403595110829, 0.87034809107500499, 0.86925080128205123, 0.86921965319833161, 0.86996711577824537, 0.8696462945139557, 0.87171474358974355, 0.86982675647750896, 0.87151106829669256, 0.86986191391791756, 0.87108999037536095, 0.87066891245402933, 0.87291666666666667, 0.87117019566274967, 0.87201235165839242, 0.87407675014142427, 0.87277430217542207, 0.87604266925235952, 0.87293669871794877, 0.8746186576367343, 0.87756657045852082, 0.87816811038165055, 0.88165704204671302, 0.88389423076923079, 0.88110147723802479, 0.88127606669258607, 0.88253930060956043, 0.88000801282051277, 0.88077478348386562, 0.88227863333949608, 0.88429271030211642, 0.88249919796586607, 0.8839027911453321, 0.88328119983342657, 0.88388273977567877, 0.88489583333333333, 0.88375080282594731, 0.88687038815553121, 0.8837624318254732, 0.88414340714135531, 0.88486525505293556, 0.88519631410256405, 0.88575786761772046, 0.8851161858974359, 0.88606833489919501, 0.88708253050738595, 0.88769249274327577, 0.88882211538461542, 0.8881663455171469, 0.88989813921732286, 0.88925649661225392, 0.89029916590285829, 0.89156650641025637, 0.88963150292845516, 0.8913461538461539, 0.89125722545266683, 0.89230429900545394, 0.89100096244478522, 0.89108116782778612, 0.89276547962784725, 0.89037937115200216, 0.89132178378556448, 0.89246470963759872, 0.8894369586331744, 0.89372996794871795, 0.89201991004521219, 0.8900841346153846, 0.89127729615877282, 0.89152644230769229, 0.89234104044328688, 0.89383012820512819, 0.89312640363144347, 0.89525128454065661, 0.89409054487179485, 0.89360549132947975, 0.89591346153846152, 0.89434953480911128, 0.89426932944523285, 0.89509143403297742, 0.89268224145176323, 0.89601362179487176, 0.89410725750828213, 0.89719682384998245, 0.89575312796945483, 0.8955929487179487, 0.89474951832357241, 0.89725560897435896, 0.89458895307668296, 0.89405048076923077, 0.89571290942209225, 0.89348732759679472, 0.89475160256410258, 0.89432948347770291, 0.89376804621739003, 0.89452874109235114]}\n"
     ]
    }
   ],
   "source": [
    "# Create model and get ready - reproducible cell\n",
    "act, act_name = \"relu\", \"relu\"\n",
    "model = create(act)\n",
    "opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "\n",
    "# Training - \n",
    "batch_size = 128\n",
    "epochs=250\n",
    "# lr scheduler\n",
    "lr_1 = keras.callbacks.LearningRateScheduler(schedule)\n",
    "\n",
    "# Fit the model\n",
    "his = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,verbose=1,validation_data=(x_test,y_test), callbacks=[lr_1])\n",
    "# Save the model\n",
    "model.save('relu_at_2.h5')\n",
    "# Display logs\n",
    "print(his.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_test):\n",
    "    y_hat = np.argmax(y_pred, axis=1)\n",
    "    y = np.argmax(y_test, axis=1)\n",
    "\n",
    "    good = np.sum(np.equal(y, y_hat))\n",
    "    return float(good/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensembler = 0\n",
    "for i in range(3):\n",
    "    model = load_model('relu_at_+'str(i)'+.h5')\n",
    "    ensembler += model.predict_proba(x_test)\n",
    "    \n",
    "print(accuracy(ensembler, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
