{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Performing a large-scale experiment on cifar10 \"\"\"\n",
    "\n",
    "import keras\n",
    "import logging\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import load_model\n",
    "import keras.regularizers as regularizers\n",
    "\n",
    "# Record settings\n",
    "LOG_FORMAT = \"%(levelname)s %(asctime)s - %(message)s\"\n",
    "logging.basicConfig(filename=\"swish_first_exp_log.txt\",format = LOG_FORMAT, level = logging.DEBUG, filemode = \"a\")\n",
    "logs = logging.getLogger()\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#z-score\n",
    "mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "std = np.std(x_train,axis=(0,1,2,3))\n",
    "x_train = (x_train-mean)/(std+1e-7)\n",
    "x_test = (x_test-mean)/(std+1e-7)\n",
    "\n",
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train,num_classes)\n",
    "y_test = np_utils.to_categorical(y_test,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Swish activation function\n",
    "# x*sigmoid(x)\n",
    "def swish(x):\n",
    "    return x*K.sigmoid(x)\n",
    "\n",
    "# Custom activation function 1\n",
    "# mix between relu and positive part of swish mirrored across x=1\n",
    "def e_swish_1(x):\n",
    "    return K.maximum(0.0, x*(2-K.sigmoid(x)))\n",
    "\n",
    "# Custom activation function 2\n",
    "# positive part of swish mirrored across x=1\n",
    "def e_swish_2(x):\n",
    "    return K.maximum(x*K.sigmoid(x), x*(2-K.sigmoid(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create(act):\n",
    "    baseMapNum = 32\n",
    "    weight_decay = 1e-4\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(2*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(2*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(4*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(4*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False\n",
    "    )\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule(x):\n",
    "    if x < 75: \n",
    "        return 0.001\n",
    "    elif x < 100:\n",
    "        return 0.0005\n",
    "    elif x < 125:\n",
    "        return 0.0003\n",
    "    elif x<150:\n",
    "        return 0.00015\n",
    "    elif x<175:\n",
    "        return 0.000075\n",
    "    elif x<200: \n",
    "        return 0.000035\n",
    "    elif x<225:\n",
    "        return 0.000017\n",
    "    else:\n",
    "        return 0.000012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "390/390 [==============================] - 181s 464ms/step - loss: 2.0738 - acc: 0.3787 - val_loss: 1.3396 - val_acc: 0.5477\n",
      "Epoch 2/200\n",
      "390/390 [==============================] - 177s 453ms/step - loss: 1.4269 - acc: 0.5314 - val_loss: 1.0368 - val_acc: 0.6504\n",
      "Epoch 3/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 1.2076 - acc: 0.6036 - val_loss: 0.8844 - val_acc: 0.7165\n",
      "Epoch 4/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 1.0982 - acc: 0.6431 - val_loss: 0.9312 - val_acc: 0.7087\n",
      "Epoch 5/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 1.0170 - acc: 0.6697 - val_loss: 0.8638 - val_acc: 0.7204\n",
      "Epoch 6/200\n",
      "390/390 [==============================] - 175s 449ms/step - loss: 0.9570 - acc: 0.6918 - val_loss: 0.7709 - val_acc: 0.7666\n",
      "Epoch 7/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.9127 - acc: 0.7112 - val_loss: 0.7347 - val_acc: 0.7747\n",
      "Epoch 8/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.8836 - acc: 0.7216 - val_loss: 0.6990 - val_acc: 0.7934\n",
      "Epoch 9/200\n",
      "390/390 [==============================] - 175s 449ms/step - loss: 0.8563 - acc: 0.7334 - val_loss: 0.6695 - val_acc: 0.8014\n",
      "Epoch 10/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.8293 - acc: 0.7423 - val_loss: 0.6457 - val_acc: 0.8079\n",
      "Epoch 11/200\n",
      "390/390 [==============================] - 176s 452ms/step - loss: 0.8102 - acc: 0.7501 - val_loss: 0.6209 - val_acc: 0.8144\n",
      "Epoch 12/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.7946 - acc: 0.7554 - val_loss: 0.6253 - val_acc: 0.8169\n",
      "Epoch 13/200\n",
      "390/390 [==============================] - 176s 450ms/step - loss: 0.7806 - acc: 0.7630 - val_loss: 0.6238 - val_acc: 0.8160\n",
      "Epoch 14/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7676 - acc: 0.7677 - val_loss: 0.6359 - val_acc: 0.8151\n",
      "Epoch 15/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7511 - acc: 0.7760 - val_loss: 0.5995 - val_acc: 0.8298\n",
      "Epoch 16/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7422 - acc: 0.7781 - val_loss: 0.5971 - val_acc: 0.8316\n",
      "Epoch 17/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7293 - acc: 0.7822 - val_loss: 0.5970 - val_acc: 0.8326\n",
      "Epoch 18/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7216 - acc: 0.7869 - val_loss: 0.5847 - val_acc: 0.8387\n",
      "Epoch 19/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7170 - acc: 0.7887 - val_loss: 0.5724 - val_acc: 0.8416\n",
      "Epoch 20/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7001 - acc: 0.7967 - val_loss: 0.5934 - val_acc: 0.8340\n",
      "Epoch 21/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7073 - acc: 0.7944 - val_loss: 0.5727 - val_acc: 0.8420\n",
      "Epoch 22/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6971 - acc: 0.7980 - val_loss: 0.5887 - val_acc: 0.8398\n",
      "Epoch 23/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6881 - acc: 0.8010 - val_loss: 0.5795 - val_acc: 0.8377\n",
      "Epoch 24/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6834 - acc: 0.8002 - val_loss: 0.5555 - val_acc: 0.8478\n",
      "Epoch 25/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6780 - acc: 0.8056 - val_loss: 0.5808 - val_acc: 0.8450\n",
      "Epoch 26/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6694 - acc: 0.8087 - val_loss: 0.5288 - val_acc: 0.8604\n",
      "Epoch 27/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6664 - acc: 0.8102 - val_loss: 0.5656 - val_acc: 0.8519\n",
      "Epoch 28/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6637 - acc: 0.8117 - val_loss: 0.5846 - val_acc: 0.8441\n",
      "Epoch 29/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6652 - acc: 0.8115 - val_loss: 0.5603 - val_acc: 0.8541\n",
      "Epoch 30/200\n",
      "390/390 [==============================] - 176s 452ms/step - loss: 0.6602 - acc: 0.8138 - val_loss: 0.5834 - val_acc: 0.8460\n",
      "Epoch 31/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.6539 - acc: 0.8143 - val_loss: 0.5562 - val_acc: 0.8533\n",
      "Epoch 32/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.6479 - acc: 0.8185 - val_loss: 0.5478 - val_acc: 0.8554\n",
      "Epoch 33/200\n",
      "390/390 [==============================] - 176s 453ms/step - loss: 0.6480 - acc: 0.8174 - val_loss: 0.5290 - val_acc: 0.8623\n",
      "Epoch 34/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.6456 - acc: 0.8186 - val_loss: 0.5237 - val_acc: 0.8662\n",
      "Epoch 35/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.6409 - acc: 0.8200 - val_loss: 0.5190 - val_acc: 0.8657\n",
      "Epoch 36/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.6417 - acc: 0.8206 - val_loss: 0.5265 - val_acc: 0.8661\n",
      "Epoch 37/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6397 - acc: 0.8211 - val_loss: 0.5297 - val_acc: 0.8629\n",
      "Epoch 38/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6375 - acc: 0.8255 - val_loss: 0.5341 - val_acc: 0.8622\n",
      "Epoch 39/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6396 - acc: 0.8250 - val_loss: 0.5483 - val_acc: 0.8583\n",
      "Epoch 40/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6333 - acc: 0.8252 - val_loss: 0.5127 - val_acc: 0.8709\n",
      "Epoch 41/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6323 - acc: 0.8232 - val_loss: 0.5198 - val_acc: 0.8644\n",
      "Epoch 42/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6191 - acc: 0.8302 - val_loss: 0.5101 - val_acc: 0.8725\n",
      "Epoch 43/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6240 - acc: 0.8282 - val_loss: 0.5391 - val_acc: 0.8662\n",
      "Epoch 44/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6241 - acc: 0.8276 - val_loss: 0.5139 - val_acc: 0.8679\n",
      "Epoch 45/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6245 - acc: 0.8274 - val_loss: 0.5194 - val_acc: 0.8676\n",
      "Epoch 46/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6174 - acc: 0.8309 - val_loss: 0.5196 - val_acc: 0.8658\n",
      "Epoch 47/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6248 - acc: 0.8296 - val_loss: 0.5104 - val_acc: 0.8718\n",
      "Epoch 48/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6160 - acc: 0.8336 - val_loss: 0.5210 - val_acc: 0.8695\n",
      "Epoch 49/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6149 - acc: 0.8317 - val_loss: 0.5055 - val_acc: 0.8755\n",
      "Epoch 50/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6095 - acc: 0.8353 - val_loss: 0.5209 - val_acc: 0.8665\n",
      "Epoch 51/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6137 - acc: 0.8347 - val_loss: 0.5293 - val_acc: 0.8643\n",
      "Epoch 52/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6149 - acc: 0.8335 - val_loss: 0.5048 - val_acc: 0.8716\n",
      "Epoch 53/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6100 - acc: 0.8361 - val_loss: 0.5130 - val_acc: 0.8726\n",
      "Epoch 54/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6075 - acc: 0.8358 - val_loss: 0.5101 - val_acc: 0.8688\n",
      "Epoch 55/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6033 - acc: 0.8362 - val_loss: 0.5189 - val_acc: 0.8666\n",
      "Epoch 56/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6120 - acc: 0.8348 - val_loss: 0.5211 - val_acc: 0.8709\n",
      "Epoch 57/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5973 - acc: 0.8390 - val_loss: 0.5192 - val_acc: 0.8670\n",
      "Epoch 58/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6051 - acc: 0.8386 - val_loss: 0.5078 - val_acc: 0.8772\n",
      "Epoch 59/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5959 - acc: 0.8397 - val_loss: 0.5162 - val_acc: 0.8681\n",
      "Epoch 60/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6022 - acc: 0.8385 - val_loss: 0.5355 - val_acc: 0.8681\n",
      "Epoch 61/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5930 - acc: 0.8403 - val_loss: 0.5117 - val_acc: 0.8745\n",
      "Epoch 62/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5985 - acc: 0.8409 - val_loss: 0.5003 - val_acc: 0.8769\n",
      "Epoch 63/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5990 - acc: 0.8394 - val_loss: 0.4987 - val_acc: 0.8739\n",
      "Epoch 64/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5972 - acc: 0.8404 - val_loss: 0.5158 - val_acc: 0.8737\n",
      "Epoch 65/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5946 - acc: 0.8408 - val_loss: 0.5010 - val_acc: 0.8760\n",
      "Epoch 66/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5931 - acc: 0.8412 - val_loss: 0.5097 - val_acc: 0.8718\n",
      "Epoch 67/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5990 - acc: 0.8399 - val_loss: 0.4921 - val_acc: 0.8805\n",
      "Epoch 68/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5945 - acc: 0.8424 - val_loss: 0.4981 - val_acc: 0.8723\n",
      "Epoch 69/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5967 - acc: 0.8404 - val_loss: 0.5170 - val_acc: 0.8749\n",
      "Epoch 70/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5886 - acc: 0.8450 - val_loss: 0.4881 - val_acc: 0.8829\n",
      "Epoch 71/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5883 - acc: 0.8448 - val_loss: 0.5076 - val_acc: 0.8767\n",
      "Epoch 72/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5882 - acc: 0.8442 - val_loss: 0.4994 - val_acc: 0.8809\n",
      "Epoch 73/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5904 - acc: 0.8448 - val_loss: 0.5110 - val_acc: 0.8738\n",
      "Epoch 74/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5892 - acc: 0.8439 - val_loss: 0.5284 - val_acc: 0.8652\n",
      "Epoch 75/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5921 - acc: 0.8422 - val_loss: 0.5199 - val_acc: 0.8696\n",
      "Epoch 76/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5479 - acc: 0.8577 - val_loss: 0.4723 - val_acc: 0.8866\n",
      "Epoch 77/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5350 - acc: 0.8624 - val_loss: 0.4661 - val_acc: 0.8872\n",
      "Epoch 78/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5290 - acc: 0.8610 - val_loss: 0.4749 - val_acc: 0.8869\n",
      "Epoch 79/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5201 - acc: 0.8647 - val_loss: 0.4587 - val_acc: 0.8902\n",
      "Epoch 80/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5172 - acc: 0.8647 - val_loss: 0.4545 - val_acc: 0.8921\n",
      "Epoch 81/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5120 - acc: 0.8671 - val_loss: 0.4536 - val_acc: 0.8896\n",
      "Epoch 82/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5096 - acc: 0.8673 - val_loss: 0.4476 - val_acc: 0.8930\n",
      "Epoch 83/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5096 - acc: 0.8676 - val_loss: 0.4417 - val_acc: 0.8924\n",
      "Epoch 84/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5062 - acc: 0.8658 - val_loss: 0.4310 - val_acc: 0.8963\n",
      "Epoch 85/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4995 - acc: 0.8675 - val_loss: 0.4582 - val_acc: 0.8878\n",
      "Epoch 86/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5029 - acc: 0.8681 - val_loss: 0.4409 - val_acc: 0.8950\n",
      "Epoch 87/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5016 - acc: 0.8660 - val_loss: 0.4393 - val_acc: 0.8952\n",
      "Epoch 88/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4947 - acc: 0.8676 - val_loss: 0.4402 - val_acc: 0.8957\n",
      "Epoch 89/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4988 - acc: 0.8681 - val_loss: 0.4408 - val_acc: 0.8913\n",
      "Epoch 90/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4920 - acc: 0.8706 - val_loss: 0.4598 - val_acc: 0.8858\n",
      "Epoch 91/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4922 - acc: 0.8701 - val_loss: 0.4346 - val_acc: 0.8926\n",
      "Epoch 92/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4867 - acc: 0.8718 - val_loss: 0.4358 - val_acc: 0.8934\n",
      "Epoch 93/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4882 - acc: 0.8698 - val_loss: 0.4363 - val_acc: 0.8902\n",
      "Epoch 94/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4898 - acc: 0.8692 - val_loss: 0.4398 - val_acc: 0.8910\n",
      "Epoch 95/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4860 - acc: 0.8715 - val_loss: 0.4347 - val_acc: 0.8945\n",
      "Epoch 96/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4859 - acc: 0.8704 - val_loss: 0.4294 - val_acc: 0.8947\n",
      "Epoch 97/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4858 - acc: 0.8717 - val_loss: 0.4414 - val_acc: 0.8877\n",
      "Epoch 98/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4723 - acc: 0.8761 - val_loss: 0.4346 - val_acc: 0.8959\n",
      "Epoch 99/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4806 - acc: 0.8716 - val_loss: 0.4317 - val_acc: 0.8894\n",
      "Epoch 100/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4771 - acc: 0.8725 - val_loss: 0.4303 - val_acc: 0.8930\n",
      "Epoch 101/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4691 - acc: 0.8759 - val_loss: 0.4073 - val_acc: 0.9027\n",
      "Epoch 102/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4557 - acc: 0.8786 - val_loss: 0.4046 - val_acc: 0.9021\n",
      "Epoch 103/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4461 - acc: 0.8828 - val_loss: 0.4059 - val_acc: 0.9004\n",
      "Epoch 104/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4506 - acc: 0.8798 - val_loss: 0.4070 - val_acc: 0.9018\n",
      "Epoch 105/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4484 - acc: 0.8827 - val_loss: 0.4061 - val_acc: 0.9037\n",
      "Epoch 106/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4457 - acc: 0.8809 - val_loss: 0.3990 - val_acc: 0.9041\n",
      "Epoch 107/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4471 - acc: 0.8810 - val_loss: 0.4061 - val_acc: 0.9027\n",
      "Epoch 108/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4377 - acc: 0.8840 - val_loss: 0.3989 - val_acc: 0.9050\n",
      "Epoch 109/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4420 - acc: 0.8824 - val_loss: 0.4043 - val_acc: 0.9026\n",
      "Epoch 110/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4411 - acc: 0.8820 - val_loss: 0.3997 - val_acc: 0.9013\n",
      "Epoch 111/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4390 - acc: 0.8825 - val_loss: 0.4076 - val_acc: 0.9025\n",
      "Epoch 112/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4378 - acc: 0.8818 - val_loss: 0.4060 - val_acc: 0.9019\n",
      "Epoch 113/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4362 - acc: 0.8830 - val_loss: 0.3946 - val_acc: 0.9047\n",
      "Epoch 114/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4329 - acc: 0.8838 - val_loss: 0.3950 - val_acc: 0.9034\n",
      "Epoch 115/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4353 - acc: 0.8835 - val_loss: 0.3903 - val_acc: 0.9013\n",
      "Epoch 116/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4328 - acc: 0.8837 - val_loss: 0.4027 - val_acc: 0.9007\n",
      "Epoch 117/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4329 - acc: 0.8836 - val_loss: 0.3977 - val_acc: 0.9026\n",
      "Epoch 118/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4287 - acc: 0.8850 - val_loss: 0.3907 - val_acc: 0.9043\n",
      "Epoch 119/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4311 - acc: 0.8829 - val_loss: 0.4071 - val_acc: 0.8989\n",
      "Epoch 120/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4293 - acc: 0.8838 - val_loss: 0.4066 - val_acc: 0.8984\n",
      "Epoch 121/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4245 - acc: 0.8864 - val_loss: 0.4008 - val_acc: 0.9011\n",
      "Epoch 122/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4226 - acc: 0.8862 - val_loss: 0.3902 - val_acc: 0.9007\n",
      "Epoch 123/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4295 - acc: 0.8836 - val_loss: 0.4070 - val_acc: 0.8995\n",
      "Epoch 124/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4223 - acc: 0.8860 - val_loss: 0.3952 - val_acc: 0.9024\n",
      "Epoch 125/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4224 - acc: 0.8862 - val_loss: 0.3888 - val_acc: 0.9026\n",
      "Epoch 126/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4120 - acc: 0.8894 - val_loss: 0.3801 - val_acc: 0.9060\n",
      "Epoch 127/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4033 - acc: 0.8921 - val_loss: 0.3826 - val_acc: 0.9075\n",
      "Epoch 128/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4016 - acc: 0.8919 - val_loss: 0.3783 - val_acc: 0.9093\n",
      "Epoch 129/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3960 - acc: 0.8961 - val_loss: 0.3833 - val_acc: 0.9066\n",
      "Epoch 130/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4076 - acc: 0.8898 - val_loss: 0.3781 - val_acc: 0.9081\n",
      "Epoch 131/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3994 - acc: 0.8931 - val_loss: 0.3823 - val_acc: 0.9070\n",
      "Epoch 132/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3933 - acc: 0.8942 - val_loss: 0.3825 - val_acc: 0.9071\n",
      "Epoch 133/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3984 - acc: 0.8931 - val_loss: 0.3786 - val_acc: 0.9066\n",
      "Epoch 134/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3906 - acc: 0.8955 - val_loss: 0.3804 - val_acc: 0.9079\n",
      "Epoch 135/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3942 - acc: 0.8936 - val_loss: 0.3854 - val_acc: 0.9065\n",
      "Epoch 136/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3917 - acc: 0.8940 - val_loss: 0.3771 - val_acc: 0.9087\n",
      "Epoch 137/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3895 - acc: 0.8944 - val_loss: 0.3792 - val_acc: 0.9085\n",
      "Epoch 138/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3935 - acc: 0.8935 - val_loss: 0.3757 - val_acc: 0.9068\n",
      "Epoch 139/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3915 - acc: 0.8926 - val_loss: 0.3780 - val_acc: 0.9084\n",
      "Epoch 140/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3897 - acc: 0.8946 - val_loss: 0.3734 - val_acc: 0.9093\n",
      "Epoch 141/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3893 - acc: 0.8962 - val_loss: 0.3782 - val_acc: 0.9086\n",
      "Epoch 142/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3918 - acc: 0.8945 - val_loss: 0.3733 - val_acc: 0.9096\n",
      "Epoch 143/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3825 - acc: 0.8968 - val_loss: 0.3733 - val_acc: 0.9079\n",
      "Epoch 144/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3880 - acc: 0.8968 - val_loss: 0.3754 - val_acc: 0.9064\n",
      "Epoch 145/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3837 - acc: 0.8958 - val_loss: 0.3733 - val_acc: 0.9075\n",
      "Epoch 146/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3863 - acc: 0.8965 - val_loss: 0.3781 - val_acc: 0.9082\n",
      "Epoch 147/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3806 - acc: 0.8982 - val_loss: 0.3708 - val_acc: 0.9096\n",
      "Epoch 148/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3798 - acc: 0.8983 - val_loss: 0.3827 - val_acc: 0.9062\n",
      "Epoch 149/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3790 - acc: 0.8982 - val_loss: 0.3792 - val_acc: 0.9059\n",
      "Epoch 150/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3845 - acc: 0.8958 - val_loss: 0.3769 - val_acc: 0.9065\n",
      "Epoch 151/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3757 - acc: 0.8990 - val_loss: 0.3710 - val_acc: 0.9084\n",
      "Epoch 152/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3753 - acc: 0.8992 - val_loss: 0.3667 - val_acc: 0.9089\n",
      "Epoch 153/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3777 - acc: 0.8979 - val_loss: 0.3694 - val_acc: 0.9082\n",
      "Epoch 154/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3701 - acc: 0.9016 - val_loss: 0.3693 - val_acc: 0.9076\n",
      "Epoch 155/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3694 - acc: 0.9005 - val_loss: 0.3673 - val_acc: 0.9090\n",
      "Epoch 156/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3658 - acc: 0.9016 - val_loss: 0.3685 - val_acc: 0.9088\n",
      "Epoch 157/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3661 - acc: 0.9015 - val_loss: 0.3656 - val_acc: 0.9093\n",
      "Epoch 158/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3675 - acc: 0.9023 - val_loss: 0.3692 - val_acc: 0.9102\n",
      "Epoch 159/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3704 - acc: 0.8999 - val_loss: 0.3653 - val_acc: 0.9095\n",
      "Epoch 160/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3600 - acc: 0.9032 - val_loss: 0.3660 - val_acc: 0.9105\n",
      "Epoch 161/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3649 - acc: 0.9033 - val_loss: 0.3668 - val_acc: 0.9091\n",
      "Epoch 162/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3672 - acc: 0.9009 - val_loss: 0.3667 - val_acc: 0.9109\n",
      "Epoch 163/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3621 - acc: 0.9018 - val_loss: 0.3613 - val_acc: 0.9107\n",
      "Epoch 164/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3634 - acc: 0.9027 - val_loss: 0.3636 - val_acc: 0.9114\n",
      "Epoch 165/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3643 - acc: 0.9029 - val_loss: 0.3636 - val_acc: 0.9086\n",
      "Epoch 166/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3633 - acc: 0.9015 - val_loss: 0.3660 - val_acc: 0.9109\n",
      "Epoch 167/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3656 - acc: 0.9013 - val_loss: 0.3653 - val_acc: 0.9101\n",
      "Epoch 168/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3562 - acc: 0.9058 - val_loss: 0.3678 - val_acc: 0.9097\n",
      "Epoch 169/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3578 - acc: 0.9045 - val_loss: 0.3601 - val_acc: 0.9119\n",
      "Epoch 170/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3656 - acc: 0.9004 - val_loss: 0.3641 - val_acc: 0.9106\n",
      "Epoch 171/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3629 - acc: 0.9014 - val_loss: 0.3638 - val_acc: 0.9112\n",
      "Epoch 172/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3626 - acc: 0.9024 - val_loss: 0.3631 - val_acc: 0.9109\n",
      "Epoch 173/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3599 - acc: 0.9024 - val_loss: 0.3660 - val_acc: 0.9090\n",
      "Epoch 174/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3592 - acc: 0.9043 - val_loss: 0.3651 - val_acc: 0.9098\n",
      "Epoch 175/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3592 - acc: 0.9028 - val_loss: 0.3653 - val_acc: 0.9098\n",
      "Epoch 176/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3566 - acc: 0.9041 - val_loss: 0.3640 - val_acc: 0.9092\n",
      "Epoch 177/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3496 - acc: 0.9073 - val_loss: 0.3636 - val_acc: 0.9112\n",
      "Epoch 178/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3600 - acc: 0.9033 - val_loss: 0.3636 - val_acc: 0.9116\n",
      "Epoch 179/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3510 - acc: 0.9057 - val_loss: 0.3639 - val_acc: 0.9126\n",
      "Epoch 180/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3477 - acc: 0.9073 - val_loss: 0.3602 - val_acc: 0.9134\n",
      "Epoch 181/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3510 - acc: 0.9048 - val_loss: 0.3616 - val_acc: 0.9120\n",
      "Epoch 182/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3527 - acc: 0.9043 - val_loss: 0.3637 - val_acc: 0.9100\n",
      "Epoch 183/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3509 - acc: 0.9055 - val_loss: 0.3614 - val_acc: 0.9127\n",
      "Epoch 184/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3560 - acc: 0.9039 - val_loss: 0.3578 - val_acc: 0.9119\n",
      "Epoch 185/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3473 - acc: 0.9077 - val_loss: 0.3626 - val_acc: 0.9117\n",
      "Epoch 186/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3477 - acc: 0.9048 - val_loss: 0.3616 - val_acc: 0.9120\n",
      "Epoch 187/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3479 - acc: 0.9064 - val_loss: 0.3593 - val_acc: 0.9135\n",
      "Epoch 188/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3538 - acc: 0.9041 - val_loss: 0.3615 - val_acc: 0.9104\n",
      "Epoch 189/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3488 - acc: 0.9061 - val_loss: 0.3621 - val_acc: 0.9110\n",
      "Epoch 190/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3521 - acc: 0.9066 - val_loss: 0.3612 - val_acc: 0.9110\n",
      "Epoch 191/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3498 - acc: 0.9060 - val_loss: 0.3584 - val_acc: 0.9119\n",
      "Epoch 192/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3525 - acc: 0.9046 - val_loss: 0.3589 - val_acc: 0.9116\n",
      "Epoch 193/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3438 - acc: 0.9087 - val_loss: 0.3593 - val_acc: 0.9112\n",
      "Epoch 194/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3505 - acc: 0.9053 - val_loss: 0.3601 - val_acc: 0.9108\n",
      "Epoch 195/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3490 - acc: 0.9054 - val_loss: 0.3583 - val_acc: 0.9119\n",
      "Epoch 196/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3462 - acc: 0.9072 - val_loss: 0.3578 - val_acc: 0.9129\n",
      "Epoch 197/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3523 - acc: 0.9055 - val_loss: 0.3582 - val_acc: 0.9116\n",
      "Epoch 198/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3487 - acc: 0.9055 - val_loss: 0.3586 - val_acc: 0.9110\n",
      "Epoch 199/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3506 - acc: 0.9060 - val_loss: 0.3575 - val_acc: 0.9119\n",
      "Epoch 200/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3483 - acc: 0.9067 - val_loss: 0.3584 - val_acc: 0.9115\n",
      "{'loss': [2.0734573200722708, 1.4272913986343125, 1.2076745853616826, 1.0981516477042068, 1.0169589683188818, 0.95715869712187074, 0.91289464091505035, 0.88364317075235554, 0.85629041389199756, 0.82932404147381578, 0.810159541399051, 0.79464801062585144, 0.78071633098903359, 0.76763230799328341, 0.751195591976905, 0.74229052740295309, 0.72930814669535338, 0.72133725833770435, 0.71697675975270858, 0.70026242258765203, 0.70724393826552912, 0.69710732661343022, 0.68807267234384306, 0.68345903424754673, 0.67802842710505395, 0.66937143308518976, 0.6664581964480254, 0.66366271049807302, 0.66516799486176803, 0.66002509482671023, 0.65389434184865913, 0.64775469284391107, 0.64798844749819673, 0.64566635286192275, 0.64085997686502372, 0.64166368758067105, 0.6393745016155904, 0.63732218180170874, 0.6397344405199038, 0.63322198201648539, 0.63228543088877476, 0.61913234125470218, 0.6239807347456614, 0.62394296150599526, 0.62457095425519804, 0.61750554202801533, 0.62460116801599985, 0.61594944913149419, 0.61480361787332793, 0.60948962913109705, 0.61348227290442603, 0.61481083261687519, 0.61001168627902635, 0.6074811638950498, 0.60323017720882421, 0.61199409612907141, 0.5973315635360813, 0.60514873795911672, 0.59605299975193571, 0.60212511317562434, 0.59294961292008619, 0.59850617861136413, 0.59903205675656168, 0.59714004811393095, 0.59443610669482694, 0.59306523975653525, 0.59879062085023049, 0.59447584243921137, 0.59663395869785496, 0.58863143492967651, 0.58825639830082754, 0.58835604544017883, 0.5905581792845801, 0.5893061294037828, 0.59211247891694407, 0.54796255419833328, 0.53500931957879427, 0.52888614648727794, 0.52028155443490143, 0.51719522361571968, 0.51201263428040633, 0.50966396684022452, 0.50963631394581921, 0.50632508542241672, 0.49948209224600171, 0.50282809795153838, 0.50164284847048102, 0.49456201478920192, 0.49881250315751785, 0.49180508942349876, 0.49223841259714346, 0.48662643687710533, 0.48820109433534248, 0.48985894073612385, 0.48587929456708978, 0.48589203946101361, 0.48579826022632466, 0.47221682323579539, 0.48064931478255835, 0.47685572625584682, 0.46906430232219209, 0.45575906113053039, 0.4461700713879258, 0.45057222850060874, 0.44834847525981208, 0.44573038663619602, 0.4469900106916691, 0.43769112427264478, 0.4419481032430877, 0.44099376130494161, 0.43901173373063407, 0.43797963321706618, 0.43627542874007641, 0.43303058268475464, 0.43531534396685084, 0.43269517906751859, 0.43290241194107915, 0.42872551578835949, 0.43108448447384129, 0.42943025670795054, 0.42459668853476445, 0.42249587882892192, 0.42954149152898313, 0.42231949307979683, 0.42233418714908277, 0.41191306558219831, 0.40336263865318517, 0.40144907152725406, 0.39604398507814892, 0.40762043504099987, 0.39936081847587684, 0.39340615090858794, 0.39837862474796099, 0.39050939241241944, 0.39403776094475196, 0.39143706360496616, 0.38954605913177531, 0.39319860334188111, 0.39149134885549619, 0.38965504719660832, 0.3892343350726209, 0.39188619826633198, 0.38241788953361044, 0.38808055450069551, 0.38373493330600933, 0.38614812916886321, 0.38049877482536015, 0.37980267780670846, 0.37902927780762696, 0.38451488075841295, 0.37569050414439958, 0.37526940484822541, 0.37754912538526914, 0.37015875956933392, 0.36939035122975322, 0.3657338910180099, 0.36611353222819754, 0.36745645434104562, 0.37031639747639516, 0.36010831104932706, 0.36489342081841269, 0.36716131476255565, 0.36209376413774702, 0.36341125414921688, 0.36442666484791764, 0.36319759808715213, 0.36559113617275324, 0.35616497909411404, 0.35778924175512727, 0.36527863205168287, 0.36292414880158391, 0.3624769796975576, 0.35987389576740753, 0.35918245375404995, 0.35933511659507766, 0.35651463105575965, 0.34963121498242405, 0.36009678994912558, 0.35105458576020654, 0.34768558105942071, 0.35102986804674863, 0.35285754904825029, 0.35091485824340429, 0.35594421381091335, 0.3473404123614996, 0.34769193718045971, 0.34790989255576243, 0.35383106244654466, 0.34885044798929032, 0.35213492657893741, 0.3494495948442834, 0.35255998064204214, 0.34371378507260752, 0.35036139630491164, 0.349038406083092, 0.34615493519948082, 0.35236498532612198, 0.34872742692629494, 0.35065215468100425, 0.34825275968282648], 'val_acc': [0.54769999999999996, 0.65039999999999998, 0.71650000000000003, 0.7087, 0.72040000000000004, 0.76659999999999995, 0.77470000000000006, 0.79339999999999999, 0.8014, 0.80789999999999995, 0.81440000000000001, 0.81689999999999996, 0.81599999999999995, 0.81510000000000005, 0.82979999999999998, 0.83160000000000001, 0.83260000000000001, 0.8387, 0.84160000000000001, 0.83399999999999996, 0.84199999999999997, 0.83979999999999999, 0.8377, 0.8478, 0.84499999999999997, 0.86040000000000005, 0.85189999999999999, 0.84409999999999996, 0.85409999999999997, 0.84599999999999997, 0.85329999999999995, 0.85540000000000005, 0.86229999999999996, 0.86619999999999997, 0.86570000000000003, 0.86609999999999998, 0.8629, 0.86219999999999997, 0.85829999999999995, 0.87090000000000001, 0.86439999999999995, 0.87250000000000005, 0.86619999999999997, 0.8679, 0.86760000000000004, 0.86580000000000001, 0.87180000000000002, 0.86950000000000005, 0.87549999999999994, 0.86650000000000005, 0.86429999999999996, 0.87160000000000004, 0.87260000000000004, 0.86880000000000002, 0.86660000000000004, 0.87090000000000001, 0.86699999999999999, 0.87719999999999998, 0.86809999999999998, 0.86809999999999998, 0.87450000000000006, 0.87690000000000001, 0.87390000000000001, 0.87370000000000003, 0.876, 0.87180000000000002, 0.88049999999999995, 0.87229999999999996, 0.87490000000000001, 0.88290000000000002, 0.87670000000000003, 0.88090000000000002, 0.87380000000000002, 0.86519999999999997, 0.86960000000000004, 0.88660000000000005, 0.88719999999999999, 0.88690000000000002, 0.89019999999999999, 0.8921, 0.88959999999999995, 0.89300000000000002, 0.89239999999999997, 0.89629999999999999, 0.88780000000000003, 0.89500000000000002, 0.8952, 0.89570000000000005, 0.89129999999999998, 0.88580000000000003, 0.89259999999999995, 0.89339999999999997, 0.89019999999999999, 0.89100000000000001, 0.89449999999999996, 0.89470000000000005, 0.88770000000000004, 0.89590000000000003, 0.88939999999999997, 0.89300000000000002, 0.90269999999999995, 0.90210000000000001, 0.90039999999999998, 0.90180000000000005, 0.90369999999999995, 0.90410000000000001, 0.90269999999999995, 0.90500000000000003, 0.90259999999999996, 0.90129999999999999, 0.90249999999999997, 0.90190000000000003, 0.90469999999999995, 0.90339999999999998, 0.90129999999999999, 0.90069999999999995, 0.90259999999999996, 0.90429999999999999, 0.89890000000000003, 0.89839999999999998, 0.90110000000000001, 0.90069999999999995, 0.89949999999999997, 0.90239999999999998, 0.90259999999999996, 0.90600000000000003, 0.90749999999999997, 0.9093, 0.90659999999999996, 0.90810000000000002, 0.90700000000000003, 0.90710000000000002, 0.90659999999999996, 0.90790000000000004, 0.90649999999999997, 0.90869999999999995, 0.90849999999999997, 0.90680000000000005, 0.90839999999999999, 0.9093, 0.90859999999999996, 0.90959999999999996, 0.90790000000000004, 0.90639999999999998, 0.90749999999999997, 0.90820000000000001, 0.90959999999999996, 0.90620000000000001, 0.90590000000000004, 0.90649999999999997, 0.90839999999999999, 0.90890000000000004, 0.90820000000000001, 0.90759999999999996, 0.90900000000000003, 0.90880000000000005, 0.9093, 0.91020000000000001, 0.90949999999999998, 0.91049999999999998, 0.90910000000000002, 0.91090000000000004, 0.91069999999999995, 0.91139999999999999, 0.90859999999999996, 0.91090000000000004, 0.91010000000000002, 0.90969999999999995, 0.91190000000000004, 0.91059999999999997, 0.91120000000000001, 0.91090000000000004, 0.90900000000000003, 0.90980000000000005, 0.90980000000000005, 0.90920000000000001, 0.91120000000000001, 0.91159999999999997, 0.91259999999999997, 0.91339999999999999, 0.91200000000000003, 0.91000000000000003, 0.91269999999999996, 0.91190000000000004, 0.91169999999999995, 0.91200000000000003, 0.91349999999999998, 0.91039999999999999, 0.91100000000000003, 0.91100000000000003, 0.91190000000000004, 0.91159999999999997, 0.91120000000000001, 0.91080000000000005, 0.91190000000000004, 0.91290000000000004, 0.91159999999999997, 0.91100000000000003, 0.91190000000000004, 0.91149999999999998], 'val_loss': [1.3395816907882689, 1.036824674987793, 0.88435082740783688, 0.93120440979003904, 0.8638203557491303, 0.77088287916183473, 0.7347268070220947, 0.69896121788024901, 0.66950247888565062, 0.6456567057609558, 0.62085683021545413, 0.62534242610931401, 0.62375168228149414, 0.6358718013763428, 0.59948848848342895, 0.59714791660308841, 0.5970211011886597, 0.58474916620254513, 0.57242951574325562, 0.59339093112945551, 0.57266797542572023, 0.58874225473403929, 0.57950424127578737, 0.55551797285079951, 0.58082227039337153, 0.52883631553649901, 0.56560740737915038, 0.58460668830871587, 0.56031485552787785, 0.58344692306518553, 0.55621253757476807, 0.54776755428314206, 0.5290074291706085, 0.52369908804893495, 0.51896458902359011, 0.52648970451354982, 0.52973453893661504, 0.53406939239501949, 0.54827296400070191, 0.51272078857421877, 0.51982128953933715, 0.51013118715286254, 0.53914858999252324, 0.513941274356842, 0.51943699131011967, 0.51955180592536931, 0.51038287553787232, 0.521018730545044, 0.50545298891067503, 0.52094876303672788, 0.52925640172958377, 0.50477490596771235, 0.51302994508743283, 0.51010979547500612, 0.51886883096694947, 0.5210852818965912, 0.51920424418449407, 0.50777937979698184, 0.51623804712295529, 0.53554461560249333, 0.51165244073867799, 0.50033885908126829, 0.49865490980148314, 0.51582276821136475, 0.50099549393653875, 0.50966958413124086, 0.49207304477691649, 0.49810171356201172, 0.51695181131362911, 0.48807874302864074, 0.50760734276771546, 0.49938186311721799, 0.51097389163970952, 0.52835148019790645, 0.51990003910064697, 0.47226969127655027, 0.46607940959930422, 0.47491611065864564, 0.45874734654426574, 0.45454254627227786, 0.45359038715362548, 0.44759502859115602, 0.44167242918014527, 0.4310029582977295, 0.45824489669799806, 0.44088756361007692, 0.4392663055419922, 0.44022864441871645, 0.44079066748619078, 0.45984732890129087, 0.43464109807014467, 0.435813472032547, 0.43633947272300722, 0.4398235381603241, 0.43473470215797422, 0.42944779934883115, 0.44136143565177915, 0.43463298940658568, 0.43166052842140196, 0.43033595032691957, 0.40725399141311647, 0.40458147945404055, 0.40586273164749148, 0.40699335484504701, 0.40611593012809755, 0.39895054206848146, 0.40610513439178469, 0.39890421109199525, 0.40426624112129211, 0.39965541019439699, 0.40756758098602297, 0.40604037427902223, 0.39458234686851501, 0.39497776918411254, 0.39034847455024718, 0.40273576812744138, 0.39765111451148988, 0.39073427944183348, 0.40705126280784609, 0.40662605772018434, 0.40077810020446775, 0.39016827816963195, 0.4069932282924652, 0.39519649348258973, 0.38883840708732603, 0.38010552825927735, 0.38263727869987491, 0.3782632680416107, 0.3833331458091736, 0.37809216175079347, 0.38227035675048826, 0.3825279847621918, 0.37858068876266482, 0.38036592183113099, 0.38535564079284668, 0.37706580719947813, 0.37922647337913512, 0.37574307894706727, 0.37800688886642458, 0.37335607256889342, 0.378239195728302, 0.37331490230560305, 0.37329430627822874, 0.37540776324272157, 0.37331965541839601, 0.37805025749206544, 0.37077644500732421, 0.38272102737426755, 0.37916082458496092, 0.37685348167419436, 0.37096887845993043, 0.36674937996864321, 0.36935661568641664, 0.36928668422698974, 0.36732635684013365, 0.36854365067481992, 0.365602210521698, 0.36924107484817503, 0.36532255916595457, 0.36597593793869021, 0.36679173870086668, 0.36669288344383238, 0.36134689197540282, 0.36358127202987672, 0.36362908024787904, 0.36603965744972228, 0.36527157492637635, 0.36783543534278867, 0.36005092401504518, 0.36410682582855225, 0.36376677861213685, 0.36305831556320189, 0.36600811929702759, 0.36507691931724551, 0.36529902296066286, 0.36398638157844543, 0.36358087935447692, 0.36364908375740052, 0.36387764940261841, 0.36024049143791198, 0.36157977681159975, 0.36368153495788574, 0.36139390392303467, 0.35784642705917358, 0.36263097214698792, 0.36159854354858401, 0.35931023039817811, 0.36152868390083315, 0.36210155081748963, 0.36117529101371765, 0.35840856351852418, 0.35889637217521669, 0.35931753754615786, 0.36007871894836424, 0.35825858330726623, 0.35783250880241396, 0.3582201413631439, 0.35863586015701293, 0.35747763137817384, 0.35840932636260986], 'acc': [0.37876965028565779, 0.53123997429605097, 0.60352502404247521, 0.64306624963721826, 0.669794674328133, 0.69169072824523437, 0.71118062243157176, 0.72160731472569783, 0.73341754894437106, 0.74228023738864146, 0.75008012820512826, 0.75535886316017686, 0.76299326277176927, 0.76764517167764168, 0.77592637154931321, 0.7780718640100126, 0.78224254090471612, 0.78693455249252786, 0.78871912096875352, 0.79669955081193156, 0.79439364777645471, 0.79794273339749755, 0.80095043310875846, 0.800168431202953, 0.80552213670811978, 0.80871029832556651, 0.81011389158152214, 0.81173804936823568, 0.81155758738556005, 0.81382338785383235, 0.81428456849534814, 0.81849534805286839, 0.81737247353224252, 0.8186557587423805, 0.82005935196009128, 0.82061298076923073, 0.82125080288336993, 0.82555341672774951, 0.82489172277214973, 0.82525264673750098, 0.82324751363490534, 0.83018527426396882, 0.82824519230769234, 0.8276131985351376, 0.82743824186101722, 0.83086701957009945, 0.82964388835418668, 0.83367420600552111, 0.83172922683978334, 0.83525641025641029, 0.83477842005767655, 0.83355389794058088, 0.83608036577452982, 0.83579964707744481, 0.83620067376298024, 0.83475697784420766, 0.83898780875225876, 0.83850657681757956, 0.83964950268873617, 0.83858678212409066, 0.84037135067680613, 0.84094551282051277, 0.83941473982697046, 0.84035129932627528, 0.8408325312418321, 0.84116586538461535, 0.84001685940231663, 0.84242788461538465, 0.84039820165073709, 0.84503205128205128, 0.84481374436107748, 0.84416105233211725, 0.84478264356753285, 0.8439003849664406, 0.84217597046531778, 0.85775585502701612, 0.86232755851164278, 0.86104427335887224, 0.86465351295501791, 0.86466346153846152, 0.86709216447617665, 0.86728023738864146, 0.86764823717948714, 0.86573628485107179, 0.8674935774307031, 0.86810234203375336, 0.86601700352903432, 0.86768126405505441, 0.86812900641025637, 0.87062459860616714, 0.87002726984896028, 0.87181183834430842, 0.86982675649663133, 0.86924526790491008, 0.87149101696528419, 0.87035256410256412, 0.87163137628514298, 0.87608381500976085, 0.87159455128205132, 0.8725915222345596, 0.87592147435897438, 0.87854908562104295, 0.88273981388539968, 0.8797768143483593, 0.88263955722835763, 0.88088942307692308, 0.88112154786756736, 0.88402309915290489, 0.88239894125145679, 0.88205806867488123, 0.88251201923076927, 0.88178387925497748, 0.88300048123195385, 0.88374238051318721, 0.88349358974358971, 0.8837508027685248, 0.88360202117420594, 0.88502566566595786, 0.88296037853089215, 0.88376243184459569, 0.8863089509143407, 0.88618864292589028, 0.88362207254385927, 0.88601762820512819, 0.8862395632626846, 0.88939685598948004, 0.89208373432171661, 0.89194337500185783, 0.89607394934847906, 0.8898179338725668, 0.89318655762566868, 0.89418912413872165, 0.89308894230769231, 0.89547240934885963, 0.89366778950298065, 0.89408886746255722, 0.89438963743368327, 0.89360549131033895, 0.89258501764517162, 0.89463141025641024, 0.89619460500963388, 0.89438963743368327, 0.89687600252832556, 0.89671559191530315, 0.89579326923076918, 0.89655518126403588, 0.89823949314058682, 0.89834216441875403, 0.89819711538461533, 0.8958734746115592, 0.8989783653846154, 0.89922200834135391, 0.89800096337199597, 0.90154796274648408, 0.90052083333333333, 0.90160811675983166, 0.90146775747821772, 0.90225594089286942, 0.89990375359011721, 0.90321222331074902, 0.90325232599268823, 0.90090144230769231, 0.90181438669839886, 0.90274439102564108, 0.90287135063856128, 0.90155346824637417, 0.90132739810099161, 0.90576923076923077, 0.90449550854000937, 0.90052986508695909, 0.90142765475803355, 0.90239011870388197, 0.90244391025641024, 0.90425489256310854, 0.90279114533205007, 0.90416265247950744, 0.9073116987179487, 0.90327954401400279, 0.90565848572345209, 0.90726259221700201, 0.90475617581007384, 0.90421478984292436, 0.90546875000000004, 0.90392180473358874, 0.90771233974358978, 0.90480491325651591, 0.90634023099133787, 0.9041345844981683, 0.90611966636496777, 0.90663060897435899, 0.90598908152883451, 0.90463586778337846, 0.90868623674699878, 0.90539781845338763, 0.90539781843426526, 0.90723157051282055, 0.90544717407180619, 0.90550881410256412, 0.90592886965934782, 0.90665064102564108]}\n"
     ]
    }
   ],
   "source": [
    "# Create model and get ready - reproducible cell\n",
    "act, act_name = e_swish_2, \"e_swish_2\"\n",
    "model = create(act)\n",
    "opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "\n",
    "# Training\n",
    "batch_size = 128\n",
    "epochs=200\n",
    "# lr scheduler\n",
    "lr_1 = keras.callbacks.LearningRateScheduler(schedule)\n",
    "\n",
    "# Fit the model\n",
    "his = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,verbose=1,validation_data=(x_test,y_test), callbacks=[lr_1])\n",
    "# Save the model\n",
    "model.save('e_swish_2_at_2.h5')\n",
    "# Display logs\n",
    "print(his.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "390/390 [==============================] - 176s 452ms/step - loss: 2.0201 - acc: 0.3906 - val_loss: 1.3828 - val_acc: 0.5563\n",
      "Epoch 2/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 1.4731 - acc: 0.5308 - val_loss: 1.1149 - val_acc: 0.6337\n",
      "Epoch 3/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 1.2980 - acc: 0.5922 - val_loss: 1.0801 - val_acc: 0.6678\n",
      "Epoch 4/200\n",
      "390/390 [==============================] - 175s 449ms/step - loss: 1.2151 - acc: 0.6298 - val_loss: 0.9754 - val_acc: 0.6856\n",
      "Epoch 5/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 1.1390 - acc: 0.6510 - val_loss: 1.5834 - val_acc: 0.6358\n",
      "Epoch 6/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 1.1029 - acc: 0.6706 - val_loss: 0.7973 - val_acc: 0.7567\n",
      "Epoch 7/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 1.0241 - acc: 0.6928 - val_loss: 0.7856 - val_acc: 0.7628\n",
      "Epoch 8/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.9956 - acc: 0.7041 - val_loss: 1.2215 - val_acc: 0.7283\n",
      "Epoch 9/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 1.0520 - acc: 0.7052 - val_loss: 1.6160 - val_acc: 0.7280\n",
      "Epoch 10/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 1.0199 - acc: 0.7127 - val_loss: 0.7939 - val_acc: 0.7845\n",
      "Epoch 11/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 1.0037 - acc: 0.7170 - val_loss: 0.7445 - val_acc: 0.7803\n",
      "Epoch 12/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.9561 - acc: 0.7259 - val_loss: 0.7351 - val_acc: 0.7896\n",
      "Epoch 13/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.9214 - acc: 0.7366 - val_loss: 0.6897 - val_acc: 0.8018\n",
      "Epoch 14/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.9240 - acc: 0.7409 - val_loss: 0.7333 - val_acc: 0.7836\n",
      "Epoch 15/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.9455 - acc: 0.7394 - val_loss: 0.6761 - val_acc: 0.8085\n",
      "Epoch 16/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.9299 - acc: 0.7462 - val_loss: 0.6588 - val_acc: 0.8135\n",
      "Epoch 17/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.9478 - acc: 0.7482 - val_loss: 0.7012 - val_acc: 0.8151\n",
      "Epoch 18/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.9236 - acc: 0.7535 - val_loss: 0.6909 - val_acc: 0.8094\n",
      "Epoch 19/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.9234 - acc: 0.7549 - val_loss: 0.6782 - val_acc: 0.8189\n",
      "Epoch 20/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.8722 - acc: 0.7605 - val_loss: 0.6582 - val_acc: 0.8214\n",
      "Epoch 21/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.8591 - acc: 0.7642 - val_loss: 0.6625 - val_acc: 0.8172\n",
      "Epoch 22/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.8269 - acc: 0.7725 - val_loss: 0.6077 - val_acc: 0.8343\n",
      "Epoch 23/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.8260 - acc: 0.7745 - val_loss: 0.6269 - val_acc: 0.8280\n",
      "Epoch 24/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.8197 - acc: 0.7769 - val_loss: 0.6442 - val_acc: 0.8197\n",
      "Epoch 25/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.8047 - acc: 0.7790 - val_loss: 0.6241 - val_acc: 0.8293\n",
      "Epoch 26/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.8188 - acc: 0.7806 - val_loss: 0.6076 - val_acc: 0.8352\n",
      "Epoch 27/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.8053 - acc: 0.7823 - val_loss: 0.6519 - val_acc: 0.8270\n",
      "Epoch 28/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.8434 - acc: 0.7808 - val_loss: 0.6330 - val_acc: 0.8318\n",
      "Epoch 29/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7910 - acc: 0.7879 - val_loss: 0.6119 - val_acc: 0.8378\n",
      "Epoch 30/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7811 - acc: 0.7891 - val_loss: 0.5965 - val_acc: 0.8451\n",
      "Epoch 31/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7707 - acc: 0.7902 - val_loss: 0.6395 - val_acc: 0.8271\n",
      "Epoch 32/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7675 - acc: 0.7932 - val_loss: 0.5810 - val_acc: 0.8464\n",
      "Epoch 33/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7828 - acc: 0.7923 - val_loss: 0.6336 - val_acc: 0.8310\n",
      "Epoch 34/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7781 - acc: 0.7948 - val_loss: 0.5732 - val_acc: 0.8480\n",
      "Epoch 35/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7599 - acc: 0.7993 - val_loss: 0.5962 - val_acc: 0.8421\n",
      "Epoch 36/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7571 - acc: 0.7990 - val_loss: 0.6151 - val_acc: 0.8365\n",
      "Epoch 37/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7565 - acc: 0.7981 - val_loss: 0.5864 - val_acc: 0.8464\n",
      "Epoch 38/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7765 - acc: 0.7984 - val_loss: 0.6086 - val_acc: 0.8371\n",
      "Epoch 39/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7530 - acc: 0.7973 - val_loss: 0.5760 - val_acc: 0.8520\n",
      "Epoch 40/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7489 - acc: 0.8030 - val_loss: 0.6164 - val_acc: 0.8407\n",
      "Epoch 41/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7573 - acc: 0.8010 - val_loss: 0.6014 - val_acc: 0.8493\n",
      "Epoch 42/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7443 - acc: 0.8053 - val_loss: 0.5915 - val_acc: 0.8531\n",
      "Epoch 43/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7395 - acc: 0.8055 - val_loss: 0.5853 - val_acc: 0.8527\n",
      "Epoch 44/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7227 - acc: 0.8074 - val_loss: 0.5628 - val_acc: 0.8596\n",
      "Epoch 45/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7235 - acc: 0.8086 - val_loss: 0.5740 - val_acc: 0.8546\n",
      "Epoch 46/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7244 - acc: 0.8098 - val_loss: 0.5851 - val_acc: 0.8470\n",
      "Epoch 47/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7256 - acc: 0.8081 - val_loss: 0.5751 - val_acc: 0.8535\n",
      "Epoch 48/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7238 - acc: 0.8093 - val_loss: 0.5957 - val_acc: 0.8468\n",
      "Epoch 49/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7146 - acc: 0.8112 - val_loss: 0.5766 - val_acc: 0.8543\n",
      "Epoch 50/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7047 - acc: 0.8129 - val_loss: 0.5858 - val_acc: 0.8507\n",
      "Epoch 51/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7186 - acc: 0.8132 - val_loss: 0.5443 - val_acc: 0.8671\n",
      "Epoch 52/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7213 - acc: 0.8124 - val_loss: 0.5546 - val_acc: 0.8629\n",
      "Epoch 53/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7064 - acc: 0.8124 - val_loss: 0.5631 - val_acc: 0.8606\n",
      "Epoch 54/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7087 - acc: 0.8195 - val_loss: 0.5347 - val_acc: 0.8653\n",
      "Epoch 55/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.7108 - acc: 0.8152 - val_loss: 0.5700 - val_acc: 0.8593\n",
      "Epoch 56/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7082 - acc: 0.8148 - val_loss: 0.5746 - val_acc: 0.8537\n",
      "Epoch 57/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6987 - acc: 0.8188 - val_loss: 0.5549 - val_acc: 0.8612\n",
      "Epoch 58/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6983 - acc: 0.8193 - val_loss: 0.5453 - val_acc: 0.8599\n",
      "Epoch 59/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7052 - acc: 0.8172 - val_loss: 0.5706 - val_acc: 0.8608\n",
      "Epoch 60/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6905 - acc: 0.8199 - val_loss: 0.5770 - val_acc: 0.8546\n",
      "Epoch 61/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6847 - acc: 0.8224 - val_loss: 0.5566 - val_acc: 0.8653\n",
      "Epoch 62/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.7043 - acc: 0.8194 - val_loss: 0.5629 - val_acc: 0.8581\n",
      "Epoch 63/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6949 - acc: 0.8193 - val_loss: 0.5522 - val_acc: 0.8611\n",
      "Epoch 64/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6857 - acc: 0.8212 - val_loss: 0.5427 - val_acc: 0.8663\n",
      "Epoch 65/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6768 - acc: 0.8262 - val_loss: 0.5607 - val_acc: 0.8605\n",
      "Epoch 66/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6842 - acc: 0.8230 - val_loss: 0.5553 - val_acc: 0.8618\n",
      "Epoch 67/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6684 - acc: 0.8280 - val_loss: 0.5316 - val_acc: 0.8725\n",
      "Epoch 68/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6789 - acc: 0.8240 - val_loss: 0.5659 - val_acc: 0.8556\n",
      "Epoch 69/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6752 - acc: 0.8236 - val_loss: 0.5545 - val_acc: 0.8627\n",
      "Epoch 70/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6796 - acc: 0.8253 - val_loss: 0.5433 - val_acc: 0.8695\n",
      "Epoch 71/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6776 - acc: 0.8265 - val_loss: 0.5272 - val_acc: 0.8718\n",
      "Epoch 72/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6764 - acc: 0.8266 - val_loss: 0.5555 - val_acc: 0.8612\n",
      "Epoch 73/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6706 - acc: 0.8262 - val_loss: 0.5707 - val_acc: 0.8638\n",
      "Epoch 74/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6767 - acc: 0.8271 - val_loss: 0.5402 - val_acc: 0.8703\n",
      "Epoch 75/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.6697 - acc: 0.8265 - val_loss: 0.5153 - val_acc: 0.8720\n",
      "Epoch 76/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6226 - acc: 0.8408 - val_loss: 0.4931 - val_acc: 0.8828\n",
      "Epoch 77/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.6095 - acc: 0.8447 - val_loss: 0.4853 - val_acc: 0.8847\n",
      "Epoch 78/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5955 - acc: 0.8486 - val_loss: 0.4940 - val_acc: 0.8829\n",
      "Epoch 79/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5962 - acc: 0.8474 - val_loss: 0.4920 - val_acc: 0.8826\n",
      "Epoch 80/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5897 - acc: 0.8493 - val_loss: 0.4860 - val_acc: 0.8858\n",
      "Epoch 81/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5869 - acc: 0.8470 - val_loss: 0.4761 - val_acc: 0.8886\n",
      "Epoch 82/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5713 - acc: 0.8523 - val_loss: 0.4877 - val_acc: 0.8820\n",
      "Epoch 83/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5799 - acc: 0.8501 - val_loss: 0.4911 - val_acc: 0.8843\n",
      "Epoch 84/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5708 - acc: 0.8521 - val_loss: 0.4748 - val_acc: 0.8885\n",
      "Epoch 85/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5667 - acc: 0.8513 - val_loss: 0.4805 - val_acc: 0.8842\n",
      "Epoch 86/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5677 - acc: 0.8520 - val_loss: 0.4675 - val_acc: 0.8858\n",
      "Epoch 87/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5537 - acc: 0.8549 - val_loss: 0.4713 - val_acc: 0.8847\n",
      "Epoch 88/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5667 - acc: 0.8520 - val_loss: 0.4694 - val_acc: 0.8878\n",
      "Epoch 89/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5627 - acc: 0.8526 - val_loss: 0.4810 - val_acc: 0.8814\n",
      "Epoch 90/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5565 - acc: 0.8538 - val_loss: 0.4680 - val_acc: 0.8862\n",
      "Epoch 91/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5535 - acc: 0.8546 - val_loss: 0.4712 - val_acc: 0.8847\n",
      "Epoch 92/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5423 - acc: 0.8576 - val_loss: 0.4586 - val_acc: 0.8902\n",
      "Epoch 93/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5474 - acc: 0.8566 - val_loss: 0.4638 - val_acc: 0.8858\n",
      "Epoch 94/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5499 - acc: 0.8535 - val_loss: 0.4542 - val_acc: 0.8877\n",
      "Epoch 95/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5362 - acc: 0.8586 - val_loss: 0.4561 - val_acc: 0.8869\n",
      "Epoch 96/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5433 - acc: 0.8543 - val_loss: 0.4426 - val_acc: 0.8915\n",
      "Epoch 97/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5360 - acc: 0.8568 - val_loss: 0.4597 - val_acc: 0.8866\n",
      "Epoch 98/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5389 - acc: 0.8566 - val_loss: 0.4537 - val_acc: 0.8865\n",
      "Epoch 99/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.5435 - acc: 0.8535 - val_loss: 0.4490 - val_acc: 0.8913\n",
      "Epoch 100/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5295 - acc: 0.8574 - val_loss: 0.4588 - val_acc: 0.8837\n",
      "Epoch 101/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5126 - acc: 0.8649 - val_loss: 0.4349 - val_acc: 0.8947\n",
      "Epoch 102/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5037 - acc: 0.8662 - val_loss: 0.4344 - val_acc: 0.8927\n",
      "Epoch 103/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.5049 - acc: 0.8671 - val_loss: 0.4230 - val_acc: 0.8981\n",
      "Epoch 104/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4946 - acc: 0.8680 - val_loss: 0.4370 - val_acc: 0.8915\n",
      "Epoch 105/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4985 - acc: 0.8676 - val_loss: 0.4267 - val_acc: 0.8978\n",
      "Epoch 106/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4964 - acc: 0.8688 - val_loss: 0.4269 - val_acc: 0.8958\n",
      "Epoch 107/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4917 - acc: 0.8703 - val_loss: 0.4228 - val_acc: 0.8984\n",
      "Epoch 108/200\n",
      "390/390 [==============================] - 176s 452ms/step - loss: 0.4929 - acc: 0.8693 - val_loss: 0.4239 - val_acc: 0.8957\n",
      "Epoch 109/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.4857 - acc: 0.8693 - val_loss: 0.4249 - val_acc: 0.8974\n",
      "Epoch 110/200\n",
      "390/390 [==============================] - 176s 452ms/step - loss: 0.4896 - acc: 0.8700 - val_loss: 0.4235 - val_acc: 0.8971\n",
      "Epoch 111/200\n",
      "390/390 [==============================] - 176s 452ms/step - loss: 0.4832 - acc: 0.8697 - val_loss: 0.4233 - val_acc: 0.8974\n",
      "Epoch 112/200\n",
      "390/390 [==============================] - 175s 448ms/step - loss: 0.4821 - acc: 0.8717 - val_loss: 0.4369 - val_acc: 0.8953\n",
      "Epoch 113/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4878 - acc: 0.8698 - val_loss: 0.4254 - val_acc: 0.8970\n",
      "Epoch 114/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4785 - acc: 0.8715 - val_loss: 0.4166 - val_acc: 0.8970\n",
      "Epoch 115/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4844 - acc: 0.8698 - val_loss: 0.4253 - val_acc: 0.8967\n",
      "Epoch 116/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4799 - acc: 0.8711 - val_loss: 0.4380 - val_acc: 0.8919\n",
      "Epoch 117/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4786 - acc: 0.8706 - val_loss: 0.4211 - val_acc: 0.8948\n",
      "Epoch 118/200\n",
      "390/390 [==============================] - 175s 447ms/step - loss: 0.4725 - acc: 0.8729 - val_loss: 0.4296 - val_acc: 0.8936\n",
      "Epoch 119/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4752 - acc: 0.8712 - val_loss: 0.4253 - val_acc: 0.8922\n",
      "Epoch 120/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4740 - acc: 0.8720 - val_loss: 0.4342 - val_acc: 0.8906\n",
      "Epoch 121/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4714 - acc: 0.8740 - val_loss: 0.4373 - val_acc: 0.8936\n",
      "Epoch 122/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4708 - acc: 0.8729 - val_loss: 0.4101 - val_acc: 0.8962\n",
      "Epoch 123/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4622 - acc: 0.8761 - val_loss: 0.4203 - val_acc: 0.8945\n",
      "Epoch 124/200\n",
      "390/390 [==============================] - 175s 447ms/step - loss: 0.4684 - acc: 0.8729 - val_loss: 0.4186 - val_acc: 0.8958\n",
      "Epoch 125/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4613 - acc: 0.8746 - val_loss: 0.4081 - val_acc: 0.8997\n",
      "Epoch 126/200\n",
      "390/390 [==============================] - 177s 455ms/step - loss: 0.4521 - acc: 0.8776 - val_loss: 0.4075 - val_acc: 0.8980\n",
      "Epoch 127/200\n",
      "390/390 [==============================] - 177s 453ms/step - loss: 0.4516 - acc: 0.8782 - val_loss: 0.4002 - val_acc: 0.8997\n",
      "Epoch 128/200\n",
      "390/390 [==============================] - 181s 464ms/step - loss: 0.4431 - acc: 0.8817 - val_loss: 0.4046 - val_acc: 0.9000\n",
      "Epoch 129/200\n",
      "390/390 [==============================] - 179s 458ms/step - loss: 0.4360 - acc: 0.8839 - val_loss: 0.3950 - val_acc: 0.9007\n",
      "Epoch 130/200\n",
      "390/390 [==============================] - 179s 458ms/step - loss: 0.4390 - acc: 0.8811 - val_loss: 0.3987 - val_acc: 0.9011\n",
      "Epoch 131/200\n",
      "390/390 [==============================] - 178s 457ms/step - loss: 0.4398 - acc: 0.8813 - val_loss: 0.3989 - val_acc: 0.8995\n",
      "Epoch 132/200\n",
      "390/390 [==============================] - 177s 455ms/step - loss: 0.4388 - acc: 0.8825 - val_loss: 0.4080 - val_acc: 0.8963\n",
      "Epoch 133/200\n",
      "390/390 [==============================] - 176s 452ms/step - loss: 0.4423 - acc: 0.8800 - val_loss: 0.4072 - val_acc: 0.8986\n",
      "Epoch 134/200\n",
      "390/390 [==============================] - 176s 450ms/step - loss: 0.4373 - acc: 0.8807 - val_loss: 0.4090 - val_acc: 0.8962\n",
      "Epoch 135/200\n",
      "390/390 [==============================] - 177s 455ms/step - loss: 0.4330 - acc: 0.8823 - val_loss: 0.4040 - val_acc: 0.8990\n",
      "Epoch 136/200\n",
      "390/390 [==============================] - 177s 454ms/step - loss: 0.4277 - acc: 0.8842 - val_loss: 0.3929 - val_acc: 0.9014\n",
      "Epoch 137/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.4341 - acc: 0.8825 - val_loss: 0.4060 - val_acc: 0.9006\n",
      "Epoch 138/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.4285 - acc: 0.8839 - val_loss: 0.3915 - val_acc: 0.9013\n",
      "Epoch 139/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.4296 - acc: 0.8833 - val_loss: 0.4029 - val_acc: 0.9015\n",
      "Epoch 140/200\n",
      "390/390 [==============================] - 176s 450ms/step - loss: 0.4305 - acc: 0.8839 - val_loss: 0.4011 - val_acc: 0.9009\n",
      "Epoch 141/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4266 - acc: 0.8849 - val_loss: 0.3881 - val_acc: 0.9016\n",
      "Epoch 142/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4307 - acc: 0.8837 - val_loss: 0.3954 - val_acc: 0.9019\n",
      "Epoch 143/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4255 - acc: 0.8869 - val_loss: 0.3977 - val_acc: 0.9002\n",
      "Epoch 144/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4284 - acc: 0.8838 - val_loss: 0.4010 - val_acc: 0.9011\n",
      "Epoch 145/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4236 - acc: 0.8841 - val_loss: 0.3886 - val_acc: 0.9028\n",
      "Epoch 146/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4223 - acc: 0.8849 - val_loss: 0.3961 - val_acc: 0.9010\n",
      "Epoch 147/200\n",
      "390/390 [==============================] - 175s 447ms/step - loss: 0.4214 - acc: 0.8852 - val_loss: 0.3977 - val_acc: 0.9002\n",
      "Epoch 148/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4208 - acc: 0.8858 - val_loss: 0.3911 - val_acc: 0.9024\n",
      "Epoch 149/200\n",
      "390/390 [==============================] - 175s 447ms/step - loss: 0.4228 - acc: 0.8851 - val_loss: 0.3914 - val_acc: 0.9003\n",
      "Epoch 150/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4217 - acc: 0.8861 - val_loss: 0.3957 - val_acc: 0.9004\n",
      "Epoch 151/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4115 - acc: 0.8871 - val_loss: 0.3882 - val_acc: 0.9029\n",
      "Epoch 152/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4149 - acc: 0.8877 - val_loss: 0.3919 - val_acc: 0.9037\n",
      "Epoch 153/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4113 - acc: 0.8888 - val_loss: 0.3920 - val_acc: 0.9013\n",
      "Epoch 154/200\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.4110 - acc: 0.8882 - val_loss: 0.3838 - val_acc: 0.9035\n",
      "Epoch 155/200\n",
      "390/390 [==============================] - 176s 452ms/step - loss: 0.4095 - acc: 0.8899 - val_loss: 0.3948 - val_acc: 0.9024\n",
      "Epoch 156/200\n",
      "390/390 [==============================] - 176s 452ms/step - loss: 0.4084 - acc: 0.8893 - val_loss: 0.3909 - val_acc: 0.9042\n",
      "Epoch 157/200\n",
      "390/390 [==============================] - 176s 452ms/step - loss: 0.4045 - acc: 0.8903 - val_loss: 0.3851 - val_acc: 0.9046\n",
      "Epoch 158/200\n",
      "390/390 [==============================] - 176s 450ms/step - loss: 0.4040 - acc: 0.8916 - val_loss: 0.3865 - val_acc: 0.9032\n",
      "Epoch 159/200\n",
      "390/390 [==============================] - 175s 449ms/step - loss: 0.4052 - acc: 0.8896 - val_loss: 0.3857 - val_acc: 0.9046\n",
      "Epoch 160/200\n",
      "390/390 [==============================] - 175s 450ms/step - loss: 0.4019 - acc: 0.8913 - val_loss: 0.3871 - val_acc: 0.9056\n",
      "Epoch 161/200\n",
      "390/390 [==============================] - 175s 449ms/step - loss: 0.4055 - acc: 0.8913 - val_loss: 0.3808 - val_acc: 0.9062\n",
      "Epoch 162/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4025 - acc: 0.8923 - val_loss: 0.3901 - val_acc: 0.9044\n",
      "Epoch 163/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4029 - acc: 0.8910 - val_loss: 0.3853 - val_acc: 0.9044\n",
      "Epoch 164/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4027 - acc: 0.8910 - val_loss: 0.3823 - val_acc: 0.9061\n",
      "Epoch 165/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3982 - acc: 0.8928 - val_loss: 0.3831 - val_acc: 0.9062\n",
      "Epoch 166/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.4029 - acc: 0.8903 - val_loss: 0.3881 - val_acc: 0.9056\n",
      "Epoch 167/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3985 - acc: 0.8912 - val_loss: 0.3836 - val_acc: 0.9070\n",
      "Epoch 168/200\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3989 - acc: 0.8925 - val_loss: 0.3874 - val_acc: 0.9035\n",
      "Epoch 169/200\n",
      "390/390 [==============================] - 177s 453ms/step - loss: 0.4048 - acc: 0.8895 - val_loss: 0.3850 - val_acc: 0.9034\n",
      "Epoch 170/200\n",
      "390/390 [==============================] - 177s 454ms/step - loss: 0.3951 - acc: 0.8937 - val_loss: 0.3849 - val_acc: 0.9045\n",
      "Epoch 171/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.3978 - acc: 0.8920 - val_loss: 0.3835 - val_acc: 0.9052\n",
      "Epoch 172/200\n",
      "390/390 [==============================] - 177s 453ms/step - loss: 0.3997 - acc: 0.8901 - val_loss: 0.3865 - val_acc: 0.9044\n",
      "Epoch 173/200\n",
      "390/390 [==============================] - 177s 453ms/step - loss: 0.3989 - acc: 0.8913 - val_loss: 0.3794 - val_acc: 0.9049\n",
      "Epoch 174/200\n",
      "390/390 [==============================] - 176s 450ms/step - loss: 0.3992 - acc: 0.8915 - val_loss: 0.3835 - val_acc: 0.9040\n",
      "Epoch 175/200\n",
      "390/390 [==============================] - 175s 449ms/step - loss: 0.3927 - acc: 0.8924 - val_loss: 0.3854 - val_acc: 0.9039\n",
      "Epoch 176/200\n",
      "390/390 [==============================] - 175s 450ms/step - loss: 0.3928 - acc: 0.8938 - val_loss: 0.3804 - val_acc: 0.9056\n",
      "Epoch 177/200\n",
      "390/390 [==============================] - 175s 449ms/step - loss: 0.3919 - acc: 0.8931 - val_loss: 0.3817 - val_acc: 0.9057\n",
      "Epoch 178/200\n",
      "390/390 [==============================] - 175s 450ms/step - loss: 0.3892 - acc: 0.8952 - val_loss: 0.3801 - val_acc: 0.9065\n",
      "Epoch 179/200\n",
      "390/390 [==============================] - 175s 450ms/step - loss: 0.3919 - acc: 0.8941 - val_loss: 0.3808 - val_acc: 0.9056\n",
      "Epoch 180/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.3892 - acc: 0.8936 - val_loss: 0.3816 - val_acc: 0.9055\n",
      "Epoch 181/200\n",
      "390/390 [==============================] - 176s 452ms/step - loss: 0.3855 - acc: 0.8959 - val_loss: 0.3810 - val_acc: 0.9069\n",
      "Epoch 182/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.3895 - acc: 0.8943 - val_loss: 0.3812 - val_acc: 0.9051\n",
      "Epoch 183/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.3922 - acc: 0.8943 - val_loss: 0.3776 - val_acc: 0.9068\n",
      "Epoch 184/200\n",
      "390/390 [==============================] - 177s 453ms/step - loss: 0.3883 - acc: 0.8951 - val_loss: 0.3796 - val_acc: 0.9074\n",
      "Epoch 185/200\n",
      "390/390 [==============================] - 180s 461ms/step - loss: 0.3942 - acc: 0.8928 - val_loss: 0.3794 - val_acc: 0.9056\n",
      "Epoch 186/200\n",
      "390/390 [==============================] - 179s 459ms/step - loss: 0.3870 - acc: 0.8960 - val_loss: 0.3777 - val_acc: 0.9072\n",
      "Epoch 187/200\n",
      "390/390 [==============================] - 177s 454ms/step - loss: 0.3899 - acc: 0.8941 - val_loss: 0.3780 - val_acc: 0.9066\n",
      "Epoch 188/200\n",
      "390/390 [==============================] - 178s 456ms/step - loss: 0.3853 - acc: 0.8971 - val_loss: 0.3778 - val_acc: 0.9066\n",
      "Epoch 189/200\n",
      "390/390 [==============================] - 178s 458ms/step - loss: 0.3878 - acc: 0.8958 - val_loss: 0.3792 - val_acc: 0.9076\n",
      "Epoch 190/200\n",
      "390/390 [==============================] - 177s 453ms/step - loss: 0.3873 - acc: 0.8956 - val_loss: 0.3781 - val_acc: 0.9085\n",
      "Epoch 191/200\n",
      "390/390 [==============================] - 176s 450ms/step - loss: 0.3876 - acc: 0.8948 - val_loss: 0.3769 - val_acc: 0.9078\n",
      "Epoch 192/200\n",
      "390/390 [==============================] - 175s 450ms/step - loss: 0.3837 - acc: 0.8973 - val_loss: 0.3768 - val_acc: 0.9075\n",
      "Epoch 193/200\n",
      "390/390 [==============================] - 175s 449ms/step - loss: 0.3876 - acc: 0.8946 - val_loss: 0.3755 - val_acc: 0.9085\n",
      "Epoch 194/200\n",
      "390/390 [==============================] - 175s 448ms/step - loss: 0.3926 - acc: 0.8941 - val_loss: 0.3756 - val_acc: 0.9087\n",
      "Epoch 195/200\n",
      "390/390 [==============================] - 175s 448ms/step - loss: 0.3839 - acc: 0.8957 - val_loss: 0.3782 - val_acc: 0.9072\n",
      "Epoch 196/200\n",
      "390/390 [==============================] - 175s 449ms/step - loss: 0.3867 - acc: 0.8934 - val_loss: 0.3760 - val_acc: 0.9079\n",
      "Epoch 197/200\n",
      "390/390 [==============================] - 175s 450ms/step - loss: 0.3884 - acc: 0.8948 - val_loss: 0.3788 - val_acc: 0.9064\n",
      "Epoch 198/200\n",
      "390/390 [==============================] - 175s 449ms/step - loss: 0.3857 - acc: 0.8943 - val_loss: 0.3773 - val_acc: 0.9082\n",
      "Epoch 199/200\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.3882 - acc: 0.8937 - val_loss: 0.3750 - val_acc: 0.9078\n",
      "Epoch 200/200\n",
      "390/390 [==============================] - 179s 458ms/step - loss: 0.3839 - acc: 0.8945 - val_loss: 0.3787 - val_acc: 0.9079\n",
      "{'loss': [2.0201412051543235, 1.4730862967280345, 1.2979482970713498, 1.2147280058112708, 1.139269012727147, 1.1029223366199796, 1.0239467595990437, 0.9956905530514073, 1.0515424083125946, 1.0200822336917739, 1.0034755292824533, 0.95621412508679693, 0.92128756541948809, 0.92403700344448558, 0.94516318151114798, 0.92964696276061687, 0.94772970659211919, 0.92375287334238987, 0.92339898792642561, 0.87195690629724742, 0.859061746097047, 0.82688941326820586, 0.8256743555463657, 0.81982975744994591, 0.80444548877340349, 0.81845859334680715, 0.80536214093730585, 0.84325594457633068, 0.79105372026557907, 0.78110432831339249, 0.77067945223035128, 0.76768239300947827, 0.78279785326669182, 0.77768564098035065, 0.76009057261907864, 0.75718800974534728, 0.75644770315300014, 0.77648020761135295, 0.75273463773130223, 0.74871308745426135, 0.75726116016889233, 0.74442003553466451, 0.73964915829742039, 0.72267108520789902, 0.7226381308381512, 0.72409669221652251, 0.72532766114682545, 0.72381119559972718, 0.71486470597098095, 0.70444184782645014, 0.71877365401818427, 0.72133107475259961, 0.70614163073202263, 0.70862732673249718, 0.7107884484987993, 0.70803516726120208, 0.69877851644233901, 0.69822317631458064, 0.70533661596431618, 0.69062833964614334, 0.68457893972256112, 0.70421360586559234, 0.6949754285628803, 0.68565615660722801, 0.67631265889156467, 0.68436918155023374, 0.66813396508777068, 0.6790378262953074, 0.67522547011349421, 0.67955225855255497, 0.67753855320441247, 0.67637158430727928, 0.67057134340970947, 0.67663838314795688, 0.66971177993676601, 0.62225125015126492, 0.60931481372709639, 0.59555977097221546, 0.59627075963269682, 0.58966724574565887, 0.58701672822753814, 0.57123861715967239, 0.57973551568519932, 0.57068959424462806, 0.56646132809556671, 0.56766898072909422, 0.55372014759364785, 0.566767399495406, 0.56289558686772556, 0.55642736568105189, 0.55349861041911341, 0.54233321058444484, 0.54742682563929357, 0.54975251402654512, 0.53632926970557748, 0.5433435326967484, 0.53580377313596128, 0.53897503449590234, 0.54353466469507949, 0.52961370136671282, 0.51228564876199301, 0.50358446919080502, 0.50495722298060697, 0.49455489364999738, 0.49852474362260452, 0.49637954658447719, 0.49158382002726786, 0.49286298759472674, 0.48583477858977969, 0.48968430696713222, 0.48323571493620288, 0.48210255549504205, 0.48791403813663492, 0.47857385481784542, 0.48428838068886149, 0.47997421168692034, 0.47857773732328246, 0.47253711353509853, 0.47528362542840202, 0.47406349281183147, 0.47121877701296283, 0.47093387618828247, 0.46223789463067688, 0.46835307028049078, 0.46146275175827473, 0.45219135473735983, 0.45157169251630119, 0.44313616960294205, 0.43602040043243995, 0.43912871536070824, 0.43975773578964444, 0.43873938171382437, 0.44233149824998319, 0.43719054341201491, 0.43305040742481293, 0.42767494873848955, 0.43417735627763043, 0.42849205241663779, 0.42967979190706784, 0.43050655899500673, 0.42658326962055304, 0.43048616991107402, 0.42554172776928689, 0.42844590824844364, 0.42353532736524008, 0.42230899435494013, 0.42144462462419119, 0.42079654560880775, 0.42279730167908547, 0.42176053559967519, 0.41150930931572738, 0.41490956218264829, 0.41129129154559896, 0.4111400498131107, 0.40955936027177448, 0.40845767976306824, 0.40462306337858339, 0.40404183234159763, 0.40509160658297971, 0.40194697009447294, 0.40556918224349725, 0.40244734728990322, 0.40287989163804139, 0.40252238456584111, 0.39835389512526825, 0.40276435301553987, 0.39822499628513719, 0.39888648662499826, 0.40480971340189237, 0.39509131220670846, 0.39762133907337716, 0.39969274554497158, 0.39883648013180467, 0.39916594200409378, 0.39281760699755119, 0.39283689818321132, 0.39197154339781776, 0.38883699598263377, 0.39187523818168885, 0.38914170988530933, 0.38552773503156806, 0.38947023925086721, 0.39224786360867331, 0.3883218056007498, 0.39438599584457884, 0.38695328732331596, 0.38983766656122532, 0.38517484679832498, 0.38784106591591516, 0.38731854684077777, 0.38764726293531071, 0.38371324646167265, 0.38736854988409297, 0.39255421047027295, 0.38406787209772653, 0.3866855384908045, 0.38842436121060298, 0.38571512767011856, 0.38810719323533066, 0.38372915907593669], 'val_acc': [0.55630000000000002, 0.63370000000000004, 0.66779999999999995, 0.68560000133514409, 0.63580000000000003, 0.75670000000000004, 0.76280000000000003, 0.72829999999999995, 0.72799999999999998, 0.78449999999999998, 0.78029999999999999, 0.78959999999999997, 0.80179999999999996, 0.78359999999999996, 0.8085, 0.8135, 0.81510000000000005, 0.80940000000000001, 0.81889999999999996, 0.82140000000000002, 0.81720000000000004, 0.83430000000000004, 0.82799999999999996, 0.81969999999999998, 0.82930000000000004, 0.83520000000000005, 0.82699999999999996, 0.83179999999999998, 0.83779999999999999, 0.84509999999999996, 0.82709999999999995, 0.84640000000000004, 0.83099999999999996, 0.84799999999999998, 0.84209999999999996, 0.83650000000000002, 0.84640000000000004, 0.83709999999999996, 0.85199999999999998, 0.84069999885559077, 0.84930000000000005, 0.85309999999999997, 0.85270000000000001, 0.85960000000000003, 0.85460000000000003, 0.84699999999999998, 0.85350000000000004, 0.8468, 0.85429999999999995, 0.85070000000000001, 0.86709999999999998, 0.8629, 0.86060000000000003, 0.86529999999999996, 0.85929999999999995, 0.85370000000000001, 0.86119999999999997, 0.8599, 0.86080000000000001, 0.85460000000000003, 0.86529999999999996, 0.85809999999999997, 0.86109999999999998, 0.86629999999999996, 0.86050000000000004, 0.86180000000000001, 0.87250000000000005, 0.85560000000000003, 0.86270000000000002, 0.86950000000000005, 0.87180000000000002, 0.86119999999999997, 0.86380000000000001, 0.87029999999999996, 0.872, 0.88280000000000003, 0.88470000000000004, 0.88290000000000002, 0.88260000000000005, 0.88580000000000003, 0.88859999999999995, 0.88200000000000001, 0.88429999999999997, 0.88849999999999996, 0.88419999999999999, 0.88580000000000003, 0.88470000000000004, 0.88780000000000003, 0.88139999999999996, 0.88619999999999999, 0.88470000000000004, 0.89019999999999999, 0.88580000000000003, 0.88770000000000004, 0.88690000000000002, 0.89149999999999996, 0.88660000000000005, 0.88649999999999995, 0.89129999999999998, 0.88370000000000004, 0.89470000000000005, 0.89270000000000005, 0.89810000000000001, 0.89149999999999996, 0.89780000000000004, 0.89580000000000004, 0.89839999999999998, 0.89570000000000005, 0.89739999999999998, 0.89710000000000001, 0.89739999999999998, 0.89529999999999998, 0.89700000000000002, 0.89700000000000002, 0.89670000000000005, 0.89190000000000003, 0.89480000000000004, 0.89359999999999995, 0.89219999999999999, 0.89059999999999995, 0.89359999999999995, 0.8962, 0.89449999999999996, 0.89580000000000004, 0.89970000000000006, 0.89800000000000002, 0.89970000000000006, 0.90000000000000002, 0.90069999999999995, 0.90110000000000001, 0.89949999999999997, 0.89629999999999999, 0.89859999999999995, 0.8962, 0.89900000000000002, 0.90139999999999998, 0.90059999999999996, 0.90129999999999999, 0.90149999999999997, 0.90090000000000003, 0.90159999999999996, 0.90190000000000003, 0.9002, 0.90110000000000001, 0.90280000000000005, 0.90100000000000002, 0.9002, 0.90239999999999998, 0.90029999999999999, 0.90039999999999998, 0.90290000000000004, 0.90369999999999995, 0.90129999999999999, 0.90349999999999997, 0.90239999999999998, 0.9042, 0.90459999999999996, 0.9032, 0.90459999999999996, 0.90559999999999996, 0.90620000000000001, 0.90439999999999998, 0.90439999999999998, 0.90610000000000002, 0.90620000000000001, 0.90559999999999996, 0.90700000000000003, 0.90349999999999997, 0.90339999999999998, 0.90449999999999997, 0.9052, 0.90439999999999998, 0.90490000000000004, 0.90400000000000003, 0.90390000000000004, 0.90559999999999996, 0.90569999999999995, 0.90649999999999997, 0.90559999999999996, 0.90549999999999997, 0.90690000000000004, 0.90510000000000002, 0.90680000000000005, 0.90739999999999998, 0.90559999999999996, 0.90720000000000001, 0.90659999999999996, 0.90659999999999996, 0.90759999999999996, 0.90849999999999997, 0.90780000000000005, 0.90749999999999997, 0.90849999999999997, 0.90869999999999995, 0.90720000000000001, 0.90790000000000004, 0.90639999999999998, 0.90820000000000001, 0.90780000000000005, 0.90790000000000004], 'val_loss': [1.3828066045761109, 1.1149481589317323, 1.0800856992721557, 0.97542974424362183, 1.5834138217926026, 0.79725623674392698, 0.78560814018249514, 1.2214744918823242, 1.6160156175613403, 0.79388591880798343, 0.74454604263305668, 0.73505972900390626, 0.68965172548294063, 0.73329002418518063, 0.67607630386352535, 0.65875459671020509, 0.70115664048194881, 0.69093471612930302, 0.67816493215560913, 0.65818354363441467, 0.66248381967544556, 0.60771105213165288, 0.62694776649475092, 0.64422553310394282, 0.62405767631530762, 0.60759727907180783, 0.65194696111679074, 0.63301668167114256, 0.61188423109054568, 0.59650948715209962, 0.63950466070175171, 0.58104936571121213, 0.63357513017654421, 0.57316147651672367, 0.59621555624008182, 0.61514090642929076, 0.58635173683166508, 0.60856553611755371, 0.57603694801330563, 0.61638281726837163, 0.60140842514038084, 0.59149319067001338, 0.58529000835418699, 0.5628053354263306, 0.57404150180816649, 0.58506468496322628, 0.57508620924949649, 0.59566203317642208, 0.57657867317199707, 0.58575306692123408, 0.54433997879028317, 0.55463515682220454, 0.56305325117111205, 0.53474248580932615, 0.5700468914031982, 0.5745738435745239, 0.55489535708427429, 0.54529797568321225, 0.57056295785903932, 0.57702060060501104, 0.55660605654716488, 0.56287061157226559, 0.5521870588302612, 0.54272465662956237, 0.56068901176452635, 0.55527155532836914, 0.53158151836395262, 0.56585892472267152, 0.55454040355682377, 0.54332448596954341, 0.52718350553512572, 0.55550083250999449, 0.57069375772476194, 0.54018706378936765, 0.51529249505996699, 0.49314017248153685, 0.48527694230079649, 0.49399032845497132, 0.49200497441291807, 0.48597445030212405, 0.47611082477569577, 0.48771644668579101, 0.49108706951141357, 0.47482822098731997, 0.48054147644042966, 0.46746555290222169, 0.47129365463256834, 0.46943403997421262, 0.48097795677185057, 0.46799174003601074, 0.47117161979675293, 0.4585959650993347, 0.46382991485595704, 0.45420955719947814, 0.45610471382141116, 0.44262643203735352, 0.45972531499862673, 0.45369298830032351, 0.44902858839035037, 0.45879654793739316, 0.43493123550415042, 0.43437450113296511, 0.42297578268051145, 0.43704067068099978, 0.42673912315368651, 0.42688748083114625, 0.42282746934890747, 0.42388535056114196, 0.42489575619697573, 0.42348796381950377, 0.4232664306640625, 0.43689974832534789, 0.42539454760551454, 0.41659560451507566, 0.42528611431121827, 0.43801623072624207, 0.42114722270965577, 0.42956444597244264, 0.42530335659980772, 0.43420361781120298, 0.43727777376174926, 0.41013834371566771, 0.4203367037296295, 0.41857446441650392, 0.40812465415000915, 0.40748909015655516, 0.40023262891769407, 0.40463097295761108, 0.39495950913429262, 0.3986885751247406, 0.3988555054664612, 0.40797302427291871, 0.40717832231521606, 0.40899492053985598, 0.40401787815093992, 0.39292701058387758, 0.40599501729011533, 0.39150876207351687, 0.40290233316421509, 0.40105933027267454, 0.38806440310478213, 0.39535146393775938, 0.39765832982063293, 0.40104306230545045, 0.38862742729187011, 0.39606499812602997, 0.39765700259208681, 0.39107732410430907, 0.39139668631553648, 0.39573748216629029, 0.3882148777484894, 0.39186592593193054, 0.39201340079307556, 0.38379741756916047, 0.39475980768203733, 0.39089255442619325, 0.38506788740158082, 0.38652037987709048, 0.38571582112312319, 0.38711544823646543, 0.38080652561187744, 0.3900695188522339, 0.38527872791290285, 0.38229644722938538, 0.38306267404556277, 0.38811155185699464, 0.38364315128326415, 0.38736110658645628, 0.38501691925525666, 0.3849272357940674, 0.38351216254234316, 0.3865489935398102, 0.37935949974060057, 0.38349194710254669, 0.38540293841362, 0.38042077803611757, 0.38166472558975217, 0.38014628505706788, 0.38078072862625123, 0.38159099376201627, 0.38099342994689939, 0.38124294857978819, 0.37758006939888, 0.37961545908451078, 0.37935590014457704, 0.3776896659374237, 0.37800100386142732, 0.37784726963043214, 0.37919219954013822, 0.37809970812797544, 0.37685366349220278, 0.37684274215698244, 0.37554384021759035, 0.37560194835662841, 0.37815707368850709, 0.37599482469558715, 0.37884682497978212, 0.37729874968528748, 0.37496673526763918, 0.37871543755531312], 'acc': [0.39059993583573949, 0.53091915301263892, 0.59221607316637936, 0.62987247357048737, 0.65096647413564024, 0.67065688157869452, 0.69281360284234994, 0.7040623997050981, 0.70522537694590803, 0.71272457493089658, 0.71711581647109246, 0.72589829964709651, 0.73664581332024681, 0.7409167468528699, 0.73947305104883199, 0.74629050370856742, 0.74821543148552949, 0.75348893170330744, 0.75489252488277336, 0.76050689761975965, 0.76417629132486509, 0.77249759387847583, 0.77458293226846031, 0.77684873279409983, 0.77899422519743189, 0.78071863969855482, 0.78230269487981885, 0.78083894774437257, 0.78785691373731448, 0.78901989088251223, 0.79022297084350634, 0.79317051650317461, 0.79230830930998053, 0.79485482837972554, 0.79924606991992153, 0.79898540264985707, 0.79806304138594808, 0.79843750000000002, 0.79740687215182748, 0.80303577153698769, 0.80102163461538467, 0.80521435448965661, 0.80544193132511877, 0.80742701319191823, 0.80867019568187215, 0.80977301894757925, 0.80810875840243679, 0.80929487179487181, 0.81103484262697345, 0.8129210779404541, 0.81308148859172136, 0.81235964069926359, 0.81252005136965333, 0.81947786333012507, 0.8151642628205128, 0.81480812454116447, 0.81877606676907577, 0.81929740136657192, 0.81717196023728089, 0.81985883860776243, 0.82236525503381308, 0.81945781196047185, 0.8193375039911438, 0.82124238045581988, 0.82623516199563529, 0.82298684632659613, 0.82801973054835076, 0.82392925890279112, 0.82358838628797071, 0.82525264673750098, 0.8265559833172923, 0.82661613729239503, 0.82624198717948716, 0.82719171487450527, 0.82650240384615381, 0.84092003853564545, 0.84474254092383849, 0.84853224250265979, 0.84738931661238071, 0.8492588141025641, 0.84696130378933843, 0.85232194413884843, 0.85009624635251546, 0.85210137956984577, 0.85141963422547018, 0.85204122553737571, 0.85488851459736925, 0.85194096886121118, 0.85256256013487175, 0.85382579401360137, 0.85456769325658988, 0.85757211538461542, 0.85657514450867056, 0.85352502409984254, 0.85859801094616917, 0.85428685897435896, 0.85683606297983594, 0.8565327237728585, 0.85354567307692308, 0.85739493098517505, 0.86496467560089474, 0.86627767081822116, 0.86707972409367984, 0.8680221366889973, 0.86760105872942084, 0.86876403595110829, 0.87034809107500499, 0.86925080128205123, 0.86921965319833161, 0.86996711577824537, 0.8696462945139557, 0.87171474358974355, 0.86982675647750896, 0.87151106829669256, 0.86986191391791756, 0.87108999037536095, 0.87066891245402933, 0.87291666666666667, 0.87117019566274967, 0.87201235165839242, 0.87407675014142427, 0.87277430217542207, 0.87604266925235952, 0.87293669871794877, 0.8746186576367343, 0.87756657045852082, 0.87816811038165055, 0.88165704204671302, 0.88389423076923079, 0.88110147723802479, 0.88127606669258607, 0.88253930060956043, 0.88000801282051277, 0.88077478348386562, 0.88227863333949608, 0.88429271030211642, 0.88249919796586607, 0.8839027911453321, 0.88328119983342657, 0.88388273977567877, 0.88489583333333333, 0.88375080282594731, 0.88687038815553121, 0.8837624318254732, 0.88414340714135531, 0.88486525505293556, 0.88519631410256405, 0.88575786761772046, 0.8851161858974359, 0.88606833489919501, 0.88708253050738595, 0.88769249274327577, 0.88882211538461542, 0.8881663455171469, 0.88989813921732286, 0.88925649661225392, 0.89029916590285829, 0.89156650641025637, 0.88963150292845516, 0.8913461538461539, 0.89125722545266683, 0.89230429900545394, 0.89100096244478522, 0.89108116782778612, 0.89276547962784725, 0.89037937115200216, 0.89132178378556448, 0.89246470963759872, 0.8894369586331744, 0.89372996794871795, 0.89201991004521219, 0.8900841346153846, 0.89127729615877282, 0.89152644230769229, 0.89234104044328688, 0.89383012820512819, 0.89312640363144347, 0.89525128454065661, 0.89409054487179485, 0.89360549132947975, 0.89591346153846152, 0.89434953480911128, 0.89426932944523285, 0.89509143403297742, 0.89268224145176323, 0.89601362179487176, 0.89410725750828213, 0.89719682384998245, 0.89575312796945483, 0.8955929487179487, 0.89474951832357241, 0.89725560897435896, 0.89458895307668296, 0.89405048076923077, 0.89571290942209225, 0.89348732759679472, 0.89475160256410258, 0.89432948347770291, 0.89376804621739003, 0.89452874109235114]}\n"
     ]
    }
   ],
   "source": [
    "# Create model and get ready - reproducible cell\n",
    "act, act_name = e_swish_2, \"e_swish_2\"\n",
    "model = create(act)\n",
    "opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "\n",
    "# Training\n",
    "batch_size = 128\n",
    "epochs=200\n",
    "# lr scheduler\n",
    "lr_1 = keras.callbacks.LearningRateScheduler(schedule)\n",
    "\n",
    "# Fit the model\n",
    "his = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,verbose=1,validation_data=(x_test,y_test), callbacks=[lr_1])\n",
    "# Save the model\n",
    "model.save('e_swish_2_at_3.h5')\n",
    "# Display logs\n",
    "print(his.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Resume training (200-250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 201/250\n",
      "390/390 [==============================] - 180s 462ms/step - loss: 0.3458 - acc: 0.9072 - val_loss: 0.3603 - val_acc: 0.9113\n",
      "Epoch 202/250\n",
      "390/390 [==============================] - 179s 459ms/step - loss: 0.3383 - acc: 0.9088 - val_loss: 0.3585 - val_acc: 0.9120\n",
      "Epoch 203/250\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.3398 - acc: 0.9091 - val_loss: 0.3578 - val_acc: 0.9116\n",
      "Epoch 204/250\n",
      "390/390 [==============================] - 177s 454ms/step - loss: 0.3422 - acc: 0.9076 - val_loss: 0.3607 - val_acc: 0.9110\n",
      "Epoch 205/250\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.3449 - acc: 0.9081 - val_loss: 0.3593 - val_acc: 0.9117\n",
      "Epoch 206/250\n",
      "390/390 [==============================] - 177s 455ms/step - loss: 0.3465 - acc: 0.9064 - val_loss: 0.3588 - val_acc: 0.9120\n",
      "Epoch 207/250\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.3460 - acc: 0.9078 - val_loss: 0.3580 - val_acc: 0.9135\n",
      "Epoch 208/250\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.3442 - acc: 0.9075 - val_loss: 0.3579 - val_acc: 0.9118\n",
      "Epoch 209/250\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.3450 - acc: 0.9065 - val_loss: 0.3580 - val_acc: 0.9122\n",
      "Epoch 210/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3432 - acc: 0.9070 - val_loss: 0.3580 - val_acc: 0.9107\n",
      "Epoch 211/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3406 - acc: 0.9088 - val_loss: 0.3579 - val_acc: 0.9126\n",
      "Epoch 212/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3393 - acc: 0.9086 - val_loss: 0.3589 - val_acc: 0.9110\n",
      "Epoch 213/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3440 - acc: 0.9071 - val_loss: 0.3574 - val_acc: 0.9133\n",
      "Epoch 214/250\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.3343 - acc: 0.9112 - val_loss: 0.3576 - val_acc: 0.9126\n",
      "Epoch 215/250\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.3396 - acc: 0.9090 - val_loss: 0.3567 - val_acc: 0.9127\n",
      "Epoch 216/250\n",
      "390/390 [==============================] - 176s 451ms/step - loss: 0.3467 - acc: 0.9065 - val_loss: 0.3587 - val_acc: 0.9122\n",
      "Epoch 217/250\n",
      "390/390 [==============================] - 175s 449ms/step - loss: 0.3422 - acc: 0.9076 - val_loss: 0.3590 - val_acc: 0.9122\n",
      "Epoch 218/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3439 - acc: 0.9075 - val_loss: 0.3570 - val_acc: 0.9131\n",
      "Epoch 219/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3336 - acc: 0.9107 - val_loss: 0.3573 - val_acc: 0.9128\n",
      "Epoch 220/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3369 - acc: 0.9098 - val_loss: 0.3565 - val_acc: 0.9120\n",
      "Epoch 221/250\n",
      "390/390 [==============================] - 175s 448ms/step - loss: 0.3423 - acc: 0.9079 - val_loss: 0.3577 - val_acc: 0.9125\n",
      "Epoch 222/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3379 - acc: 0.9089 - val_loss: 0.3563 - val_acc: 0.9123\n",
      "Epoch 223/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3388 - acc: 0.9075 - val_loss: 0.3559 - val_acc: 0.9127\n",
      "Epoch 224/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3374 - acc: 0.9095 - val_loss: 0.3563 - val_acc: 0.9125\n",
      "Epoch 225/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3468 - acc: 0.9069 - val_loss: 0.3567 - val_acc: 0.9133\n",
      "Epoch 226/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3438 - acc: 0.9082 - val_loss: 0.3575 - val_acc: 0.9120\n",
      "Epoch 227/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3467 - acc: 0.9055 - val_loss: 0.3569 - val_acc: 0.9132\n",
      "Epoch 228/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3415 - acc: 0.9081 - val_loss: 0.3578 - val_acc: 0.9125\n",
      "Epoch 229/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3409 - acc: 0.9082 - val_loss: 0.3565 - val_acc: 0.9124\n",
      "Epoch 230/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3399 - acc: 0.9092 - val_loss: 0.3569 - val_acc: 0.9128\n",
      "Epoch 231/250\n",
      "390/390 [==============================] - 175s 448ms/step - loss: 0.3386 - acc: 0.9085 - val_loss: 0.3572 - val_acc: 0.9123\n",
      "Epoch 232/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3393 - acc: 0.9092 - val_loss: 0.3553 - val_acc: 0.9120\n",
      "Epoch 233/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3372 - acc: 0.9099 - val_loss: 0.3547 - val_acc: 0.9125\n",
      "Epoch 234/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3375 - acc: 0.9089 - val_loss: 0.3561 - val_acc: 0.9122\n",
      "Epoch 235/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3365 - acc: 0.9092 - val_loss: 0.3573 - val_acc: 0.9120\n",
      "Epoch 236/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3417 - acc: 0.9079 - val_loss: 0.3554 - val_acc: 0.9128\n",
      "Epoch 237/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3391 - acc: 0.9091 - val_loss: 0.3546 - val_acc: 0.9123\n",
      "Epoch 238/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3369 - acc: 0.9095 - val_loss: 0.3559 - val_acc: 0.9114\n",
      "Epoch 239/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3434 - acc: 0.9078 - val_loss: 0.3558 - val_acc: 0.9120\n",
      "Epoch 240/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3374 - acc: 0.9095 - val_loss: 0.3560 - val_acc: 0.9117\n",
      "Epoch 241/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3380 - acc: 0.9084 - val_loss: 0.3581 - val_acc: 0.9116\n",
      "Epoch 242/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3432 - acc: 0.9084 - val_loss: 0.3574 - val_acc: 0.9128\n",
      "Epoch 243/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3383 - acc: 0.9080 - val_loss: 0.3563 - val_acc: 0.9130\n",
      "Epoch 244/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3370 - acc: 0.9097 - val_loss: 0.3573 - val_acc: 0.9118\n",
      "Epoch 245/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3376 - acc: 0.9080 - val_loss: 0.3569 - val_acc: 0.9125\n",
      "Epoch 246/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3361 - acc: 0.9102 - val_loss: 0.3573 - val_acc: 0.9122\n",
      "Epoch 247/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3393 - acc: 0.9093 - val_loss: 0.3566 - val_acc: 0.9123\n",
      "Epoch 248/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3353 - acc: 0.9100 - val_loss: 0.3578 - val_acc: 0.9126\n",
      "Epoch 249/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3338 - acc: 0.9106 - val_loss: 0.3568 - val_acc: 0.9123\n",
      "Epoch 250/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3374 - acc: 0.9081 - val_loss: 0.3565 - val_acc: 0.9122\n",
      "{'val_acc': [0.9113, 0.91200000000000003, 0.91159999999999997, 0.91100000000000003, 0.91169999999999995, 0.91200000000000003, 0.91349999999999998, 0.91180000000000005, 0.91220000000000001, 0.91069999999999995, 0.91259999999999997, 0.91100000000000003, 0.9133, 0.91259999999999997, 0.91269999999999996, 0.91220000000000001, 0.91220000000000001, 0.91310000000000002, 0.91279999999999994, 0.91200000000000003, 0.91249999999999998, 0.9123, 0.91269999999999996, 0.91249999999999998, 0.9133, 0.91200000000000003, 0.91320000000000001, 0.91249999999999998, 0.91239999999999999, 0.91279999999999994, 0.9123, 0.91200000000000003, 0.91249999999999998, 0.91220000000000001, 0.91200000000000003, 0.91279999999999994, 0.9123, 0.91139999999999999, 0.91200000000000003, 0.91169999999999995, 0.91159999999999997, 0.91279999999999994, 0.91300000000000003, 0.91180000000000005, 0.91249999999999998, 0.91220000000000001, 0.9123, 0.91259999999999997, 0.9123, 0.91220000000000001], 'acc': [0.9072224896115525, 0.90878649346140816, 0.9091273660571062, 0.90758341357690386, 0.9081047481743999, 0.90636028236099109, 0.90776387555957949, 0.90758341357690386, 0.90651041666666665, 0.90703275533693295, 0.90880654475457168, 0.90860603145961005, 0.90718238692961328, 0.91109239655425234, 0.9089268527430221, 0.90654074428629949, 0.90758341355778138, 0.90752325952531132, 0.91063121591273666, 0.90976900864305277, 0.9078525641025641, 0.90889932560706332, 0.90750320823214781, 0.90946823865280424, 0.9069016682898956, 0.90818495346178874, 0.90553817771587919, 0.90814485085633923, 0.90820500485056443, 0.90926772537696499, 0.90845352564102566, 0.90928066797032903, 0.90988931668887052, 0.90884664745563337, 0.90916746868167808, 0.90796438881629626, 0.909047160750595, 0.90950834133474345, 0.90776387555957949, 0.9094481873787631, 0.90836541542534188, 0.90844562075097557, 0.90800449153648033, 0.90972890601848078, 0.90804459412280747, 0.91025024065422178, 0.90936798201488467, 0.91000962465819846, 0.91059111324991981, 0.90808469680474668], 'loss': [0.34583065683427894, 0.33846848355507597, 0.33977897937075235, 0.34223311177652388, 0.34492587328033641, 0.34649803632147846, 0.34607297571224777, 0.34410663170427475, 0.3449773800296661, 0.34319592539553623, 0.34073498968918198, 0.33932921050411025, 0.34380678783290691, 0.33442501325063456, 0.33969482898061998, 0.34669154068771219, 0.34216012506673149, 0.34383424584935174, 0.33368047633001274, 0.33697681576443822, 0.34234578842535995, 0.33791483973147551, 0.33877898199572792, 0.33734676509588851, 0.3467844627895606, 0.3437917545738991, 0.34674114225994596, 0.34157870595639511, 0.34091899069525089, 0.33977250151042188, 0.3386111978536997, 0.33899250466737829, 0.33718449190942607, 0.33751418828046359, 0.33640786508241066, 0.34166705080588583, 0.33913296199127924, 0.33683839003398014, 0.34347188388609068, 0.33749183724620307, 0.3379644472169922, 0.34315824593761851, 0.33835918636784018, 0.33695026927167193, 0.33746689748297448, 0.33600294362439304, 0.33916378637938915, 0.33536710349420418, 0.33383783906516562, 0.33742620430009501], 'val_loss': [0.36025282177925111, 0.35847312402725218, 0.35775190348625185, 0.36071583547592162, 0.35930837817192079, 0.35880326938629148, 0.35803041267395019, 0.35793254947662356, 0.35803777732849124, 0.35802743630409239, 0.35794903073310852, 0.3589173144340515, 0.35737328486442566, 0.3575584867954254, 0.35666787939071654, 0.35874873046875, 0.35901206283569337, 0.35696074590682986, 0.35728783864974978, 0.3565059983730316, 0.35767435097694394, 0.3562911828517914, 0.35590344061851503, 0.35627087020874021, 0.35669478273391725, 0.35747841596603391, 0.35694752469062807, 0.35775206675529481, 0.35645347895622254, 0.35690190682411194, 0.35719448018074035, 0.35531951389312744, 0.35466044759750365, 0.3561012767791748, 0.35732869486808777, 0.35541306529045102, 0.35457460079193115, 0.35587861161231993, 0.35581504049301149, 0.35600364303588866, 0.35813192152976991, 0.35743286514282224, 0.35628065261840819, 0.3573427780151367, 0.35688709259033202, 0.35734143276214597, 0.35658669672012328, 0.35775420579910278, 0.35676126613616943, 0.35650277915000916]}\n"
     ]
    }
   ],
   "source": [
    "# Create model and get ready - reproducible cell\n",
    "model = load_model('e_swish_2_at_2.h5', custom_objects={\"e_swish_2\": e_swish_2})\n",
    "opt_rms = keras.optimizers.rmsprop(lr=0.000017,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "\n",
    "# Training\n",
    "batch_size = 128\n",
    "epochs=250\n",
    "# lr scheduler\n",
    "lr_1 = keras.callbacks.LearningRateScheduler(schedule)\n",
    "\n",
    "# Fit the model\n",
    "his = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,verbose=1,validation_data=(x_test,y_test), callbacks=[lr_1], initial_epoch=200)\n",
    "# Save the model\n",
    "model.save('e_swish_2_at_2.h5')\n",
    "# Display logs\n",
    "print(his.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 201/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3818 - acc: 0.8962 - val_loss: 0.3782 - val_acc: 0.9078\n",
      "Epoch 202/250\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3836 - acc: 0.8963 - val_loss: 0.3775 - val_acc: 0.9067\n",
      "Epoch 203/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3824 - acc: 0.8976 - val_loss: 0.3762 - val_acc: 0.9088\n",
      "Epoch 204/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3854 - acc: 0.8958 - val_loss: 0.3776 - val_acc: 0.9070\n",
      "Epoch 205/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3780 - acc: 0.8981 - val_loss: 0.3781 - val_acc: 0.9057\n",
      "Epoch 206/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3822 - acc: 0.8972 - val_loss: 0.3765 - val_acc: 0.9060\n",
      "Epoch 207/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3793 - acc: 0.8960 - val_loss: 0.3769 - val_acc: 0.9060\n",
      "Epoch 208/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3807 - acc: 0.8976 - val_loss: 0.3757 - val_acc: 0.9079\n",
      "Epoch 209/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3811 - acc: 0.8971 - val_loss: 0.3752 - val_acc: 0.9081\n",
      "Epoch 210/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3829 - acc: 0.8961 - val_loss: 0.3751 - val_acc: 0.9073\n",
      "Epoch 211/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3807 - acc: 0.8955 - val_loss: 0.3756 - val_acc: 0.9077\n",
      "Epoch 212/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3787 - acc: 0.8972 - val_loss: 0.3758 - val_acc: 0.9076\n",
      "Epoch 213/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3797 - acc: 0.8960 - val_loss: 0.3757 - val_acc: 0.9071\n",
      "Epoch 214/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3789 - acc: 0.8955 - val_loss: 0.3749 - val_acc: 0.9074\n",
      "Epoch 215/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3822 - acc: 0.8958 - val_loss: 0.3747 - val_acc: 0.9068\n",
      "Epoch 216/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3793 - acc: 0.8958 - val_loss: 0.3744 - val_acc: 0.9071\n",
      "Epoch 217/250\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3826 - acc: 0.8951 - val_loss: 0.3751 - val_acc: 0.9073\n",
      "Epoch 218/250\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3817 - acc: 0.8962 - val_loss: 0.3744 - val_acc: 0.9071\n",
      "Epoch 219/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3800 - acc: 0.8978 - val_loss: 0.3750 - val_acc: 0.9069\n",
      "Epoch 220/250\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3760 - acc: 0.8967 - val_loss: 0.3757 - val_acc: 0.9066\n",
      "Epoch 221/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3808 - acc: 0.8956 - val_loss: 0.3742 - val_acc: 0.9072\n",
      "Epoch 222/250\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3775 - acc: 0.8975 - val_loss: 0.3752 - val_acc: 0.9076\n",
      "Epoch 223/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3772 - acc: 0.8976 - val_loss: 0.3729 - val_acc: 0.9085\n",
      "Epoch 224/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3767 - acc: 0.8978 - val_loss: 0.3731 - val_acc: 0.9084\n",
      "Epoch 225/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3789 - acc: 0.8964 - val_loss: 0.3752 - val_acc: 0.9071\n",
      "Epoch 226/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3784 - acc: 0.8970 - val_loss: 0.3739 - val_acc: 0.9076\n",
      "Epoch 227/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3806 - acc: 0.8957 - val_loss: 0.3744 - val_acc: 0.9080\n",
      "Epoch 228/250\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3729 - acc: 0.8995 - val_loss: 0.3753 - val_acc: 0.9083\n",
      "Epoch 229/250\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3720 - acc: 0.8988 - val_loss: 0.3749 - val_acc: 0.9082\n",
      "Epoch 230/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3792 - acc: 0.8967 - val_loss: 0.3751 - val_acc: 0.9079\n",
      "Epoch 231/250\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3762 - acc: 0.8984 - val_loss: 0.3752 - val_acc: 0.9078\n",
      "Epoch 232/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3787 - acc: 0.8963 - val_loss: 0.3746 - val_acc: 0.9076\n",
      "Epoch 233/250\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3801 - acc: 0.8978 - val_loss: 0.3756 - val_acc: 0.9074\n",
      "Epoch 234/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3752 - acc: 0.8976 - val_loss: 0.3743 - val_acc: 0.9081\n",
      "Epoch 235/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3780 - acc: 0.8964 - val_loss: 0.3734 - val_acc: 0.9078\n",
      "Epoch 236/250\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3717 - acc: 0.8989 - val_loss: 0.3750 - val_acc: 0.9082\n",
      "Epoch 237/250\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3747 - acc: 0.8973 - val_loss: 0.3745 - val_acc: 0.9070\n",
      "Epoch 238/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3735 - acc: 0.8971 - val_loss: 0.3725 - val_acc: 0.9089\n",
      "Epoch 239/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3738 - acc: 0.8988 - val_loss: 0.3742 - val_acc: 0.9077\n",
      "Epoch 240/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3733 - acc: 0.8977 - val_loss: 0.3734 - val_acc: 0.9077\n",
      "Epoch 241/250\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3769 - acc: 0.8974 - val_loss: 0.3738 - val_acc: 0.9073\n",
      "Epoch 242/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3726 - acc: 0.8982 - val_loss: 0.3740 - val_acc: 0.9077\n",
      "Epoch 243/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3763 - acc: 0.8980 - val_loss: 0.3723 - val_acc: 0.9083\n",
      "Epoch 244/250\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3717 - acc: 0.8986 - val_loss: 0.3733 - val_acc: 0.9078\n",
      "Epoch 245/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3760 - acc: 0.8976 - val_loss: 0.3726 - val_acc: 0.9084\n",
      "Epoch 246/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3791 - acc: 0.8974 - val_loss: 0.3723 - val_acc: 0.9093\n",
      "Epoch 247/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3760 - acc: 0.8976 - val_loss: 0.3731 - val_acc: 0.9081\n",
      "Epoch 248/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3766 - acc: 0.8962 - val_loss: 0.3728 - val_acc: 0.9077\n",
      "Epoch 249/250\n",
      "390/390 [==============================] - 174s 446ms/step - loss: 0.3777 - acc: 0.8967 - val_loss: 0.3731 - val_acc: 0.9083\n",
      "Epoch 250/250\n",
      "390/390 [==============================] - 174s 447ms/step - loss: 0.3720 - acc: 0.8983 - val_loss: 0.3729 - val_acc: 0.9082\n",
      "{'val_acc': [0.90780000000000005, 0.90669999999999995, 0.90880000000000005, 0.90700000000000003, 0.90569999999999995, 0.90600000000000003, 0.90600000000000003, 0.90790000000000004, 0.90810000000000002, 0.9073, 0.90769999999999995, 0.90759999999999996, 0.90710000000000002, 0.90739999999999998, 0.90680000000000005, 0.90710000000000002, 0.9073, 0.90710000000000002, 0.90690000000000004, 0.90659999999999996, 0.90720000000000001, 0.90759999999999996, 0.90849999999999997, 0.90839999999999999, 0.90710000000000002, 0.90759999999999996, 0.90800000000000003, 0.9083, 0.90820000000000001, 0.90790000000000004, 0.90780000000000005, 0.90759999999999996, 0.90739999999999998, 0.90810000000000002, 0.90780000000000005, 0.90820000000000001, 0.90700000000000003, 0.90890000000000004, 0.90769999999999995, 0.90769999999999995, 0.9073, 0.90769999999999995, 0.9083, 0.90780000000000005, 0.90839999999999999, 0.9093, 0.90810000000000002, 0.90769999999999995, 0.9083, 0.90820000000000001], 'acc': [0.89619425729868463, 0.89627446266256316, 0.89753769650304782, 0.89581328198280252, 0.89805903108142149, 0.89715672122541057, 0.89601379529688652, 0.89767805578466175, 0.89711661858171621, 0.89607394934847906, 0.89551251201167636, 0.89713666983663476, 0.89597369265319216, 0.89557266604414654, 0.8957732371794872, 0.89577317937735301, 0.89509071933204876, 0.89625441129290984, 0.89785851778645986, 0.89671559193442563, 0.89563282005749412, 0.89755774781533382, 0.8975777991276197, 0.8978184151427655, 0.89633461661854352, 0.89697625918536761, 0.89569310897435894, 0.89948619131714236, 0.89876082775720545, 0.89673564326583399, 0.89839990371536438, 0.89629451397484905, 0.89787856915611308, 0.8976179018286814, 0.89645492458787146, 0.89888113568828854, 0.89727702917561614, 0.897056464587491, 0.89878087906949133, 0.89767628205128203, 0.89745905593410857, 0.89829964709656718, 0.89795673076923077, 0.89860308285163781, 0.89753769648392534, 0.89739733716406656, 0.89759785053551788, 0.89619391025641026, 0.89669637122671808, 0.89827959572691396], 'loss': [0.38192385609118573, 0.38366235368596768, 0.38243792514637337, 0.38529716774864309, 0.3780222623670258, 0.3821340719495479, 0.37929437002349364, 0.38054197024641262, 0.38106803024736857, 0.38285865186497614, 0.38067114429669691, 0.3789266852669736, 0.37971156418686841, 0.37872003302346219, 0.38215108131751035, 0.37933256552890204, 0.38256422186709399, 0.38166929811554762, 0.37994617678853693, 0.37597584788486443, 0.3808525275124699, 0.37747222928997792, 0.37706229256099716, 0.37657802916506294, 0.37898766002021694, 0.37830471665498955, 0.38057093031895467, 0.37296465769477127, 0.37208341142861245, 0.37913693226144829, 0.37625773273525537, 0.37866799462136069, 0.38012097520173493, 0.37506118954126749, 0.37796168323315832, 0.37171630813289919, 0.37480485195489432, 0.37357130567347502, 0.3739193721788221, 0.37334053279497686, 0.37675303092505125, 0.37235717508400484, 0.37632885819826373, 0.37153071385204756, 0.37608381087557413, 0.3790424772570215, 0.3759769953459855, 0.3766193736440096, 0.37768305890745435, 0.3720105104542788], 'val_loss': [0.37823536205291747, 0.37747736020088196, 0.37619412536621094, 0.37756411218643188, 0.37812863197326663, 0.37653512229919434, 0.37685041356086729, 0.37571484167575836, 0.37520423455238344, 0.37509226880073548, 0.37559298319816592, 0.37577983741760251, 0.37568331146240236, 0.37494024095535278, 0.3747356897354126, 0.37437291817665103, 0.37507482862472535, 0.37437629785537718, 0.37495786361694333, 0.37567552423477174, 0.37424509992599486, 0.37515580270290377, 0.37290649290084837, 0.37313869249820708, 0.37515520482063291, 0.37387374358177183, 0.37436483311653135, 0.3752700170278549, 0.37491791720390322, 0.37514262261390685, 0.37520979089736939, 0.37460635347366333, 0.37557938446998596, 0.37433548512458803, 0.37342010231018069, 0.37502991566658023, 0.37452910547256468, 0.37250451855659483, 0.37424582848548887, 0.37337181138992309, 0.37381111397743227, 0.37404450764656066, 0.37225306589603424, 0.37333880391120911, 0.37256061735153201, 0.37233733358383181, 0.37306021027565001, 0.37276746668815613, 0.37307930479049684, 0.3729367765903473]}\n"
     ]
    }
   ],
   "source": [
    "# Create model and get ready - reproducible cell\n",
    "model = load_model('e_swish_2_at_3.h5', custom_objects={\"e_swish_2\": e_swish_2})\n",
    "opt_rms = keras.optimizers.rmsprop(lr=0.000017,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "\n",
    "# Training\n",
    "batch_size = 128\n",
    "epochs=250\n",
    "# lr scheduler\n",
    "lr_1 = keras.callbacks.LearningRateScheduler(schedule)\n",
    "\n",
    "# Fit the model\n",
    "his = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,verbose=1,validation_data=(x_test,y_test), callbacks=[lr_1], initial_epoch=200)\n",
    "# Save the model\n",
    "model.save('e_swish_2_at_3.h5')\n",
    "# Display logs\n",
    "print(his.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_test):\n",
    "    y_hat = np.argmax(y_pred, axis=1)\n",
    "    y = np.argmax(y_test, axis=1)\n",
    "\n",
    "    good = np.sum(np.equal(y, y_hat))\n",
    "    return float(good/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "0.9239\n"
     ]
    }
   ],
   "source": [
    "import keras.regularizers as regularizers\n",
    "ensembler = 0\n",
    "for i in range(3):\n",
    "    if i == 0:\n",
    "        model = create(e_swish_2)\n",
    "        model.load_weights('e_swish_2_at_'+str(i+1)+'.h5')\n",
    "        model.save('e_swish_2_at_'+str(i+1)+'.h5')\n",
    "    else:\n",
    "        model = load_model('e_swish_2_at_'+str(i+1)+'.h5', custom_objects={\"e_swish_2\": e_swish_2})\n",
    "    ensembler += model.predict_proba(x_test)\n",
    "    \n",
    "print(accuracy(ensembler, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
